{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5a7309",
   "metadata": {},
   "source": [
    "# 📊 Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
    "# Memuat dan Memproses Data dengan TensorFlow\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Tujuan Pembelajaran\n",
    "\n",
    "Setelah menyelesaikan chapter ini, Anda akan mampu:\n",
    "- ✅ Memahami konsep TensorFlow Data API\n",
    "- ✅ Membuat dan memanipulasi dataset dengan tf.data\n",
    "- ✅ Memuat data dari berbagai sumber (CSV, TFRecord, dll)\n",
    "- ✅ Melakukan preprocessing data yang efisien\n",
    "- ✅ Mengoptimalkan performa data pipeline\n",
    "- ✅ Mengintegrasikan data pipeline dengan model Keras\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Outline Chapter\n",
    "\n",
    "1. **Data API - Konsep Dasar** 🔰\n",
    "2. **Loading Data dari Files** 📁\n",
    "3. **Data Preprocessing & Feature Engineering** 🔧\n",
    "4. **Performance Optimization** 🚀\n",
    "5. **TFRecord Format** 💾\n",
    "6. **Integrasi dengan Keras** 🤝\n",
    "7. **Best Practices & Tips** 💡\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Pengantar\n",
    "\n",
    "Chapter 13 ini membahas cara memuat dan memproses data secara efisien untuk sistem Deep Learning. Sejauh ini kita hanya menggunakan dataset yang muat di memori, namun sistem Deep Learning sering dilatih dengan dataset sangat besar yang tidak muat di RAM.\n",
    "\n",
    "**TensorFlow Data API** menyediakan solusi untuk:\n",
    "- 📊 Memuat dataset besar secara efisien\n",
    "- ⚡ Memproses data dengan multithreading, queuing, batching, dan prefetching\n",
    "- 🔗 Integrasi seamless dengan tf.keras\n",
    "- 🎯 Pipeline data yang scalable dan reproducible\n",
    "\n",
    "**Dataset yang didukung:**\n",
    "- 📄 File teks (CSV, JSON)  \n",
    "- 🗂️ File binary dengan record berukuran tetap\n",
    "- 💾 TFRecord format (record berukuran variabel)\n",
    "- 🗃️ Database SQL\n",
    "- 🌐 Berbagai sumber data lainnya melalui ekstensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37eea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 CHAPTER 13: Loading and Preprocessing Data with TensorFlow\n",
      "============================================================\n",
      "📦 TensorFlow version: 2.19.0\n",
      "📦 NumPy version: 2.1.3\n",
      "📦 Pandas version: 2.3.0\n",
      "💻 Running on CPU\n",
      "\n",
      "✅ Setup complete! Ready to explore TensorFlow Data API\n",
      "============================================================\n",
      "📦 TensorFlow version: 2.19.0\n",
      "📦 NumPy version: 2.1.3\n",
      "📦 Pandas version: 2.3.0\n",
      "💻 Running on CPU\n",
      "\n",
      "✅ Setup complete! Ready to explore TensorFlow Data API\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Setup & Import Libraries\n",
    "print(\"=\" * 60)\n",
    "print(\"🚀 CHAPTER 13: Loading and Preprocessing Data with TensorFlow\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display versions\n",
    "print(f\"📦 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"📦 NumPy version: {np.__version__}\")\n",
    "print(f\"📦 Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🎮 GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print(\"💻 Running on CPU\")\n",
    "\n",
    "print(\"\\n✅ Setup complete! Ready to explore TensorFlow Data API\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f0572",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔰 1. Data API - Konsep Dasar\n",
    "\n",
    "## 📚 Pengantar tf.data\n",
    "\n",
    "**tf.data API** adalah inti dari data loading di TensorFlow yang menyediakan:\n",
    "\n",
    "- 🔄 **Dataset**: Abstraksi untuk sequence of elements\n",
    "- ⚡ **Transformations**: Map, filter, batch, shuffle, dll\n",
    "- 🚀 **Performance**: Prefetching, caching, parallelization\n",
    "- 🔗 **Integration**: Seamless dengan tf.keras\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Membuat Dataset Sederhana 📊\n",
    "\n",
    "Data API berpusat pada konsep **'Dataset'** yang merepresentasikan urutan item data. Dataset biasanya membaca data dari disk secara bertahap, namun untuk kesederhanaan kita mulai dengan dataset di RAM menggunakan `tf.data.Dataset.from_tensor_slices()`\n",
    "\n",
    "### Metode Pembuatan Dataset:\n",
    "- `tf.data.Dataset.from_tensor_slices()` - dari array/tensor\n",
    "- `tf.data.Dataset.from_tensors()` - dari single tensor\n",
    "- `tf.data.Dataset.range()` - range values\n",
    "- `tf.data.Dataset.from_generator()` - dari generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9861b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔰 BASIC DATASET CREATION\n",
      "==================================================\n",
      "\n",
      "📋 1. Dataset dari tensor slices:\n",
      "Dataset dari range(10): [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9)]\n",
      "\n",
      "📋 2. Dataset dari multiple arrays:\n",
      "Dataset dari (X, Y):\n",
      "  X: 0, Y: 10\n",
      "  X: 1, Y: 11\n",
      "  X: 2, Y: 12\n",
      "  X: 3, Y: 13\n",
      "  X: 4, Y: 14\n",
      "\n",
      "📋 3. Dataset dengan dictionary:\n",
      "Dataset dictionary:\n",
      "  Sample 1:\n",
      "    Features shape: (3,)\n",
      "    Label: 0\n",
      "  Sample 2:\n",
      "    Features shape: (3,)\n",
      "    Label: 1\n",
      "  Sample 3:\n",
      "    Features shape: (3,)\n",
      "    Label: 2\n",
      "  Sample 4:\n",
      "    Features shape: (3,)\n",
      "    Label: 3\n",
      "  Sample 5:\n",
      "    Features shape: (3,)\n",
      "    Label: 4\n",
      "\n",
      "📋 4. Dataset dari single tensor:\n",
      "Dataset dari single tensor:\n",
      "  Shape: (3, 2), Values:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "📋 5. Dataset range:\n",
      "Range dataset: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "\n",
      "✅ Basic dataset creation complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 📊 1.1 Membuat Dataset Sederhana dari Array\n",
    "print(\"🔰 BASIC DATASET CREATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Dataset dari tensor slices (paling umum)\n",
    "print(\"\\n📋 1. Dataset dari tensor slices:\")\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "print(f\"Dataset dari range(10): {list(dataset.as_numpy_iterator())}\")\n",
    "\n",
    "# 2. Dataset dari multiple arrays\n",
    "print(\"\\n📋 2. Dataset dari multiple arrays:\")\n",
    "X = tf.range(5) \n",
    "Y = tf.range(10, 15)\n",
    "dataset_xy = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "print(\"Dataset dari (X, Y):\")\n",
    "for x, y in dataset_xy:\n",
    "    print(f\"  X: {x.numpy()}, Y: {y.numpy()}\")\n",
    "\n",
    "# 3. Dataset dengan dictionary (sangat berguna!)\n",
    "print(\"\\n📋 3. Dataset dengan dictionary:\")\n",
    "dataset_dict = tf.data.Dataset.from_tensor_slices({\n",
    "    \"features\": tf.random.normal((5, 3)),\n",
    "    \"labels\": tf.range(5)\n",
    "})\n",
    "print(\"Dataset dictionary:\")\n",
    "for i, item in enumerate(dataset_dict):\n",
    "    print(f\"  Sample {i+1}:\")\n",
    "    print(f\"    Features shape: {item['features'].shape}\")\n",
    "    print(f\"    Label: {item['labels'].numpy()}\")\n",
    "\n",
    "# 4. Dataset dari single tensor\n",
    "print(\"\\n📋 4. Dataset dari single tensor:\")\n",
    "tensor_data = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "dataset_tensor = tf.data.Dataset.from_tensors(tensor_data)\n",
    "print(\"Dataset dari single tensor:\")\n",
    "for item in dataset_tensor:\n",
    "    print(f\"  Shape: {item.shape}, Values:\\n{item.numpy()}\")\n",
    "\n",
    "# 5. Dataset range\n",
    "print(\"\\n📋 5. Dataset range:\")\n",
    "range_dataset = tf.data.Dataset.range(5)\n",
    "print(f\"Range dataset: {list(range_dataset.as_numpy_iterator())}\")\n",
    "\n",
    "print(\"\\n✅ Basic dataset creation complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284383f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Transformasi Dataset 🔄\n",
    "\n",
    "Dataset memiliki berbagai **method transformasi** yang dapat di-chain untuk membangun data pipeline yang powerful:\n",
    "\n",
    "### 🛠️ Core Transformations:\n",
    "- **`map(func)`** - Menerapkan fungsi ke setiap elemen  \n",
    "- **`filter(predicate)`** - Menyaring elemen berdasarkan kondisi\n",
    "- **`batch(batch_size)`** - Mengelompokkan elemen dalam batch\n",
    "- **`shuffle(buffer_size)`** - Mengacak urutan elemen\n",
    "- **`repeat(count)`** - Mengulang dataset\n",
    "- **`take(count)`** - Mengambil n elemen pertama\n",
    "- **`skip(count)`** - Melewati n elemen pertama\n",
    "- **`cache(filename)`** - Cache data di memory/disk\n",
    "- **`prefetch(buffer_size)`** - Load data di background\n",
    "\n",
    "### 🔗 Method Chaining:\n",
    "Transformasi dapat di-chain menggunakan fluent interface:\n",
    "```python\n",
    "dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
    "           .map(preprocess_func)\n",
    "           .filter(lambda x: x > 0)\n",
    "           .shuffle(1000)\n",
    "           .batch(32)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef659b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 1.2 Demonstrasi Transformasi Dataset\n",
    "print(\"🔄 DATASET TRANSFORMATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dataset awal untuk demo\n",
    "base_data = tf.range(12)\n",
    "print(f\"📊 Base dataset: {list(base_data.as_numpy_iterator())}\")\n",
    "\n",
    "print(\"\\n🛠️ Individual Transformations:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. MAP - Transform setiap elemen\n",
    "print(\"\\n1️⃣ MAP Transformation:\")\n",
    "mapped = tf.data.Dataset.from_tensor_slices(base_data).map(lambda x: x ** 2)\n",
    "print(f\"   Original: {list(base_data.as_numpy_iterator())}\")\n",
    "print(f\"   Squared:  {list(mapped.as_numpy_iterator())}\")\n",
    "\n",
    "# 2. FILTER - Saring elemen\n",
    "print(\"\\n2️⃣ FILTER Transformation:\")\n",
    "filtered = tf.data.Dataset.from_tensor_slices(base_data).filter(lambda x: x % 3 == 0)\n",
    "print(f\"   Original: {list(base_data.as_numpy_iterator())}\")\n",
    "print(f\"   Divisible by 3: {list(filtered.as_numpy_iterator())}\")\n",
    "\n",
    "# 3. BATCH - Kelompokkan dalam batch\n",
    "print(\"\\n3️⃣ BATCH Transformation:\")\n",
    "batched = tf.data.Dataset.from_tensor_slices(base_data).batch(4)\n",
    "print(\"   Batched data:\")\n",
    "for i, batch in enumerate(batched):\n",
    "    print(f\"     Batch {i+1}: {batch.numpy()}\")\n",
    "\n",
    "# 4. SHUFFLE - Acak data\n",
    "print(\"\\n4️⃣ SHUFFLE Transformation:\")\n",
    "shuffled = (tf.data.Dataset.from_tensor_slices(base_data)\n",
    "            .shuffle(buffer_size=12, seed=42))\n",
    "print(f\"   Original: {list(base_data.as_numpy_iterator())}\")\n",
    "print(f\"   Shuffled: {list(shuffled.as_numpy_iterator())}\")\n",
    "\n",
    "print(\"\\n🔗 CHAINED TRANSFORMATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Complex pipeline dengan chaining\n",
    "result = (tf.data.Dataset.from_tensor_slices(base_data)\n",
    "          .filter(lambda x: x < 10)          # Filter: x < 10\n",
    "          .map(lambda x: x * 2)              # Map: multiply by 2  \n",
    "          .shuffle(buffer_size=20, seed=42)   # Shuffle\n",
    "          .batch(3)                          # Batch size 3\n",
    "          .take(2))                          # Take first 2 batches\n",
    "\n",
    "print(\"Pipeline: filter(x<10) → map(x*2) → shuffle → batch(3) → take(2)\")\n",
    "for i, batch in enumerate(result):\n",
    "    print(f\"   Batch {i+1}: {batch.numpy()}\")\n",
    "\n",
    "print(\"\\n✅ Dataset transformations complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185daf88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📁 2. Loading Data dari Files\n",
    "\n",
    "## 🗂️ Sumber Data yang Didukung\n",
    "\n",
    "TensorFlow Data API mendukung berbagai format file:\n",
    "\n",
    "### 📊 Structured Data:\n",
    "- **CSV Files** - `tf.data.experimental.make_csv_dataset()`\n",
    "- **JSON Files** - Custom parsing dengan `TextLineDataset`\n",
    "- **Parquet Files** - Via TensorFlow I/O\n",
    "\n",
    "### 💾 Binary Data:\n",
    "- **TFRecord** - `tf.data.TFRecordDataset()` (format native TF)\n",
    "- **Fixed-length records** - `tf.data.FixedLengthRecordDataset()`\n",
    "- **Raw binary** - `tf.data.RawRecordDataset()`\n",
    "\n",
    "### 📄 Text Data:\n",
    "- **Text files** - `tf.data.TextLineDataset()`\n",
    "- **Image files** - `tf.data.Dataset.list_files()` + `tf.io.read_file()`\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 CSV Files 📊\n",
    "\n",
    "CSV adalah format paling umum untuk structured data. TensorFlow menyediakan 2 pendekatan:\n",
    "1. **High-level**: `make_csv_dataset()` - otomatis parsing\n",
    "2. **Low-level**: `TextLineDataset()` - kontrol manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 2.1 Loading CSV Data - Practical Examples\n",
    "print(\"📁 LOADING CSV DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample CSV data\n",
    "csv_content = \"\"\"longitude,latitude,housing_median_age,total_rooms,population,median_income,price\n",
    "-122.23,37.88,41.0,880.0,322.0,8.3252,452600.0\n",
    "-122.22,37.86,21.0,1106.0,2401.0,8.3014,358500.0\n",
    "-122.24,37.85,52.0,1467.0,496.0,7.2574,352100.0\n",
    "-122.25,37.85,52.0,1274.0,558.0,5.6431,341300.0\n",
    "-122.25,37.85,52.0,1627.0,565.0,3.8462,342200.0\"\"\"\n",
    "\n",
    "# Write to temporary file\n",
    "temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_file.write(csv_content)\n",
    "temp_file.close()\n",
    "print(f\"📝 Created sample CSV: {os.path.basename(temp_file.name)}\")\n",
    "\n",
    "print(\"\\n🎯 METHOD 1: make_csv_dataset (Recommended)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# High-level CSV loading\n",
    "csv_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    temp_file.name,\n",
    "    batch_size=2,\n",
    "    label_name=\"price\",           # Target column\n",
    "    na_value=\"?\",                 # Missing value indicator\n",
    "    num_epochs=1,                 # Number of epochs\n",
    "    ignore_errors=True,           # Skip problematic rows\n",
    "    shuffle=False                 # Keep order for demo\n",
    ")\n",
    "\n",
    "print(\"✅ Dataset created with automatic type inference\")\n",
    "print(\"📋 Sample data:\")\n",
    "for batch_num, (features, labels) in enumerate(csv_dataset.take(2)):\n",
    "    print(f\"\\n   Batch {batch_num + 1}:\")\n",
    "    print(f\"   Features: {list(features.keys())}\")\n",
    "    for key, values in features.items():\n",
    "        print(f\"     {key}: {values.numpy()}\")\n",
    "    print(f\"   Labels (price): {labels.numpy()}\")\n",
    "\n",
    "print(\"\\n🔧 METHOD 2: TextLineDataset (Manual Control)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Low-level manual parsing\n",
    "def parse_csv_line(line):\n",
    "    \"\"\"Parse a single CSV line\"\"\"\n",
    "    # Define default values (for type inference)\n",
    "    defaults = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # All float\n",
    "    fields = tf.io.decode_csv(line, defaults)\n",
    "    \n",
    "    # Features (all except last column)\n",
    "    features = tf.stack(fields[:-1])\n",
    "    # Label (last column) \n",
    "    label = fields[-1]\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "# Create dataset from text lines\n",
    "text_dataset = tf.data.TextLineDataset(temp_file.name)\n",
    "text_dataset = text_dataset.skip(1)  # Skip header\n",
    "parsed_dataset = text_dataset.map(parse_csv_line)\n",
    "\n",
    "print(\"✅ Manual parsing complete\")\n",
    "print(\"📋 Parsed data samples:\")\n",
    "for i, (features, label) in enumerate(parsed_dataset.take(2)):\n",
    "    print(f\"   Sample {i+1}: Features={features.numpy()}, Price={label.numpy()}\")\n",
    "\n",
    "print(\"\\n🔄 PIPELINE-READY DATASET:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create training-ready pipeline\n",
    "training_dataset = (parsed_dataset\n",
    "                   .shuffle(buffer_size=100, seed=42)\n",
    "                   .batch(2)\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"✅ Training pipeline: shuffle → batch → prefetch\")\n",
    "for batch in training_dataset.take(1):\n",
    "    features, labels = batch\n",
    "    print(f\"   Batch shape: Features {features.shape}, Labels {labels.shape}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(temp_file.name)\n",
    "print(f\"\\n🧹 Cleaned up temporary file\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d24fe",
   "metadata": {},
   "source": [
    "# 🔧 3. Data Preprocessing & Feature Engineering\n",
    "\n",
    "## 🎯 Mengapa Preprocessing Penting?\n",
    "\n",
    "Deep learning models membutuhkan data yang **terstandarisasi** dan **bersih**:\n",
    "\n",
    "### 📊 Numerical Data:\n",
    "- **Normalisasi** - Scale data ke range [0,1]: `(x - min) / (max - min)`\n",
    "- **Standardisasi** - Zero mean, unit variance: `(x - mean) / std`\n",
    "- **Robust scaling** - Menggunakan median dan IQR\n",
    "\n",
    "### 📝 Text Data:\n",
    "- **Tokenization** - Split text menjadi tokens\n",
    "- **Vocabulary mapping** - Convert tokens ke integers\n",
    "- **Padding** - Uniform sequence length\n",
    "- **Embedding** - Dense vector representation\n",
    "\n",
    "### 🖼️ Image Data:\n",
    "- **Normalization** - Pixel values [0,1] atau [-1,1]\n",
    "- **Resize** - Uniform image dimensions\n",
    "- **Augmentation** - Rotation, flip, crop, dll\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Numerical Preprocessing 🔢\n",
    "\n",
    "Teknik preprocessing untuk data numerik:\n",
    "- **Normalisasi**: Mengubah skala data ke range [0,1]  \n",
    "- **Standardisasi**: Mengubah data memiliki mean=0 dan std=1\n",
    "- **Robust scaling**: Menggunakan median dan IQR\n",
    "\n",
    "## 3.2 Text Preprocessing 📚\n",
    "\n",
    "Untuk data teks, kita perlu:\n",
    "- **Tokenisasi**: Memecah teks menjadi token-token\n",
    "- **Pemetaan Kosakata**: Mengonversi token menjadi bilangan bulat\n",
    "- **Padding**: Menyamakan panjang urutan\n",
    "- **Penyematan**: Representasi vektor yang padat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Data Preprocessing Examples\n",
    "print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# Sample data\n",
    "data = tf.constant([\n",
    "    [1.0, 100.0, 0.5],  \n",
    "    [2.0, 200.0, 1.5],\n",
    "    [3.0, 150.0, 2.0],\n",
    "    [4.0, 50.0, 0.8]\n",
    "])\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data.numpy())\n",
    "\n",
    "# 1. Normalization (Min-Max Scaling)\n",
    "print(\"\\n=== NORMALIZATION (Min-Max) ===\")\n",
    "def normalize_minmax(data):\n",
    "    min_vals = tf.reduce_min(data, axis=0)\n",
    "    max_vals = tf.reduce_max(data, axis=0)\n",
    "    return (data - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "normalized_data = normalize_minmax(data)\n",
    "print(\"Normalized data (0-1 range):\")\n",
    "print(normalized_data.numpy())\n",
    "\n",
    "# 2. Standardization (Z-score)\n",
    "print(\"\\n=== STANDARDIZATION (Z-score) ===\")\n",
    "def standardize(data):\n",
    "    mean = tf.reduce_mean(data, axis=0)\n",
    "    std = tf.math.reduce_std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "standardized_data = standardize(data)\n",
    "print(\"Standardized data (mean=0, std=1):\")\n",
    "print(standardized_data.numpy())\n",
    "print(\"Mean:\", tf.reduce_mean(standardized_data, axis=0).numpy())\n",
    "print(\"Std:\", tf.math.reduce_std(standardized_data, axis=0).numpy())\n",
    "\n",
    "# 3. Feature Engineering - Polynomial Features\n",
    "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "def create_polynomial_features(data):\n",
    "    # Create interaction features\n",
    "    x1, x2, x3 = tf.split(data, 3, axis=1)\n",
    "    \n",
    "    # Original features + polynomial features\n",
    "    features = tf.concat([\n",
    "        data,                    # Original features\n",
    "        x1 * x2,                # Interaction x1*x2\n",
    "        x1 * x3,                # Interaction x1*x3  \n",
    "        x2 * x3,                # Interaction x2*x3\n",
    "        tf.square(x1),          # x1²\n",
    "        tf.square(x2),          # x2²\n",
    "        tf.square(x3)           # x3²\n",
    "    ], axis=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "poly_features = create_polynomial_features(data)\n",
    "print(\"Original + Polynomial features:\")\n",
    "print(f\"Shape: {data.shape} → {poly_features.shape}\")\n",
    "print(\"First sample:\")\n",
    "print(f\"  Original: {data[0].numpy()}\")\n",
    "print(f\"  Enhanced: {poly_features[0].numpy()}\")\n",
    "\n",
    "# 🔢 3.1 Numerical Data Preprocessing\n",
    "print(\"🔧 NUMERICAL PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample numerical data (different scales)\n",
    "raw_data = tf.constant([\n",
    "    [1.0, 100.0, 0.5, 1000.0],    # Mixed scales\n",
    "    [2.0, 200.0, 1.5, 2000.0],\n",
    "    [3.0, 150.0, 2.0, 1500.0],\n",
    "    [4.0, 50.0, 0.8, 800.0],\n",
    "    [5.0, 300.0, 1.2, 3000.0]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "print(\"📊 Original data (mixed scales):\")\n",
    "print(raw_data.numpy())\n",
    "print(f\"   Min values: {tf.reduce_min(raw_data, axis=0).numpy()}\")\n",
    "print(f\"   Max values: {tf.reduce_max(raw_data, axis=0).numpy()}\")\n",
    "\n",
    "print(\"\\n1️⃣ MIN-MAX NORMALIZATION [0,1]\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "def normalize_minmax(data):\n",
    "    \"\"\"Min-Max normalization to [0,1] range\"\"\"\n",
    "    min_vals = tf.reduce_min(data, axis=0)\n",
    "    max_vals = tf.reduce_max(data, axis=0)\n",
    "    # Avoid division by zero\n",
    "    range_vals = tf.maximum(max_vals - min_vals, 1e-8)\n",
    "    return (data - min_vals) / range_vals\n",
    "\n",
    "normalized_data = normalize_minmax(raw_data)\n",
    "print(\"✅ Normalized data [0,1]:\")\n",
    "print(normalized_data.numpy())\n",
    "print(f\"   New min: {tf.reduce_min(normalized_data, axis=0).numpy()}\")\n",
    "print(f\"   New max: {tf.reduce_max(normalized_data, axis=0).numpy()}\")\n",
    "\n",
    "print(\"\\n2️⃣ Z-SCORE STANDARDIZATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def standardize_zscore(data):\n",
    "    \"\"\"Z-score standardization (mean=0, std=1)\"\"\"\n",
    "    mean = tf.reduce_mean(data, axis=0)\n",
    "    std = tf.math.reduce_std(data, axis=0)\n",
    "    # Avoid division by zero\n",
    "    std = tf.maximum(std, 1e-8)\n",
    "    return (data - mean) / std\n",
    "\n",
    "standardized_data = standardize_zscore(raw_data)\n",
    "print(\"✅ Standardized data (μ=0, σ=1):\")\n",
    "print(standardized_data.numpy())\n",
    "print(f\"   New mean: {tf.reduce_mean(standardized_data, axis=0).numpy()}\")\n",
    "print(f\"   New std:  {tf.math.reduce_std(standardized_data, axis=0).numpy()}\")\n",
    "\n",
    "print(\"\\n3️⃣ FEATURE ENGINEERING\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "def create_polynomial_features(data):\n",
    "    \"\"\"Create polynomial and interaction features\"\"\"\n",
    "    # Original features\n",
    "    original_features = data\n",
    "    \n",
    "    # Polynomial features (squares)\n",
    "    squared_features = tf.square(data)\n",
    "    \n",
    "    # Interaction features (pairwise products)\n",
    "    # For demo, just first two columns\n",
    "    interaction = tf.expand_dims(data[:, 0] * data[:, 1], axis=1)\n",
    "    \n",
    "    # Combine all features\n",
    "    enhanced_features = tf.concat([\n",
    "        original_features,      # Original\n",
    "        squared_features,       # x²\n",
    "        interaction            # x₁ × x₂\n",
    "    ], axis=1)\n",
    "    \n",
    "    return enhanced_features\n",
    "\n",
    "enhanced_data = create_polynomial_features(raw_data)\n",
    "print(\"✅ Enhanced features:\")\n",
    "print(f\"   Original shape: {raw_data.shape}\")\n",
    "print(f\"   Enhanced shape: {enhanced_data.shape}\")\n",
    "print(\"   First sample enhanced:\")\n",
    "print(f\"     Original: {raw_data[0].numpy()}\")\n",
    "print(f\"     Enhanced: {enhanced_data[0].numpy()}\")\n",
    "\n",
    "print(\"\\n🔄 DATASET INTEGRATION\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Apply preprocessing to a dataset\n",
    "def preprocess_fn(data):\n",
    "    \"\"\"Preprocessing function for dataset.map()\"\"\"\n",
    "    return standardize_zscore(data)\n",
    "\n",
    "# Create dataset and apply preprocessing\n",
    "dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n",
    "preprocessed_dataset = dataset.map(preprocess_fn)\n",
    "\n",
    "print(\"✅ Preprocessing applied to dataset:\")\n",
    "for i, (original, processed) in enumerate(zip(dataset.take(2), preprocessed_dataset.take(2))):\n",
    "    print(f\"   Sample {i+1}:\")\n",
    "    print(f\"     Before: {original.numpy()}\")\n",
    "    print(f\"     After:  {processed.numpy()}\")\n",
    "\n",
    "print(\"\\n✅ Numerical preprocessing complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73428fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📝 3.2 Text Data Preprocessing\n",
    "print(\"📝 TEXT PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"I love machine learning and deep learning\",\n",
    "    \"TensorFlow is an amazing framework\", \n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"Deep neural networks are powerful\",\n",
    "    \"Data science and AI are the future\"\n",
    "]\n",
    "\n",
    "print(\"📋 Original texts:\")\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"   {i}. {text}\")\n",
    "\n",
    "print(\"\\n1️⃣ TOKENIZATION & VOCABULARY\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=50,           # Vocabulary size\n",
    "    oov_token=\"<OOV>\",      # Out-of-vocabulary token\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # Characters to filter\n",
    ")\n",
    "\n",
    "# Fit on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "print(f\"✅ Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(\"📚 Top 10 words in vocabulary:\")\n",
    "for word, idx in list(tokenizer.word_index.items())[:10]:\n",
    "    print(f\"     '{word}': {idx}\")\n",
    "\n",
    "print(\"\\n2️⃣ TEXT TO SEQUENCES\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"✅ Text → Sequences conversion:\")\n",
    "for i, (text, seq) in enumerate(zip(texts, sequences)):\n",
    "    print(f\"   {i+1}. '{text[:30]}...' → {seq}\")\n",
    "\n",
    "print(\"\\n3️⃣ SEQUENCE PADDING\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Pad sequences to uniform length\n",
    "max_length = 8\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=max_length, \n",
    "    padding='post',      # Pad at the end\n",
    "    truncating='post'    # Truncate at the end\n",
    ")\n",
    "\n",
    "print(f\"✅ Padded sequences (max_length={max_length}):\")\n",
    "for i, (original, padded) in enumerate(zip(sequences, padded_sequences)):\n",
    "    print(f\"   {i+1}. {original} → {padded}\")\n",
    "\n",
    "print(\"\\n4️⃣ TENSORFLOW DATASET INTEGRATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(texts)\n",
    "padded_dataset = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "# Combine text and sequences\n",
    "combined_dataset = tf.data.Dataset.zip((text_dataset, padded_dataset))\n",
    "\n",
    "print(\"✅ Text dataset created:\")\n",
    "for text, sequence in combined_dataset.take(2):\n",
    "    print(f\"   Text: {text.numpy().decode('utf-8')}\")\n",
    "    print(f\"   Sequence: {sequence.numpy()}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n5️⃣ MODERN APPROACH: TextVectorization\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# TensorFlow 2.x way (more efficient)\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=50,\n",
    "    output_sequence_length=max_length,\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "# Adapt to text data\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "print(\"✅ TextVectorization layer created\")\n",
    "print(f\"📚 Vocabulary size: {vectorizer.vocabulary_size()}\")\n",
    "\n",
    "# Apply vectorization\n",
    "vectorized_texts = vectorizer(texts)\n",
    "print(\"📋 Vectorized texts:\")\n",
    "for i, vec in enumerate(vectorized_texts.numpy()):\n",
    "    print(f\"   {i+1}. {vec}\")\n",
    "\n",
    "print(\"\\n🔄 COMPLETE TEXT PIPELINE\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create complete preprocessing pipeline\n",
    "def text_preprocessing_pipeline(text_data, max_tokens=50, max_length=8):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    \n",
    "    # Create and adapt vectorizer\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_sequence_length=max_length,\n",
    "        output_mode='int'\n",
    "    )\n",
    "    vectorizer.adapt(text_data)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    \n",
    "    # Apply vectorization\n",
    "    vectorized_dataset = dataset.map(\n",
    "        lambda x: vectorizer(x),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    return vectorized_dataset, vectorizer\n",
    "\n",
    "# Apply complete pipeline\n",
    "processed_dataset, text_vectorizer = text_preprocessing_pipeline(texts)\n",
    "\n",
    "print(\"✅ Complete pipeline applied:\")\n",
    "for i, processed_text in enumerate(processed_dataset.take(2)):\n",
    "    print(f\"   Sample {i+1}: {processed_text.numpy()}\")\n",
    "\n",
    "print(\"\\n✅ Text preprocessing complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b963f",
   "metadata": {},
   "source": [
    "# 🚀 4. Performance Optimization\n",
    "\n",
    "## ⚡ Mengapa Optimasi Penting?\n",
    "\n",
    "Data loading sering menjadi **bottleneck** dalam deep learning training. Tanpa optimasi yang tepat:\n",
    "- 🐌 Model menunggu data (GPU idle)\n",
    "- 💰 Pemborosan resource komputasi\n",
    "- ⏱️ Training time sangat lama\n",
    "\n",
    "## 🛠️ Teknik Optimasi Utama\n",
    "\n",
    "### 1. **Prefetching** 🔄\n",
    "Load data di background saat model training\n",
    "```python\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "### 2. **Caching** 💾\n",
    "Simpan preprocessed data di memory/disk\n",
    "```python\n",
    "dataset = dataset.cache()  # Memory cache\n",
    "dataset = dataset.cache('/path/to/cache')  # Disk cache\n",
    "```\n",
    "\n",
    "### 3. **Parallelization** 🔀\n",
    "Gunakan multiple cores untuk preprocessing\n",
    "```python\n",
    "dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "### 4. **Vectorization** 📊\n",
    "Batch operations lebih efisien dari single-item operations\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Urutan Optimasi yang Tepat ✅\n",
    "\n",
    "**Recommended Order:**\n",
    "1. `shuffle()` (untuk dataset kecil)\n",
    "2. `map()` (preprocessing)  \n",
    "3. `cache()` (jika memori cukup)\n",
    "4. `batch()`\n",
    "5. `prefetch()`\n",
    "\n",
    "**❌ Avoid:** Shuffle setelah batch, cache sebelum expensive operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 4.1 Performance Optimization Examples\n",
    "print(\"🚀 PERFORMANCE OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create synthetic dataset for performance testing\n",
    "def create_synthetic_data(n_samples=1000):\n",
    "    \"\"\"Create synthetic dataset for performance testing\"\"\"\n",
    "    data = tf.random.normal((n_samples, 100))  # 100 features\n",
    "    labels = tf.random.uniform((n_samples,), maxval=2, dtype=tf.int32)\n",
    "    return tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "# Expensive preprocessing simulation\n",
    "def expensive_preprocessing(features, label):\n",
    "    \"\"\"Simulate expensive preprocessing operation\"\"\"\n",
    "    # Simulate computational cost\n",
    "    processed_features = tf.nn.l2_normalize(features, axis=0)\n",
    "    processed_features = tf.math.sin(processed_features) * tf.math.cos(processed_features)\n",
    "    return processed_features, label\n",
    "\n",
    "print(\"📊 Created synthetic dataset (1000 samples, 100 features)\")\n",
    "\n",
    "print(\"\\n❌ BAD PIPELINE (Unoptimized)\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Bad pipeline - no optimization\n",
    "bad_pipeline = (create_synthetic_data()\n",
    "                .map(expensive_preprocessing)\n",
    "                .batch(32))\n",
    "\n",
    "print(\"✅ Bad pipeline structure:\")\n",
    "print(\"   data → map(expensive) → batch\")\n",
    "print(\"   Issues: No caching, no prefetching, no parallelization\")\n",
    "\n",
    "print(\"\\n✅ GOOD PIPELINE (Optimized)\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Good pipeline - fully optimized\n",
    "good_pipeline = (create_synthetic_data()\n",
    "                 .map(expensive_preprocessing, \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)  # Parallel processing\n",
    "                 .cache()                                   # Cache processed data\n",
    "                 .shuffle(buffer_size=1000)                 # Shuffle\n",
    "                 .batch(32)                                 # Batch\n",
    "                 .prefetch(tf.data.AUTOTUNE))              # Prefetch\n",
    "\n",
    "print(\"✅ Good pipeline structure:\")\n",
    "print(\"   data → map(expensive, parallel) → cache → shuffle → batch → prefetch\")\n",
    "\n",
    "print(\"\\n⚡ PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "def time_dataset(dataset, name, num_batches=10):\n",
    "    \"\"\"Time dataset iteration\"\"\"\n",
    "    print(f\"\\n🕐 Timing {name}:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(dataset.take(num_batches)):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"   Processed batch {i+1}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   ⏱️ Time: {elapsed:.2f}s ({elapsed/num_batches:.3f}s per batch)\")\n",
    "    return elapsed\n",
    "\n",
    "# Time both pipelines\n",
    "bad_time = time_dataset(bad_pipeline, \"Bad Pipeline\", 10)\n",
    "good_time = time_dataset(good_pipeline, \"Good Pipeline\", 10)\n",
    "\n",
    "improvement = (bad_time - good_time) / bad_time * 100\n",
    "print(f\"\\n🎯 PERFORMANCE IMPROVEMENT: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\n💡 OPTIMIZATION TECHNIQUES BREAKDOWN\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "print(\"1️⃣ AUTOTUNE - Automatic optimization\")\n",
    "print(\"   tf.data.AUTOTUNE automatically determines optimal values\")\n",
    "print(\"   for buffer_size, num_parallel_calls, etc.\")\n",
    "\n",
    "print(\"\\n2️⃣ PREFETCHING - Overlap computation\")\n",
    "optimized_for_prefetch = (tf.data.Dataset.range(100)\n",
    "                         .map(lambda x: tf.cast(x, tf.float32))\n",
    "                         .batch(10)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"   ✅ Prefetch added - data loading overlaps with training\")\n",
    "\n",
    "print(\"\\n3️⃣ CACHING - Avoid recomputation\")\n",
    "cached_dataset = (tf.data.Dataset.range(100)\n",
    "                 .map(lambda x: x ** 2)  # Expensive operation\n",
    "                 .cache()                # Cache results\n",
    "                 .batch(10))\n",
    "\n",
    "print(\"   ✅ Cache added - expensive operations computed once\")\n",
    "\n",
    "print(\"\\n4️⃣ PARALLEL MAP - Use multiple cores\")\n",
    "parallel_dataset = (tf.data.Dataset.range(100)\n",
    "                   .map(lambda x: tf.math.sin(tf.cast(x, tf.float32)),\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                   .batch(10))\n",
    "\n",
    "print(\"   ✅ Parallel processing - utilizes multiple CPU cores\")\n",
    "\n",
    "print(\"\\n🔧 MEMORY OPTIMIZATION TIPS\")\n",
    "print(\"-\" * 28)\n",
    "print(\"💾 For large datasets:\")\n",
    "print(\"   - Use cache() only if data fits in memory\")\n",
    "print(\"   - Consider disk caching: cache('/path/to/cache')\")\n",
    "print(\"   - Use prefetch() to overlap I/O with computation\")\n",
    "print(\"   - Batch after expensive operations\")\n",
    "\n",
    "print(\"\\n✅ Performance optimization complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9029b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 💾 5. TFRecord Format\n",
    "\n",
    "## 🎯 Mengapa TFRecord?\n",
    "\n",
    "**TFRecord** adalah binary format native TensorFlow dengan keunggulan:\n",
    "\n",
    "### ✅ Keuntungan:\n",
    "- **🚀 Performa** - Loading 2-3x lebih cepat dari CSV\n",
    "- **📦 Kompresi** - Built-in compression (GZIP, ZLIB)\n",
    "- **🔄 Efisiensi** - Optimal untuk streaming data besar\n",
    "- **🏗️ Fleksibilitas** - Mendukung data kompleks (nested, variable-length)\n",
    "- **⚡ Integrasi** - Perfect dengan tf.data pipeline\n",
    "\n",
    "### 📋 Struktur TFRecord:\n",
    "```\n",
    "TFRecord File\n",
    "├── Example 1 (Protocol Buffer)\n",
    "│   ├── Feature 1 (bytes/float/int64)\n",
    "│   ├── Feature 2 (bytes/float/int64)\n",
    "│   └── ...\n",
    "├── Example 2\n",
    "└── ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Creating & Reading TFRecord 🛠️\n",
    "\n",
    "TFRecord menggunakan **Protocol Buffers** untuk serialisasi data yang efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 5.1 TFRecord Complete Example\n",
    "print(\"💾 TFRECORD FORMAT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Helper functions for TFRecord creation\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_list_feature(values):\n",
    "    \"\"\"Returns a float_list from a list of floats.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "print(\"🔧 Helper functions created for TFRecord serialization\")\n",
    "\n",
    "print(\"\\n1️⃣ CREATING TFRECORD FILE\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Sample structured data\n",
    "samples = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'features': [1.0, 2.0, 3.0, 4.0],\n",
    "        'label': 'positive',\n",
    "        'score': 0.95\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'features': [5.0, 6.0, 7.0, 8.0],\n",
    "        'label': 'negative', \n",
    "        'score': 0.12\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'features': [9.0, 10.0, 11.0, 12.0],\n",
    "        'label': 'positive',\n",
    "        'score': 0.87\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create TFRecord file\n",
    "tfrecord_filename = 'sample_data.tfrecord'\n",
    "\n",
    "with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n",
    "    for sample in samples:\n",
    "        # Create features dictionary\n",
    "        feature_dict = {\n",
    "            'id': _int64_feature(sample['id']),\n",
    "            'features': _float_list_feature(sample['features']),\n",
    "            'label': _bytes_feature(sample['label'].encode('utf-8')),\n",
    "            'score': _float_feature(sample['score'])\n",
    "        }\n",
    "        \n",
    "        # Create example\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature_dict)\n",
    "        )\n",
    "        \n",
    "        # Write to file\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "print(f\"✅ Created TFRecord: {tfrecord_filename}\")\n",
    "print(f\"   📊 Contains {len(samples)} examples\")\n",
    "print(f\"   💾 File size: {os.path.getsize(tfrecord_filename)} bytes\")\n",
    "\n",
    "print(\"\\n2️⃣ READING TFRECORD FILE\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Define feature parsing schema\n",
    "feature_description = {\n",
    "    'id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'features': tf.io.VarLenFeature(tf.float32),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "    'score': tf.io.FixedLenFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"Parse TFRecord example\"\"\"\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Convert sparse tensor to dense\n",
    "    parsed['features'] = tf.sparse.to_dense(parsed['features'])\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "# Read TFRecord dataset\n",
    "tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_filename)\n",
    "parsed_dataset = tfrecord_dataset.map(parse_tfrecord)\n",
    "\n",
    "print(\"✅ TFRecord dataset loaded and parsed\")\n",
    "print(\"📋 Sample data:\")\n",
    "for i, record in enumerate(parsed_dataset):\n",
    "    print(f\"\\n   Example {i+1}:\")\n",
    "    print(f\"     ID: {record['id'].numpy()}\")\n",
    "    print(f\"     Features: {record['features'].numpy()}\")\n",
    "    print(f\"     Label: {record['label'].numpy().decode('utf-8')}\")\n",
    "    print(f\"     Score: {record['score'].numpy():.3f}\")\n",
    "\n",
    "print(\"\\n3️⃣ TRAINING-READY PIPELINE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def prepare_for_training(record):\n",
    "    \"\"\"Prepare parsed record for training\"\"\"\n",
    "    features = record['features']\n",
    "    # Convert string label to integer\n",
    "    label = tf.cond(\n",
    "        tf.equal(record['label'], b'positive'),\n",
    "        lambda: tf.constant(1, dtype=tf.int32),\n",
    "        lambda: tf.constant(0, dtype=tf.int32)\n",
    "    )\n",
    "    return features, label\n",
    "\n",
    "# Create training pipeline\n",
    "training_pipeline = (parsed_dataset\n",
    "                    .map(prepare_for_training)\n",
    "                    .shuffle(buffer_size=100)\n",
    "                    .batch(2)\n",
    "                    .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"✅ Training pipeline created:\")\n",
    "print(\"   TFRecord → parse → prepare → shuffle → batch → prefetch\")\n",
    "\n",
    "print(\"\\n📊 Training batches:\")\n",
    "for i, (batch_features, batch_labels) in enumerate(training_pipeline):\n",
    "    print(f\"   Batch {i+1}:\")\n",
    "    print(f\"     Features shape: {batch_features.shape}\")\n",
    "    print(f\"     Labels: {batch_labels.numpy()}\")\n",
    "    print(f\"     Features: {batch_features.numpy()}\")\n",
    "\n",
    "print(\"\\n4️⃣ COMPRESSION BENEFITS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create compressed TFRecord\n",
    "compressed_filename = 'sample_data_compressed.tfrecord'\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "\n",
    "with tf.io.TFRecordWriter(compressed_filename, options=options) as writer:\n",
    "    for sample in samples:\n",
    "        feature_dict = {\n",
    "            'id': _int64_feature(sample['id']),\n",
    "            'features': _float_list_feature(sample['features']),\n",
    "            'label': _bytes_feature(sample['label'].encode('utf-8')),\n",
    "            'score': _float_feature(sample['score'])\n",
    "        }\n",
    "        \n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(feature=feature_dict)\n",
    "        )\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "# Compare file sizes\n",
    "original_size = os.path.getsize(tfrecord_filename)\n",
    "compressed_size = os.path.getsize(compressed_filename)\n",
    "compression_ratio = (1 - compressed_size / original_size) * 100\n",
    "\n",
    "print(f\"✅ Compression results:\")\n",
    "print(f\"   📄 Original: {original_size} bytes\")\n",
    "print(f\"   📦 Compressed: {compressed_size} bytes\")\n",
    "print(f\"   💾 Compression: {compression_ratio:.1f}% reduction\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tfrecord_filename)\n",
    "os.unlink(compressed_filename)\n",
    "print(f\"\\n🧹 Files cleaned up\")\n",
    "\n",
    "print(\"\\n✅ TFRecord examples complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b9a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🤝 6. Integrasi dengan Keras\n",
    "\n",
    "## 🔗 Seamless Integration\n",
    "\n",
    "tf.data Dataset dapat langsung digunakan dengan Keras:\n",
    "\n",
    "```python\n",
    "# Dataset training\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                .shuffle(1000)\n",
    "                .batch(32)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Train model\n",
    "model.fit(train_dataset, epochs=10)\n",
    "```\n",
    "\n",
    "### 🎯 Keuntungan:\n",
    "- ✅ No need untuk manual batching\n",
    "- ✅ Automatic prefetching\n",
    "- ✅ Memory efficient untuk dataset besar\n",
    "- ✅ Reproducible dengan random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤝 6.1 Keras Integration Example\n",
    "print(\"🤝 KERAS INTEGRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create synthetic dataset for demo\n",
    "print(\"📊 Creating synthetic classification dataset...\")\n",
    "n_samples, n_features, n_classes = 1000, 20, 3\n",
    "\n",
    "# Generate synthetic data\n",
    "X_data = tf.random.normal((n_samples, n_features))\n",
    "y_data = tf.random.uniform((n_samples,), maxval=n_classes, dtype=tf.int32)\n",
    "\n",
    "print(f\"✅ Dataset created: {n_samples} samples, {n_features} features, {n_classes} classes\")\n",
    "\n",
    "# Split data (80/20)\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_test = X_data[:split_idx], X_data[split_idx:]\n",
    "y_train, y_test = y_data[:split_idx], y_data[split_idx:]\n",
    "\n",
    "print(f\"📋 Train: {len(X_train)} samples\")\n",
    "print(f\"📋 Test:  {len(X_test)} samples\")\n",
    "\n",
    "print(\"\\n1️⃣ CREATE OPTIMIZED DATASETS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Training dataset with full pipeline\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "                .shuffle(buffer_size=1000, seed=42)\n",
    "                .batch(32)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Validation dataset (no shuffle needed)\n",
    "val_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "              .batch(32)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"✅ Datasets created:\")\n",
    "print(\"   🔄 Train: shuffle → batch → prefetch\")\n",
    "print(\"   📊 Val:   batch → prefetch\")\n",
    "\n",
    "print(\"\\n2️⃣ CREATE SIMPLE MODEL\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Simple neural network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"✅ Model created:\")\n",
    "print(f\"   📊 Architecture: {n_features} → 64 → 32 → {n_classes}\")\n",
    "print(\"   🎯 Task: Multi-class classification\")\n",
    "\n",
    "print(\"\\n3️⃣ TRAIN WITH tf.data\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Train model with tf.data datasets\n",
    "print(\"🚀 Training model...\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "\n",
    "print(\"\\n4️⃣ EVALUATE PERFORMANCE\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"📊 Test Results:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n5️⃣ BATCH PREDICTION\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Make predictions on a batch\n",
    "sample_batch = next(iter(val_dataset))\n",
    "batch_features, batch_labels = sample_batch\n",
    "\n",
    "predictions = model.predict(batch_features, verbose=0)\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"🔮 Batch predictions:\")\n",
    "print(f\"   Batch size: {len(batch_labels)}\")\n",
    "print(f\"   True labels: {batch_labels.numpy()[:5]}\")\n",
    "print(f\"   Predictions: {predicted_classes.numpy()[:5]}\")\n",
    "\n",
    "# Calculate batch accuracy\n",
    "batch_accuracy = tf.reduce_mean(\n",
    "    tf.cast(tf.equal(predicted_classes, batch_labels), tf.float32)\n",
    ")\n",
    "print(f\"   Batch accuracy: {batch_accuracy.numpy():.4f}\")\n",
    "\n",
    "print(\"\\n💡 KEY BENEFITS OF tf.data + Keras:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"✅ Automatic batching and prefetching\")\n",
    "print(\"✅ Memory efficient for large datasets\") \n",
    "print(\"✅ No need to load entire dataset in memory\")\n",
    "print(\"✅ Seamless integration with model.fit()\")\n",
    "print(\"✅ Support for validation_data parameter\")\n",
    "print(\"✅ Built-in support for steps_per_epoch\")\n",
    "\n",
    "print(\"\\n✅ Keras integration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac24d43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 💡 7. Best Practices & Tips\n",
    "\n",
    "## 🎯 Performance Best Practices\n",
    "\n",
    "### 1. **Pipeline Order** 🔄\n",
    "```python\n",
    "# ✅ OPTIMAL ORDER:\n",
    "dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
    "           .shuffle(buffer_size)     # 1. Shuffle first (if needed)\n",
    "           .map(preprocess_fn)       # 2. Apply transformations\n",
    "           .cache()                  # 3. Cache processed data\n",
    "           .batch(batch_size)        # 4. Batch data\n",
    "           .prefetch(AUTOTUNE))      # 5. Prefetch last\n",
    "```\n",
    "\n",
    "### 2. **Memory Management** 💾\n",
    "- Use `.cache()` hanya jika data muat di memory\n",
    "- Gunakan `.cache('/path/to/disk')` untuk disk caching\n",
    "- Hindari shuffle pada dataset sangat besar\n",
    "\n",
    "### 3. **Parallelization** ⚡\n",
    "- Gunakan `num_parallel_calls=tf.data.AUTOTUNE`\n",
    "- Biarkan TensorFlow optimize secara otomatis\n",
    "- Monitor CPU utilization\n",
    "\n",
    "---\n",
    "\n",
    "## 🚨 Common Pitfalls\n",
    "\n",
    "### ❌ Don't Do This:\n",
    "```python\n",
    "# BAD: Shuffle after batch\n",
    "dataset.batch(32).shuffle(1000)  \n",
    "\n",
    "# BAD: Cache before expensive operations  \n",
    "dataset.cache().map(expensive_fn)\n",
    "\n",
    "# BAD: No prefetching\n",
    "dataset.batch(32)  # Missing prefetch()\n",
    "```\n",
    "\n",
    "### ✅ Do This Instead:\n",
    "```python\n",
    "# GOOD: Proper order\n",
    "dataset.shuffle(1000).map(expensive_fn).cache().batch(32).prefetch(AUTOTUNE)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Debugging Tips\n",
    "\n",
    "### 1. **Inspect Dataset**\n",
    "```python\n",
    "# Check first few samples\n",
    "for sample in dataset.take(3):\n",
    "    print(sample)\n",
    "\n",
    "# Check shapes\n",
    "print(dataset.element_spec)\n",
    "```\n",
    "\n",
    "### 2. **Performance Profiling**\n",
    "```python\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for batch in dataset.take(100):\n",
    "    pass\n",
    "print(f\"Time: {time.time() - start:.2f}s\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59042899",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 Chapter Summary\n",
    "\n",
    "## 📚 Apa yang Telah Dipelajari\n",
    "\n",
    "Dalam Chapter 13 ini, kita telah mempelajari:\n",
    "\n",
    "### 🔰 1. Data API Fundamentals\n",
    "- ✅ Konsep tf.data.Dataset\n",
    "- ✅ Membuat dataset dari berbagai sumber\n",
    "- ✅ Transformasi dasar (map, filter, batch, shuffle)\n",
    "\n",
    "### 📁 2. Data Loading\n",
    "- ✅ Loading dari CSV files  \n",
    "- ✅ TextLineDataset untuk custom parsing\n",
    "- ✅ Binary dan structured data formats\n",
    "\n",
    "### 🔧 3. Preprocessing\n",
    "- ✅ Numerical preprocessing (normalization, standardization)\n",
    "- ✅ Text preprocessing (tokenization, padding, vectorization)\n",
    "- ✅ Feature engineering techniques\n",
    "\n",
    "### 🚀 4. Performance Optimization\n",
    "- ✅ Prefetching dan caching\n",
    "- ✅ Parallel processing\n",
    "- ✅ Optimal pipeline ordering\n",
    "- ✅ Memory management\n",
    "\n",
    "### 💾 5. TFRecord Format\n",
    "- ✅ Creating dan reading TFRecord files\n",
    "- ✅ Protocol buffers serialization\n",
    "- ✅ Compression benefits\n",
    "\n",
    "### 🤝 6. Keras Integration\n",
    "- ✅ Seamless integration dengan model.fit()\n",
    "- ✅ Training dan validation pipelines\n",
    "- ✅ Batch prediction workflow\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Key Takeaways\n",
    "\n",
    "### 💡 **The Golden Pipeline:**\n",
    "```python\n",
    "optimal_pipeline = (\n",
    "    tf.data.Dataset.from_tensor_slices(data)\n",
    "    .shuffle(buffer_size)\n",
    "    .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)  \n",
    "    .cache()\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "```\n",
    "\n",
    "### 🚀 **Performance Mantra:**\n",
    "> \"Shuffle → Map → Cache → Batch → Prefetch\"\n",
    "\n",
    "### 💾 **Format Choice:**\n",
    "- 📊 **CSV**: Prototyping dan dataset kecil\n",
    "- 💾 **TFRecord**: Production dan dataset besar  \n",
    "- 🔄 **tf.data**: Always untuk training pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 🔮 Next Steps\n",
    "\n",
    "Setelah menguasai Chapter 13, Anda siap untuk:\n",
    "\n",
    "- 🧠 **Chapter 14**: Convolutional Neural Networks\n",
    "- 🎯 **Chapter 15**: Processing Sequences using RNNs  \n",
    "- 🚀 **Advanced Topics**: Custom training loops, distributed training\n",
    "- 💼 **Real Projects**: Apply tf.data pada dataset real-world\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Congratulations!\n",
    "\n",
    "🎉 **Selamat!** Anda telah menguasai TensorFlow Data API - foundational skill untuk deep learning yang scalable dan efisien!\n",
    "\n",
    "**Remember**: \n",
    "> *\"Good data pipelines are the backbone of successful deep learning projects\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 📖 Resources for Further Learning\n",
    "\n",
    "- 📚 [TensorFlow Data Guide](https://www.tensorflow.org/guide/data)\n",
    "- 🎥 [tf.data Best Practices](https://www.tensorflow.org/guide/data_performance)\n",
    "- 💻 [TensorFlow Datasets](https://www.tensorflow.org/datasets)\n",
    "- 🔬 [Advanced tf.data Techniques](https://www.tensorflow.org/guide/data_performance)\n",
    "\n",
    "**Happy Learning! 🚀**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe980cc",
   "metadata": {},
   "source": [
    "## 6. Integration dengan Keras 🤝\n",
    "\n",
    "### 6.1 Dataset untuk Training\n",
    "\n",
    "tf.data.Dataset dapat langsung digunakan dengan:\n",
    "- `model.fit()` untuk training\n",
    "- `model.evaluate()` untuk evaluation  \n",
    "- `model.predict()` untuk prediction\n",
    "\n",
    "### 6.2 Preprocessing Layers\n",
    "\n",
    "Keras juga menyediakan preprocessing layers yang dapat diintegrasikan dalam model:\n",
    "- `tf.keras.layers.Normalization`\n",
    "- `tf.keras.layers.StringLookup`  \n",
    "- `tf.keras.layers.TextVectorization`\n",
    "- `tf.keras.layers.CategoryEncoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870867a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤝 Keras Integration Examples\n",
    "print(\"=== KERAS INTEGRATION ===\")\n",
    "\n",
    "# 1. Create sample dataset\n",
    "print(\"=== PREPARING DATASET FOR KERAS ===\")\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X_data = np.random.randn(1000, 4)  # 1000 samples, 4 features\n",
    "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int)  # Binary classification\n",
    "\n",
    "print(f\"Data shape: {X_data.shape}\")\n",
    "print(f\"Labels shape: {y_data.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_data)}\")\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessed_dataset = (dataset\n",
    "                       .shuffle(buffer_size=1000)\n",
    "                       .batch(32)\n",
    "                       .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Split into train/validation\n",
    "train_size = int(0.8 * len(X_data))\n",
    "train_dataset = preprocessed_dataset.take(train_size // 32)\n",
    "val_dataset = preprocessed_dataset.skip(train_size // 32)\n",
    "\n",
    "print(f\"Training batches: ~{train_size // 32}\")\n",
    "print(f\"Validation batches: ~{(len(X_data) - train_size) // 32}\")\n",
    "\n",
    "# 2. Create and train Keras model\n",
    "print(\"\\n=== KERAS MODEL WITH tf.data ===\")\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Train with tf.data.Dataset\n",
    "print(\"\\n=== TRAINING WITH tf.data ===\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Training completed!\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# 3. Demonstrate preprocessing layers\n",
    "print(\"\\n=== PREPROCESSING LAYERS ===\")\n",
    "\n",
    "# Example with text data\n",
    "text_data = [\n",
    "    \"I love machine learning\",\n",
    "    \"TensorFlow is great\", \n",
    "    \"Deep learning rocks\",\n",
    "    \"AI is the future\"\n",
    "]\n",
    "\n",
    "# Create text vectorization layer\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=10\n",
    ")\n",
    "\n",
    "# Adapt to text data\n",
    "text_vectorizer.adapt(text_data)\n",
    "\n",
    "print(\"Text vectorization example:\")\n",
    "print(\"Original texts:\", text_data[:2])\n",
    "vectorized = text_vectorizer(text_data[:2])\n",
    "print(\"Vectorized:\", vectorized.numpy())\n",
    "\n",
    "# Example with normalization layer\n",
    "print(\"\\n=== NORMALIZATION LAYER ===\")\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "\n",
    "# Adapt to data\n",
    "sample_data = np.random.randn(100, 3) * 10 + 5  # Mean≈5, Std≈10\n",
    "normalizer.adapt(sample_data)\n",
    "\n",
    "print(\"Original data stats:\")\n",
    "print(f\"  Mean: {np.mean(sample_data, axis=0)}\")\n",
    "print(f\"  Std: {np.std(sample_data, axis=0)}\")\n",
    "\n",
    "normalized = normalizer(sample_data[:5])\n",
    "print(\"Normalized data (first 5 samples):\")\n",
    "print(normalized.numpy())\n",
    "print(f\"Normalized mean: {np.mean(normalized.numpy(), axis=0)}\")\n",
    "print(f\"Normalized std: {np.std(normalized.numpy(), axis=0)}\")\n",
    "\n",
    "print(\"\\n✅ Keras integration examples completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61f96e",
   "metadata": {},
   "source": [
    "## 7. Best Practices & Summary 📋\n",
    "\n",
    "### 7.1 Data Pipeline Best Practices\n",
    "\n",
    "1. **Urutan Optimal Transformasi**:\n",
    "   ```python\n",
    "   dataset = (tf.data.Dataset.from_generator(...)\n",
    "              .map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "              .cache()  # Cache after expensive operations\n",
    "              .shuffle(buffer_size)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(AUTOTUNE))\n",
    "   ```\n",
    "\n",
    "2. **Performance Tips**:\n",
    "   - Gunakan `num_parallel_calls=AUTOTUNE` untuk operasi map\n",
    "   - Implementasikan `prefetch()` di akhir pipeline\n",
    "   - Cache dataset setelah operasi mahal, sebelum shuffle\n",
    "   - Gunakan TFRecord untuk dataset besar\n",
    "   - Batch sebelum expensive transformations jika memungkinkan\n",
    "\n",
    "3. **Memory Management**:\n",
    "   - Hindari `.cache()` untuk dataset yang terlalu besar\n",
    "   - Gunakan generator untuk data yang tidak muat di memory\n",
    "   - Pertimbangkan `.cache(filename)` untuk cache ke disk\n",
    "\n",
    "### 7.2 Common Patterns\n",
    "\n",
    "- **Image Data**: `map(decode_image) → cache() → shuffle() → batch() → prefetch()`\n",
    "- **Text Data**: `map(tokenize) → padded_batch() → prefetch()`\n",
    "- **Structured Data**: `map(normalize) → batch() → prefetch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Chapter 13 Summary & Best Practices\n",
    "print(\"=== CHAPTER 13 SUMMARY ===\")\n",
    "print(\"🎯 Loading and Preprocessing Data with TensorFlow\")\n",
    "print()\n",
    "\n",
    "print(\"📚 KEY CONCEPTS LEARNED:\")\n",
    "concepts = [\n",
    "    \"1. tf.data.Dataset fundamentals and creation methods\",\n",
    "    \"2. Dataset transformations (map, filter, batch, shuffle)\",\n",
    "    \"3. Loading data from various sources (CSV, TFRecord, etc.)\",\n",
    "    \"4. Data preprocessing and feature engineering\",\n",
    "    \"5. Text preprocessing and vectorization\", \n",
    "    \"6. Performance optimization (prefetch, cache, parallel)\",\n",
    "    \"7. TFRecord format for efficient storage\",\n",
    "    \"8. Integration with Keras models and preprocessing layers\"\n",
    "]\n",
    "\n",
    "for concept in concepts:\n",
    "    print(f\"   ✅ {concept}\")\n",
    "\n",
    "print(\"\\n🚀 PERFORMANCE OPTIMIZATION CHECKLIST:\")\n",
    "optimizations = [\n",
    "    \"Use num_parallel_calls=AUTOTUNE for map operations\",\n",
    "    \"Implement prefetch(AUTOTUNE) at end of pipeline\",\n",
    "    \"Cache expensive operations with cache()\",\n",
    "    \"Shuffle with appropriate buffer_size\",\n",
    "    \"Use TFRecord for large datasets\",\n",
    "    \"Batch data for efficient processing\",\n",
    "    \"Consider preprocessing layers in model\"\n",
    "]\n",
    "\n",
    "for opt in optimizations:\n",
    "    print(f\"   🔧 {opt}\")\n",
    "\n",
    "print(\"\\n📊 COMMON DATA PIPELINE PATTERN:\")\n",
    "print(\"\"\"\n",
    "   dataset = (tf.data.Dataset.from_source(...)\n",
    "              .map(preprocessing_fn, num_parallel_calls=AUTOTUNE)\n",
    "              .cache()\n",
    "              .shuffle(buffer_size=1000)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(AUTOTUNE))\n",
    "\"\"\")\n",
    "\n",
    "print(\"🎉 NEXT STEPS:\")\n",
    "next_steps = [\n",
    "    \"Practice with real-world datasets\",\n",
    "    \"Experiment with different preprocessing techniques\",\n",
    "    \"Benchmark pipeline performance\",\n",
    "    \"Explore advanced tf.data features\",\n",
    "    \"Integrate with complex model architectures\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   📈 {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎊 CONGRATULATIONS! You've completed Chapter 13!\")\n",
    "print(\"   You now understand TensorFlow Data API and\")\n",
    "print(\"   can build efficient data pipelines for deep learning!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
