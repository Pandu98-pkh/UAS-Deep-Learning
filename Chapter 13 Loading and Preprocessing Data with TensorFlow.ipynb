{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pandu98-pkh/UAS-Deep-Learning/blob/main/Chapter%2013%20Loading%20and%20Preprocessing%20Data%20with%20TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5a7309",
      "metadata": {
        "id": "4c5a7309"
      },
      "source": [
        "# üìä Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "# Memuat dan Memproses Data dengan TensorFlow\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Tujuan Pembelajaran\n",
        "\n",
        "Setelah menyelesaikan chapter ini, Anda akan mampu:\n",
        "- ‚úÖ Memahami konsep TensorFlow Data API\n",
        "- ‚úÖ Membuat dan memanipulasi dataset dengan tf.data\n",
        "- ‚úÖ Memuat data dari berbagai sumber (CSV, TFRecord, dll)\n",
        "- ‚úÖ Melakukan preprocessing data yang efisien\n",
        "- ‚úÖ Mengoptimalkan performa data pipeline\n",
        "- ‚úÖ Mengintegrasikan data pipeline dengan model Keras\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Outline Chapter\n",
        "\n",
        "1. **Data API - Konsep Dasar** üî∞\n",
        "2. **Loading Data dari Files** üìÅ\n",
        "3. **Data Preprocessing & Feature Engineering** üîß\n",
        "4. **Performance Optimization** üöÄ\n",
        "5. **TFRecord Format** üíæ\n",
        "6. **Integrasi dengan Keras** ü§ù\n",
        "7. **Best Practices & Tips** üí°\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Pengantar\n",
        "\n",
        "Chapter 13 ini membahas cara memuat dan memproses data secara efisien untuk sistem Deep Learning. Sejauh ini kita hanya menggunakan dataset yang muat di memori, namun sistem Deep Learning sering dilatih dengan dataset sangat besar yang tidak muat di RAM.\n",
        "\n",
        "**TensorFlow Data API** menyediakan solusi untuk:\n",
        "- üìä Memuat dataset besar secara efisien\n",
        "- ‚ö° Memproses data dengan multithreading, queuing, batching, dan prefetching\n",
        "- üîó Integrasi seamless dengan tf.keras\n",
        "- üéØ Pipeline data yang scalable dan reproducible\n",
        "\n",
        "**Dataset yang didukung:**\n",
        "- üìÑ File teks (CSV, JSON)  \n",
        "- üóÇÔ∏è File binary dengan record berukuran tetap\n",
        "- üíæ TFRecord format (record berukuran variabel)\n",
        "- üóÉÔ∏è Database SQL\n",
        "- üåê Berbagai sumber data lainnya melalui ekstensi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c37eea0c",
      "metadata": {
        "id": "c37eea0c",
        "outputId": "1e2fb984-1d23-47c7-bfbf-5025215cb112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ CHAPTER 13: Loading and Preprocessing Data with TensorFlow\n",
            "============================================================\n",
            "üì¶ TensorFlow version: 2.18.0\n",
            "üì¶ NumPy version: 2.0.2\n",
            "üì¶ Pandas version: 2.2.2\n",
            "üéÆ GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "‚úÖ Setup complete! Ready to explore TensorFlow Data API\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# üîß Setup & Import Libraries\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ CHAPTER 13: Loading and Preprocessing Data with TensorFlow\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Display versions\n",
        "print(f\"üì¶ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"üì¶ NumPy version: {np.__version__}\")\n",
        "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
        "\n",
        "# Check GPU availability\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"üéÆ GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "else:\n",
        "    print(\"üíª Running on CPU\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete! Ready to explore TensorFlow Data API\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119f0572",
      "metadata": {
        "id": "119f0572"
      },
      "source": [
        "---\n",
        "\n",
        "# üî∞ 1. Data API - Konsep Dasar\n",
        "\n",
        "## üìö Pengantar tf.data\n",
        "\n",
        "**tf.data API** adalah inti dari data loading di TensorFlow yang menyediakan:\n",
        "\n",
        "- üîÑ **Dataset**: Abstraksi untuk sequence of elements\n",
        "- ‚ö° **Transformations**: Map, filter, batch, shuffle, dll\n",
        "- üöÄ **Performance**: Prefetching, caching, parallelization\n",
        "- üîó **Integration**: Seamless dengan tf.keras\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Membuat Dataset Sederhana üìä\n",
        "\n",
        "Data API berpusat pada konsep **'Dataset'** yang merepresentasikan urutan item data. Dataset biasanya membaca data dari disk secara bertahap, namun untuk kesederhanaan kita mulai dengan dataset di RAM menggunakan `tf.data.Dataset.from_tensor_slices()`\n",
        "\n",
        "### Metode Pembuatan Dataset:\n",
        "- `tf.data.Dataset.from_tensor_slices()` - dari array/tensor\n",
        "- `tf.data.Dataset.from_tensors()` - dari single tensor\n",
        "- `tf.data.Dataset.range()` - range values\n",
        "- `tf.data.Dataset.from_generator()` - dari generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9861b812",
      "metadata": {
        "id": "9861b812",
        "outputId": "a1f80951-cedc-4b59-c607-28ae26ab07fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∞ BASIC DATASET CREATION\n",
            "==================================================\n",
            "\n",
            "üìã 1. Dataset dari tensor slices:\n",
            "Dataset dari range(10): [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9)]\n",
            "\n",
            "üìã 2. Dataset dari multiple arrays:\n",
            "Dataset dari (X, Y):\n",
            "  X: 0, Y: 10\n",
            "  X: 1, Y: 11\n",
            "  X: 2, Y: 12\n",
            "  X: 3, Y: 13\n",
            "  X: 4, Y: 14\n",
            "\n",
            "üìã 3. Dataset dengan dictionary:\n",
            "Dataset dictionary:\n",
            "  Sample 1:\n",
            "    Features shape: (3,)\n",
            "    Label: 0\n",
            "  Sample 2:\n",
            "    Features shape: (3,)\n",
            "    Label: 1\n",
            "  Sample 3:\n",
            "    Features shape: (3,)\n",
            "    Label: 2\n",
            "  Sample 4:\n",
            "    Features shape: (3,)\n",
            "    Label: 3\n",
            "  Sample 5:\n",
            "    Features shape: (3,)\n",
            "    Label: 4\n",
            "\n",
            "üìã 4. Dataset dari single tensor:\n",
            "Dataset dari single tensor:\n",
            "  Shape: (3, 2), Values:\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "\n",
            "üìã 5. Dataset range:\n",
            "Range dataset: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
            "\n",
            "‚úÖ Basic dataset creation complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Dataset awal untuk demo\n",
        "print(\"üî∞ BASIC DATASET CREATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Dataset dari tensor slices (paling umum)\n",
        "print(\"\\nüìã 1. Dataset dari tensor slices:\")\n",
        "X = tf.range(10)\n",
        "# Create a dataset from the tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "# Now you can iterate over the dataset\n",
        "print(f\"Dataset dari range(10): {list(dataset.as_numpy_iterator())}\")\n",
        "\n",
        "# 2. Dataset dari multiple arrays\n",
        "print(\"\\nüìã 2. Dataset dari multiple arrays:\")\n",
        "X = tf.range(5)\n",
        "Y = tf.range(10, 15)\n",
        "dataset_xy = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "print(\"Dataset dari (X, Y):\")\n",
        "for x, y in dataset_xy:\n",
        "    print(f\"  X: {x.numpy()}, Y: {y.numpy()}\")\n",
        "\n",
        "# 3. Dataset dengan dictionary (sangat berguna!)\n",
        "print(\"\\nüìã 3. Dataset dengan dictionary:\")\n",
        "dataset_dict = tf.data.Dataset.from_tensor_slices({\n",
        "    \"features\": tf.random.normal((5, 3)),\n",
        "    \"labels\": tf.range(5)\n",
        "})\n",
        "print(\"Dataset dictionary:\")\n",
        "for i, item in enumerate(dataset_dict):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    Features shape: {item['features'].shape}\")\n",
        "    print(f\"    Label: {item['labels'].numpy()}\")\n",
        "\n",
        "# 4. Dataset dari single tensor\n",
        "print(\"\\nüìã 4. Dataset dari single tensor:\")\n",
        "tensor_data = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
        "dataset_tensor = tf.data.Dataset.from_tensors(tensor_data)\n",
        "print(\"Dataset dari single tensor:\")\n",
        "for item in dataset_tensor:\n",
        "    print(f\"  Shape: {item.shape}, Values:\\n{item.numpy()}\")\n",
        "\n",
        "# 5. Dataset range\n",
        "print(\"\\nüìã 5. Dataset range:\")\n",
        "range_dataset = tf.data.Dataset.range(5)\n",
        "print(f\"Range dataset: {list(range_dataset.as_numpy_iterator())}\")\n",
        "\n",
        "print(\"\\n‚úÖ Basic dataset creation complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a284383f",
      "metadata": {
        "id": "a284383f"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.2 Transformasi Dataset üîÑ\n",
        "\n",
        "Dataset memiliki berbagai **method transformasi** yang dapat di-chain untuk membangun data pipeline yang powerful:\n",
        "\n",
        "### üõ†Ô∏è Core Transformations:\n",
        "- **`map(func)`** - Menerapkan fungsi ke setiap elemen  \n",
        "- **`filter(predicate)`** - Menyaring elemen berdasarkan kondisi\n",
        "- **`batch(batch_size)`** - Mengelompokkan elemen dalam batch\n",
        "- **`shuffle(buffer_size)`** - Mengacak urutan elemen\n",
        "- **`repeat(count)`** - Mengulang dataset\n",
        "- **`take(count)`** - Mengambil n elemen pertama\n",
        "- **`skip(count)`** - Melewati n elemen pertama\n",
        "- **`cache(filename)`** - Cache data di memory/disk\n",
        "- **`prefetch(buffer_size)`** - Load data di background\n",
        "\n",
        "### üîó Method Chaining:\n",
        "Transformasi dapat di-chain menggunakan fluent interface:\n",
        "```python\n",
        "dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
        "           .map(preprocess_func)\n",
        "           .filter(lambda x: x > 0)\n",
        "           .shuffle(1000)\n",
        "           .batch(32)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ef659b78",
      "metadata": {
        "id": "ef659b78",
        "outputId": "755c0138-4593-4d02-f5b9-3e2487b836a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∞ BASIC DATASET CREATION\n",
            "==================================================\n",
            "\n",
            "üìã 1. Dataset dari tensor slices:\n",
            "Dataset dari range(10): [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9)]\n",
            "\n",
            "üìã 2. Dataset dari multiple arrays:\n",
            "Dataset dari (X, Y):\n",
            "  X: 0, Y: 10\n",
            "  X: 1, Y: 11\n",
            "  X: 2, Y: 12\n",
            "  X: 3, Y: 13\n",
            "  X: 4, Y: 14\n",
            "\n",
            "üìã 3. Dataset dengan dictionary:\n",
            "Dataset dictionary:\n",
            "  Sample 1:\n",
            "    Features shape: (3,)\n",
            "    Label: 0\n",
            "  Sample 2:\n",
            "    Features shape: (3,)\n",
            "    Label: 1\n",
            "  Sample 3:\n",
            "    Features shape: (3,)\n",
            "    Label: 2\n",
            "  Sample 4:\n",
            "    Features shape: (3,)\n",
            "    Label: 3\n",
            "  Sample 5:\n",
            "    Features shape: (3,)\n",
            "    Label: 4\n",
            "\n",
            "üìã 4. Dataset dari single tensor:\n",
            "Dataset dari single tensor:\n",
            "  Shape: (3, 2), Values:\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "\n",
            "üìã 5. Dataset range:\n",
            "Range dataset: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
            "\n",
            "‚úÖ Basic dataset creation complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üìä 1.1 Membuat Dataset Sederhana dari Array\n",
        "print(\"üî∞ BASIC DATASET CREATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Dataset dari tensor slices (paling umum)\n",
        "print(\"\\nüìã 1. Dataset dari tensor slices:\")\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(f\"Dataset dari range(10): {list(dataset.as_numpy_iterator())}\")\n",
        "\n",
        "# 2. Dataset dari multiple arrays\n",
        "print(\"\\nüìã 2. Dataset dari multiple arrays:\")\n",
        "X = tf.range(5)\n",
        "Y = tf.range(10, 15)\n",
        "dataset_xy = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "print(\"Dataset dari (X, Y):\")\n",
        "for x, y in dataset_xy:\n",
        "    print(f\"  X: {x.numpy()}, Y: {y.numpy()}\")\n",
        "\n",
        "# 3. Dataset dengan dictionary (sangat berguna!)\n",
        "print(\"\\nüìã 3. Dataset dengan dictionary:\")\n",
        "dataset_dict = tf.data.Dataset.from_tensor_slices({\n",
        "    \"features\": tf.random.normal((5, 3)),\n",
        "    \"labels\": tf.range(5)\n",
        "})\n",
        "print(\"Dataset dictionary:\")\n",
        "for i, item in enumerate(dataset_dict):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    Features shape: {item['features'].shape}\")\n",
        "    print(f\"    Label: {item['labels'].numpy()}\")\n",
        "\n",
        "# 4. Dataset dari single tensor\n",
        "print(\"\\nüìã 4. Dataset dari single tensor:\")\n",
        "tensor_data = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
        "dataset_tensor = tf.data.Dataset.from_tensors(tensor_data)\n",
        "print(\"Dataset dari single tensor:\")\n",
        "for item in dataset_tensor:\n",
        "    print(f\"  Shape: {item.shape}, Values:\\n{item.numpy()}\")\n",
        "\n",
        "# 5. Dataset range\n",
        "print(\"\\nüìã 5. Dataset range:\")\n",
        "range_dataset = tf.data.Dataset.range(5)\n",
        "print(f\"Range dataset: {list(range_dataset.as_numpy_iterator())}\")\n",
        "\n",
        "print(\"\\n‚úÖ Basic dataset creation complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "185daf88",
      "metadata": {
        "id": "185daf88"
      },
      "source": [
        "---\n",
        "\n",
        "# üìÅ 2. Loading Data dari Files\n",
        "\n",
        "## üóÇÔ∏è Sumber Data yang Didukung\n",
        "\n",
        "TensorFlow Data API mendukung berbagai format file:\n",
        "\n",
        "### üìä Structured Data:\n",
        "- **CSV Files** - `tf.data.experimental.make_csv_dataset()`\n",
        "- **JSON Files** - Custom parsing dengan `TextLineDataset`\n",
        "- **Parquet Files** - Via TensorFlow I/O\n",
        "\n",
        "### üíæ Binary Data:\n",
        "- **TFRecord** - `tf.data.TFRecordDataset()` (format native TF)\n",
        "- **Fixed-length records** - `tf.data.FixedLengthRecordDataset()`\n",
        "- **Raw binary** - `tf.data.RawRecordDataset()`\n",
        "\n",
        "### üìÑ Text Data:\n",
        "- **Text files** - `tf.data.TextLineDataset()`\n",
        "- **Image files** - `tf.data.Dataset.list_files()` + `tf.io.read_file()`\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 CSV Files üìä\n",
        "\n",
        "CSV adalah format paling umum untuk structured data. TensorFlow menyediakan 2 pendekatan:\n",
        "1. **High-level**: `make_csv_dataset()` - otomatis parsing\n",
        "2. **Low-level**: `TextLineDataset()` - kontrol manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3ec4926f",
      "metadata": {
        "id": "3ec4926f",
        "outputId": "7ce680ab-9259-4be2-e0d1-801907f2a11f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ LOADING CSV DATA\n",
            "==================================================\n",
            "üìù Created sample CSV: tmpfss__2um.csv\n",
            "\n",
            "üéØ METHOD 1: make_csv_dataset (Recommended)\n",
            "----------------------------------------\n",
            "‚úÖ Dataset created with automatic type inference\n",
            "üìã Sample data:\n",
            "\n",
            "   Batch 1:\n",
            "   Features: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'population', 'median_income']\n",
            "     longitude: [-122.23 -122.22]\n",
            "     latitude: [37.88 37.86]\n",
            "     housing_median_age: [41. 21.]\n",
            "     total_rooms: [ 880. 1106.]\n",
            "     population: [ 322. 2401.]\n",
            "     median_income: [8.3252 8.3014]\n",
            "   Labels (price): [452600. 358500.]\n",
            "\n",
            "   Batch 2:\n",
            "   Features: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'population', 'median_income']\n",
            "     longitude: [-122.24 -122.25]\n",
            "     latitude: [37.85 37.85]\n",
            "     housing_median_age: [52. 52.]\n",
            "     total_rooms: [1467. 1274.]\n",
            "     population: [496. 558.]\n",
            "     median_income: [7.2574 5.6431]\n",
            "   Labels (price): [352100. 341300.]\n",
            "\n",
            "üîß METHOD 2: TextLineDataset (Manual Control)\n",
            "----------------------------------------\n",
            "‚úÖ Manual parsing complete\n",
            "üìã Parsed data samples:\n",
            "   Sample 1: Features=[-122.23     37.88     41.      880.      322.        8.3252], Price=452600.0\n",
            "   Sample 2: Features=[-122.22     37.86     21.     1106.     2401.        8.3014], Price=358500.0\n",
            "\n",
            "üîÑ PIPELINE-READY DATASET:\n",
            "-------------------------\n",
            "‚úÖ Training pipeline: shuffle ‚Üí batch ‚Üí prefetch\n",
            "   Batch shape: Features (2, 6), Labels (2,)\n",
            "\n",
            "üßπ Cleaned up temporary file\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üìä 2.1 Loading CSV Data - Practical Examples\n",
        "print(\"üìÅ LOADING CSV DATA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create sample CSV data\n",
        "csv_content = \"\"\"longitude,latitude,housing_median_age,total_rooms,population,median_income,price\n",
        "-122.23,37.88,41.0,880.0,322.0,8.3252,452600.0\n",
        "-122.22,37.86,21.0,1106.0,2401.0,8.3014,358500.0\n",
        "-122.24,37.85,52.0,1467.0,496.0,7.2574,352100.0\n",
        "-122.25,37.85,52.0,1274.0,558.0,5.6431,341300.0\n",
        "-122.25,37.85,52.0,1627.0,565.0,3.8462,342200.0\"\"\"\n",
        "\n",
        "# Write to temporary file\n",
        "temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
        "temp_file.write(csv_content)\n",
        "temp_file.close()\n",
        "print(f\"üìù Created sample CSV: {os.path.basename(temp_file.name)}\")\n",
        "\n",
        "print(\"\\nüéØ METHOD 1: make_csv_dataset (Recommended)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# High-level CSV loading\n",
        "csv_dataset = tf.data.experimental.make_csv_dataset(\n",
        "    temp_file.name,\n",
        "    batch_size=2,\n",
        "    label_name=\"price\",           # Target column\n",
        "    na_value=\"?\",                 # Missing value indicator\n",
        "    num_epochs=1,                 # Number of epochs\n",
        "    ignore_errors=True,           # Skip problematic rows\n",
        "    shuffle=False                 # Keep order for demo\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset created with automatic type inference\")\n",
        "print(\"üìã Sample data:\")\n",
        "for batch_num, (features, labels) in enumerate(csv_dataset.take(2)):\n",
        "    print(f\"\\n   Batch {batch_num + 1}:\")\n",
        "    print(f\"   Features: {list(features.keys())}\")\n",
        "    for key, values in features.items():\n",
        "        print(f\"     {key}: {values.numpy()}\")\n",
        "    print(f\"   Labels (price): {labels.numpy()}\")\n",
        "\n",
        "print(\"\\nüîß METHOD 2: TextLineDataset (Manual Control)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Low-level manual parsing\n",
        "def parse_csv_line(line):\n",
        "    \"\"\"Parse a single CSV line\"\"\"\n",
        "    # Define default values (for type inference)\n",
        "    defaults = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # All float\n",
        "    fields = tf.io.decode_csv(line, defaults)\n",
        "\n",
        "    # Features (all except last column)\n",
        "    features = tf.stack(fields[:-1])\n",
        "    # Label (last column)\n",
        "    label = fields[-1]\n",
        "\n",
        "    return features, label\n",
        "\n",
        "# Create dataset from text lines\n",
        "text_dataset = tf.data.TextLineDataset(temp_file.name)\n",
        "text_dataset = text_dataset.skip(1)  # Skip header\n",
        "parsed_dataset = text_dataset.map(parse_csv_line)\n",
        "\n",
        "print(\"‚úÖ Manual parsing complete\")\n",
        "print(\"üìã Parsed data samples:\")\n",
        "for i, (features, label) in enumerate(parsed_dataset.take(2)):\n",
        "    print(f\"   Sample {i+1}: Features={features.numpy()}, Price={label.numpy()}\")\n",
        "\n",
        "print(\"\\nüîÑ PIPELINE-READY DATASET:\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create training-ready pipeline\n",
        "training_dataset = (parsed_dataset\n",
        "                   .shuffle(buffer_size=100, seed=42)\n",
        "                   .batch(2)\n",
        "                   .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"‚úÖ Training pipeline: shuffle ‚Üí batch ‚Üí prefetch\")\n",
        "for batch in training_dataset.take(1):\n",
        "    features, labels = batch\n",
        "    print(f\"   Batch shape: Features {features.shape}, Labels {labels.shape}\")\n",
        "\n",
        "# Cleanup\n",
        "os.unlink(temp_file.name)\n",
        "print(f\"\\nüßπ Cleaned up temporary file\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f67d24fe",
      "metadata": {
        "id": "f67d24fe"
      },
      "source": [
        "# üîß 3. Data Preprocessing & Feature Engineering\n",
        "\n",
        "## üéØ Mengapa Preprocessing Penting?\n",
        "\n",
        "Deep learning models membutuhkan data yang **terstandarisasi** dan **bersih**:\n",
        "\n",
        "### üìä Numerical Data:\n",
        "- **Normalisasi** - Scale data ke range [0,1]: `(x - min) / (max - min)`\n",
        "- **Standardisasi** - Zero mean, unit variance: `(x - mean) / std`\n",
        "- **Robust scaling** - Menggunakan median dan IQR\n",
        "\n",
        "### üìù Text Data:\n",
        "- **Tokenization** - Split text menjadi tokens\n",
        "- **Vocabulary mapping** - Convert tokens ke integers\n",
        "- **Padding** - Uniform sequence length\n",
        "- **Embedding** - Dense vector representation\n",
        "\n",
        "### üñºÔ∏è Image Data:\n",
        "- **Normalization** - Pixel values [0,1] atau [-1,1]\n",
        "- **Resize** - Uniform image dimensions\n",
        "- **Augmentation** - Rotation, flip, crop, dll\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 Numerical Preprocessing üî¢\n",
        "\n",
        "Teknik preprocessing untuk data numerik:\n",
        "- **Normalisasi**: Mengubah skala data ke range [0,1]  \n",
        "- **Standardisasi**: Mengubah data memiliki mean=0 dan std=1\n",
        "- **Robust scaling**: Menggunakan median dan IQR\n",
        "\n",
        "## 3.2 Text Preprocessing üìö\n",
        "\n",
        "Untuk data teks, kita perlu:\n",
        "- **Tokenisasi**: Memecah teks menjadi token-token\n",
        "- **Pemetaan Kosakata**: Mengonversi token menjadi bilangan bulat\n",
        "- **Padding**: Menyamakan panjang urutan\n",
        "- **Penyematan**: Representasi vektor yang padat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "93bb46e3",
      "metadata": {
        "id": "93bb46e3",
        "outputId": "a66b5ea2-6113-4f67-b493-ba36c611d25f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATA PREPROCESSING ===\n",
            "Original data:\n",
            "[[  1.  100.    0.5]\n",
            " [  2.  200.    1.5]\n",
            " [  3.  150.    2. ]\n",
            " [  4.   50.    0.8]]\n",
            "\n",
            "=== NORMALIZATION (Min-Max) ===\n",
            "Normalized data (0-1 range):\n",
            "[[0.         0.33333334 0.        ]\n",
            " [0.33333334 1.         0.6666667 ]\n",
            " [0.6666667  0.6666667  1.        ]\n",
            " [1.         0.         0.2       ]]\n",
            "\n",
            "=== STANDARDIZATION (Z-score) ===\n",
            "Standardized data (mean=0, std=1):\n",
            "[[-1.341641   -0.4472136  -1.1917592 ]\n",
            " [-0.44721365  1.3416408   0.51075387]\n",
            " [ 0.44721365  0.4472136   1.3620104 ]\n",
            " [ 1.341641   -1.3416408  -0.6810053 ]]\n",
            "Mean: [ 0.0000000e+00  0.0000000e+00 -7.4505806e-08]\n",
            "Std: [1.0000001 1.        1.       ]\n",
            "\n",
            "=== FEATURE ENGINEERING ===\n",
            "Original + Polynomial features:\n",
            "Shape: (4, 3) ‚Üí (4, 9)\n",
            "First sample:\n",
            "  Original: [  1.  100.    0.5]\n",
            "  Enhanced: [1.0e+00 1.0e+02 5.0e-01 1.0e+02 5.0e-01 5.0e+01 1.0e+00 1.0e+04 2.5e-01]\n",
            "üîß NUMERICAL PREPROCESSING\n",
            "==================================================\n",
            "üìä Original data (mixed scales):\n",
            "[[1.0e+00 1.0e+02 5.0e-01 1.0e+03]\n",
            " [2.0e+00 2.0e+02 1.5e+00 2.0e+03]\n",
            " [3.0e+00 1.5e+02 2.0e+00 1.5e+03]\n",
            " [4.0e+00 5.0e+01 8.0e-01 8.0e+02]\n",
            " [5.0e+00 3.0e+02 1.2e+00 3.0e+03]]\n",
            "   Min values: [1.e+00 5.e+01 5.e-01 8.e+02]\n",
            "   Max values: [5.e+00 3.e+02 2.e+00 3.e+03]\n",
            "\n",
            "1Ô∏è‚É£ MIN-MAX NORMALIZATION [0,1]\n",
            "-----------------------------------\n",
            "‚úÖ Normalized data [0,1]:\n",
            "[[0.         0.2        0.         0.09090909]\n",
            " [0.25       0.6        0.6666667  0.54545456]\n",
            " [0.5        0.4        1.         0.3181818 ]\n",
            " [0.75       0.         0.2        0.        ]\n",
            " [1.         1.         0.4666667  1.        ]]\n",
            "   New min: [0. 0. 0. 0.]\n",
            "   New max: [1. 1. 1. 1.]\n",
            "\n",
            "2Ô∏è‚É£ Z-SCORE STANDARDIZATION\n",
            "------------------------------\n",
            "‚úÖ Standardized data (Œº=0, œÉ=1):\n",
            "[[-1.4142135  -0.6974858  -1.3324271  -0.8365832 ]\n",
            " [-0.70710677  0.46499056  0.5710401   0.4309671 ]\n",
            " [ 0.         -0.11624764  1.5227737  -0.20280804]\n",
            " [ 0.70710677 -1.278724   -0.761387   -1.0900933 ]\n",
            " [ 1.4142135   1.6274669   0.          1.6985173 ]]\n",
            "   New mean: [ 2.3841858e-08  0.0000000e+00 -5.9604645e-08  0.0000000e+00]\n",
            "   New std:  [0.99999994 0.99999994 0.9999998  0.99999994]\n",
            "\n",
            "3Ô∏è‚É£ FEATURE ENGINEERING\n",
            "-------------------------\n",
            "‚úÖ Enhanced features:\n",
            "   Original shape: (5, 4)\n",
            "   Enhanced shape: (5, 9)\n",
            "   First sample enhanced:\n",
            "     Original: [1.e+00 1.e+02 5.e-01 1.e+03]\n",
            "     Enhanced: [1.0e+00 1.0e+02 5.0e-01 1.0e+03 1.0e+00 1.0e+04 2.5e-01 1.0e+06 1.0e+02]\n",
            "\n",
            "üîÑ DATASET INTEGRATION\n",
            "----------------------\n",
            "‚úÖ Preprocessing applied to dataset:\n",
            "   Sample 1:\n",
            "     Before: [1.e+00 1.e+02 5.e-01 1.e+03]\n",
            "     After:  [-0.6527764  -0.41724157 -0.65396595  1.723984  ]\n",
            "   Sample 2:\n",
            "     Before: [2.0e+00 2.0e+02 1.5e+00 2.0e+03]\n",
            "     After:  [-0.6529886  -0.4174309  -0.65358347  1.724003  ]\n",
            "\n",
            "‚úÖ Numerical preprocessing complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üîß Data Preprocessing Examples\n",
        "print(\"=== DATA PREPROCESSING ===\")\n",
        "\n",
        "# Sample data\n",
        "data = tf.constant([\n",
        "    [1.0, 100.0, 0.5],\n",
        "    [2.0, 200.0, 1.5],\n",
        "    [3.0, 150.0, 2.0],\n",
        "    [4.0, 50.0, 0.8]\n",
        "])\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(data.numpy())\n",
        "\n",
        "# 1. Normalization (Min-Max Scaling)\n",
        "print(\"\\n=== NORMALIZATION (Min-Max) ===\")\n",
        "def normalize_minmax(data):\n",
        "    min_vals = tf.reduce_min(data, axis=0)\n",
        "    max_vals = tf.reduce_max(data, axis=0)\n",
        "    return (data - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "normalized_data = normalize_minmax(data)\n",
        "print(\"Normalized data (0-1 range):\")\n",
        "print(normalized_data.numpy())\n",
        "\n",
        "# 2. Standardization (Z-score)\n",
        "print(\"\\n=== STANDARDIZATION (Z-score) ===\")\n",
        "def standardize(data):\n",
        "    mean = tf.reduce_mean(data, axis=0)\n",
        "    std = tf.math.reduce_std(data, axis=0)\n",
        "    return (data - mean) / std\n",
        "\n",
        "standardized_data = standardize(data)\n",
        "print(\"Standardized data (mean=0, std=1):\")\n",
        "print(standardized_data.numpy())\n",
        "print(\"Mean:\", tf.reduce_mean(standardized_data, axis=0).numpy())\n",
        "print(\"Std:\", tf.math.reduce_std(standardized_data, axis=0).numpy())\n",
        "\n",
        "# 3. Feature Engineering - Polynomial Features\n",
        "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
        "def create_polynomial_features(data):\n",
        "    # Create interaction features\n",
        "    x1, x2, x3 = tf.split(data, 3, axis=1)\n",
        "\n",
        "    # Original features + polynomial features\n",
        "    features = tf.concat([\n",
        "        data,                    # Original features\n",
        "        x1 * x2,                # Interaction x1*x2\n",
        "        x1 * x3,                # Interaction x1*x3\n",
        "        x2 * x3,                # Interaction x2*x3\n",
        "        tf.square(x1),          # x1¬≤\n",
        "        tf.square(x2),          # x2¬≤\n",
        "        tf.square(x3)           # x3¬≤\n",
        "    ], axis=1)\n",
        "\n",
        "    return features\n",
        "\n",
        "poly_features = create_polynomial_features(data)\n",
        "print(\"Original + Polynomial features:\")\n",
        "print(f\"Shape: {data.shape} ‚Üí {poly_features.shape}\")\n",
        "print(\"First sample:\")\n",
        "print(f\"  Original: {data[0].numpy()}\")\n",
        "print(f\"  Enhanced: {poly_features[0].numpy()}\")\n",
        "\n",
        "# üî¢ 3.1 Numerical Data Preprocessing\n",
        "print(\"üîß NUMERICAL PREPROCESSING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sample numerical data (different scales)\n",
        "raw_data = tf.constant([\n",
        "    [1.0, 100.0, 0.5, 1000.0],    # Mixed scales\n",
        "    [2.0, 200.0, 1.5, 2000.0],\n",
        "    [3.0, 150.0, 2.0, 1500.0],\n",
        "    [4.0, 50.0, 0.8, 800.0],\n",
        "    [5.0, 300.0, 1.2, 3000.0]\n",
        "], dtype=tf.float32)\n",
        "\n",
        "print(\"üìä Original data (mixed scales):\")\n",
        "print(raw_data.numpy())\n",
        "print(f\"   Min values: {tf.reduce_min(raw_data, axis=0).numpy()}\")\n",
        "print(f\"   Max values: {tf.reduce_max(raw_data, axis=0).numpy()}\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ MIN-MAX NORMALIZATION [0,1]\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "def normalize_minmax(data):\n",
        "    \"\"\"Min-Max normalization to [0,1] range\"\"\"\n",
        "    min_vals = tf.reduce_min(data, axis=0)\n",
        "    max_vals = tf.reduce_max(data, axis=0)\n",
        "    # Avoid division by zero\n",
        "    range_vals = tf.maximum(max_vals - min_vals, 1e-8)\n",
        "    return (data - min_vals) / range_vals\n",
        "\n",
        "normalized_data = normalize_minmax(raw_data)\n",
        "print(\"‚úÖ Normalized data [0,1]:\")\n",
        "print(normalized_data.numpy())\n",
        "print(f\"   New min: {tf.reduce_min(normalized_data, axis=0).numpy()}\")\n",
        "print(f\"   New max: {tf.reduce_max(normalized_data, axis=0).numpy()}\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Z-SCORE STANDARDIZATION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def standardize_zscore(data):\n",
        "    \"\"\"Z-score standardization (mean=0, std=1)\"\"\"\n",
        "    mean = tf.reduce_mean(data, axis=0)\n",
        "    std = tf.math.reduce_std(data, axis=0)\n",
        "    # Avoid division by zero\n",
        "    std = tf.maximum(std, 1e-8)\n",
        "    return (data - mean) / std\n",
        "\n",
        "standardized_data = standardize_zscore(raw_data)\n",
        "print(\"‚úÖ Standardized data (Œº=0, œÉ=1):\")\n",
        "print(standardized_data.numpy())\n",
        "print(f\"   New mean: {tf.reduce_mean(standardized_data, axis=0).numpy()}\")\n",
        "print(f\"   New std:  {tf.math.reduce_std(standardized_data, axis=0).numpy()}\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ FEATURE ENGINEERING\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "def create_polynomial_features(data):\n",
        "    \"\"\"Create polynomial and interaction features\"\"\"\n",
        "    # Original features\n",
        "    original_features = data\n",
        "\n",
        "    # Polynomial features (squares)\n",
        "    squared_features = tf.square(data)\n",
        "\n",
        "    # Interaction features (pairwise products)\n",
        "    # For demo, just first two columns\n",
        "    interaction = tf.expand_dims(data[:, 0] * data[:, 1], axis=1)\n",
        "\n",
        "    # Combine all features\n",
        "    enhanced_features = tf.concat([\n",
        "        original_features,      # Original\n",
        "        squared_features,       # x¬≤\n",
        "        interaction            # x‚ÇÅ √ó x‚ÇÇ\n",
        "    ], axis=1)\n",
        "\n",
        "    return enhanced_features\n",
        "\n",
        "enhanced_data = create_polynomial_features(raw_data)\n",
        "print(\"‚úÖ Enhanced features:\")\n",
        "print(f\"   Original shape: {raw_data.shape}\")\n",
        "print(f\"   Enhanced shape: {enhanced_data.shape}\")\n",
        "print(\"   First sample enhanced:\")\n",
        "print(f\"     Original: {raw_data[0].numpy()}\")\n",
        "print(f\"     Enhanced: {enhanced_data[0].numpy()}\")\n",
        "\n",
        "print(\"\\nüîÑ DATASET INTEGRATION\")\n",
        "print(\"-\" * 22)\n",
        "\n",
        "# Apply preprocessing to a dataset\n",
        "def preprocess_fn(data):\n",
        "    \"\"\"Preprocessing function for dataset.map()\"\"\"\n",
        "    return standardize_zscore(data)\n",
        "\n",
        "# Create dataset and apply preprocessing\n",
        "dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n",
        "preprocessed_dataset = dataset.map(preprocess_fn)\n",
        "\n",
        "print(\"‚úÖ Preprocessing applied to dataset:\")\n",
        "for i, (original, processed) in enumerate(zip(dataset.take(2), preprocessed_dataset.take(2))):\n",
        "    print(f\"   Sample {i+1}:\")\n",
        "    print(f\"     Before: {original.numpy()}\")\n",
        "    print(f\"     After:  {processed.numpy()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Numerical preprocessing complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f73428fe",
      "metadata": {
        "id": "f73428fe",
        "outputId": "314747ab-b1a0-4a83-fbe2-01cda449cf2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù TEXT PREPROCESSING\n",
            "==================================================\n",
            "üìã Original texts:\n",
            "   1. I love machine learning and deep learning\n",
            "   2. TensorFlow is an amazing framework\n",
            "   3. Natural language processing is fascinating\n",
            "   4. Deep neural networks are powerful\n",
            "   5. Data science and AI are the future\n",
            "\n",
            "1Ô∏è‚É£ TOKENIZATION & VOCABULARY\n",
            "-----------------------------------\n",
            "‚úÖ Vocabulary size: 25\n",
            "üìö Top 10 words in vocabulary:\n",
            "     '<OOV>': 1\n",
            "     'learning': 2\n",
            "     'and': 3\n",
            "     'deep': 4\n",
            "     'is': 5\n",
            "     'are': 6\n",
            "     'i': 7\n",
            "     'love': 8\n",
            "     'machine': 9\n",
            "     'tensorflow': 10\n",
            "\n",
            "2Ô∏è‚É£ TEXT TO SEQUENCES\n",
            "----------------------\n",
            "‚úÖ Text ‚Üí Sequences conversion:\n",
            "   1. 'I love machine learning and de...' ‚Üí [7, 8, 9, 2, 3, 4, 2]\n",
            "   2. 'TensorFlow is an amazing frame...' ‚Üí [10, 5, 11, 12, 13]\n",
            "   3. 'Natural language processing is...' ‚Üí [14, 15, 16, 5, 17]\n",
            "   4. 'Deep neural networks are power...' ‚Üí [4, 18, 19, 6, 20]\n",
            "   5. 'Data science and AI are the fu...' ‚Üí [21, 22, 3, 23, 6, 24, 25]\n",
            "\n",
            "3Ô∏è‚É£ SEQUENCE PADDING\n",
            "--------------------\n",
            "‚úÖ Padded sequences (max_length=8):\n",
            "   1. [7, 8, 9, 2, 3, 4, 2] ‚Üí [7 8 9 2 3 4 2 0]\n",
            "   2. [10, 5, 11, 12, 13] ‚Üí [10  5 11 12 13  0  0  0]\n",
            "   3. [14, 15, 16, 5, 17] ‚Üí [14 15 16  5 17  0  0  0]\n",
            "   4. [4, 18, 19, 6, 20] ‚Üí [ 4 18 19  6 20  0  0  0]\n",
            "   5. [21, 22, 3, 23, 6, 24, 25] ‚Üí [21 22  3 23  6 24 25  0]\n",
            "\n",
            "4Ô∏è‚É£ TENSORFLOW DATASET INTEGRATION\n",
            "-----------------------------------\n",
            "‚úÖ Text dataset created:\n",
            "   Text: I love machine learning and deep learning\n",
            "   Sequence: [7 8 9 2 3 4 2 0]\n",
            "\n",
            "   Text: TensorFlow is an amazing framework\n",
            "   Sequence: [10  5 11 12 13  0  0  0]\n",
            "\n",
            "\n",
            "5Ô∏è‚É£ MODERN APPROACH: TextVectorization\n",
            "-----------------------------------\n",
            "‚úÖ TextVectorization layer created\n",
            "üìö Vocabulary size: 26\n",
            "üìã Vectorized texts:\n",
            "   1. [18 16 15  2  6  4  2  0]\n",
            "   2. [ 8  3 23 24 20  0  0  0]\n",
            "   3. [14 17 10  3 21  0  0  0]\n",
            "   4. [ 4 12 13  5 11  0  0  0]\n",
            "   5. [22  9  6 25  5  7 19  0]\n",
            "\n",
            "üîÑ COMPLETE TEXT PIPELINE\n",
            "-------------------------\n",
            "‚úÖ Complete pipeline applied:\n",
            "   Sample 1: [18 16 15  2  6  4  2  0]\n",
            "   Sample 2: [ 8  3 23 24 20  0  0  0]\n",
            "\n",
            "‚úÖ Text preprocessing complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üìù 3.2 Text Data Preprocessing\n",
        "print(\"üìù TEXT PREPROCESSING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love machine learning and deep learning\",\n",
        "    \"TensorFlow is an amazing framework\",\n",
        "    \"Natural language processing is fascinating\",\n",
        "    \"Deep neural networks are powerful\",\n",
        "    \"Data science and AI are the future\"\n",
        "]\n",
        "\n",
        "print(\"üìã Original texts:\")\n",
        "for i, text in enumerate(texts, 1):\n",
        "    print(f\"   {i}. {text}\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ TOKENIZATION & VOCABULARY\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=50,           # Vocabulary size\n",
        "    oov_token=\"<OOV>\",      # Out-of-vocabulary token\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # Characters to filter\n",
        ")\n",
        "\n",
        "# Fit on texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "print(f\"‚úÖ Vocabulary size: {len(tokenizer.word_index)}\")\n",
        "print(\"üìö Top 10 words in vocabulary:\")\n",
        "for word, idx in list(tokenizer.word_index.items())[:10]:\n",
        "    print(f\"     '{word}': {idx}\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ TEXT TO SEQUENCES\")\n",
        "print(\"-\" * 22)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(\"‚úÖ Text ‚Üí Sequences conversion:\")\n",
        "for i, (text, seq) in enumerate(zip(texts, sequences)):\n",
        "    print(f\"   {i+1}. '{text[:30]}...' ‚Üí {seq}\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ SEQUENCE PADDING\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Pad sequences to uniform length\n",
        "max_length = 8\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=max_length,\n",
        "    padding='post',      # Pad at the end\n",
        "    truncating='post'    # Truncate at the end\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Padded sequences (max_length={max_length}):\")\n",
        "for i, (original, padded) in enumerate(zip(sequences, padded_sequences)):\n",
        "    print(f\"   {i+1}. {original} ‚Üí {padded}\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ TENSORFLOW DATASET INTEGRATION\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(texts)\n",
        "padded_dataset = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
        "\n",
        "# Combine text and sequences\n",
        "combined_dataset = tf.data.Dataset.zip((text_dataset, padded_dataset))\n",
        "\n",
        "print(\"‚úÖ Text dataset created:\")\n",
        "for text, sequence in combined_dataset.take(2):\n",
        "    print(f\"   Text: {text.numpy().decode('utf-8')}\")\n",
        "    print(f\"   Sequence: {sequence.numpy()}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ MODERN APPROACH: TextVectorization\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# TensorFlow 2.x way (more efficient)\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=50,\n",
        "    output_sequence_length=max_length,\n",
        "    output_mode='int'\n",
        ")\n",
        "\n",
        "# Adapt to text data\n",
        "vectorizer.adapt(texts)\n",
        "\n",
        "print(\"‚úÖ TextVectorization layer created\")\n",
        "print(f\"üìö Vocabulary size: {vectorizer.vocabulary_size()}\")\n",
        "\n",
        "# Apply vectorization\n",
        "vectorized_texts = vectorizer(texts)\n",
        "print(\"üìã Vectorized texts:\")\n",
        "for i, vec in enumerate(vectorized_texts.numpy()):\n",
        "    print(f\"   {i+1}. {vec}\")\n",
        "\n",
        "print(\"\\nüîÑ COMPLETE TEXT PIPELINE\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create complete preprocessing pipeline\n",
        "def text_preprocessing_pipeline(text_data, max_tokens=50, max_length=8):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "\n",
        "    # Create and adapt vectorizer\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=max_tokens,\n",
        "        output_sequence_length=max_length,\n",
        "        output_mode='int'\n",
        "    )\n",
        "    vectorizer.adapt(text_data)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(text_data)\n",
        "\n",
        "    # Apply vectorization\n",
        "    vectorized_dataset = dataset.map(\n",
        "        lambda x: vectorizer(x),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return vectorized_dataset, vectorizer\n",
        "\n",
        "# Apply complete pipeline\n",
        "processed_dataset, text_vectorizer = text_preprocessing_pipeline(texts)\n",
        "\n",
        "print(\"‚úÖ Complete pipeline applied:\")\n",
        "for i, processed_text in enumerate(processed_dataset.take(2)):\n",
        "    print(f\"   Sample {i+1}: {processed_text.numpy()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Text preprocessing complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013b963f",
      "metadata": {
        "id": "013b963f"
      },
      "source": [
        "# üöÄ 4. Performance Optimization\n",
        "\n",
        "## ‚ö° Mengapa Optimasi Penting?\n",
        "\n",
        "Data loading sering menjadi **bottleneck** dalam deep learning training. Tanpa optimasi yang tepat:\n",
        "- üêå Model menunggu data (GPU idle)\n",
        "- üí∞ Pemborosan resource komputasi\n",
        "- ‚è±Ô∏è Training time sangat lama\n",
        "\n",
        "## üõ†Ô∏è Teknik Optimasi Utama\n",
        "\n",
        "### 1. **Prefetching** üîÑ\n",
        "Load data di background saat model training\n",
        "```python\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "```\n",
        "\n",
        "### 2. **Caching** üíæ\n",
        "Simpan preprocessed data di memory/disk\n",
        "```python\n",
        "dataset = dataset.cache()  # Memory cache\n",
        "dataset = dataset.cache('/path/to/cache')  # Disk cache\n",
        "```\n",
        "\n",
        "### 3. **Parallelization** üîÄ\n",
        "Gunakan multiple cores untuk preprocessing\n",
        "```python\n",
        "dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "```\n",
        "\n",
        "### 4. **Vectorization** üìä\n",
        "Batch operations lebih efisien dari single-item operations\n",
        "\n",
        "---\n",
        "\n",
        "## 4.1 Urutan Optimasi yang Tepat ‚úÖ\n",
        "\n",
        "**Recommended Order:**\n",
        "1. `shuffle()` (untuk dataset kecil)\n",
        "2. `map()` (preprocessing)  \n",
        "3. `cache()` (jika memori cukup)\n",
        "4. `batch()`\n",
        "5. `prefetch()`\n",
        "\n",
        "**‚ùå Avoid:** Shuffle setelah batch, cache sebelum expensive operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f3c9a952",
      "metadata": {
        "id": "f3c9a952",
        "outputId": "bd3c405c-df92-40ec-abf0-678c8cdce640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ PERFORMANCE OPTIMIZATION\n",
            "==================================================\n",
            "üìä Created synthetic dataset (1000 samples, 100 features)\n",
            "\n",
            "‚ùå BAD PIPELINE (Unoptimized)\n",
            "-----------------------------------\n",
            "‚úÖ Bad pipeline structure:\n",
            "   data ‚Üí map(expensive) ‚Üí batch\n",
            "   Issues: No caching, no prefetching, no parallelization\n",
            "\n",
            "‚úÖ GOOD PIPELINE (Optimized)\n",
            "--------------------------------\n",
            "‚úÖ Good pipeline structure:\n",
            "   data ‚Üí map(expensive, parallel) ‚Üí cache ‚Üí shuffle ‚Üí batch ‚Üí prefetch\n",
            "\n",
            "‚ö° PERFORMANCE COMPARISON\n",
            "----------------------------\n",
            "\n",
            "üïê Timing Bad Pipeline:\n",
            "   Processed batch 1\n",
            "   Processed batch 6\n",
            "   ‚è±Ô∏è Time: 0.03s (0.003s per batch)\n",
            "\n",
            "üïê Timing Good Pipeline:\n",
            "   Processed batch 1\n",
            "   Processed batch 6\n",
            "   ‚è±Ô∏è Time: 0.04s (0.004s per batch)\n",
            "\n",
            "üéØ PERFORMANCE IMPROVEMENT: -52.4%\n",
            "\n",
            "üí° OPTIMIZATION TECHNIQUES BREAKDOWN\n",
            "--------------------------------------\n",
            "1Ô∏è‚É£ AUTOTUNE - Automatic optimization\n",
            "   tf.data.AUTOTUNE automatically determines optimal values\n",
            "   for buffer_size, num_parallel_calls, etc.\n",
            "\n",
            "2Ô∏è‚É£ PREFETCHING - Overlap computation\n",
            "   ‚úÖ Prefetch added - data loading overlaps with training\n",
            "\n",
            "3Ô∏è‚É£ CACHING - Avoid recomputation\n",
            "   ‚úÖ Cache added - expensive operations computed once\n",
            "\n",
            "4Ô∏è‚É£ PARALLEL MAP - Use multiple cores\n",
            "   ‚úÖ Parallel processing - utilizes multiple CPU cores\n",
            "\n",
            "üîß MEMORY OPTIMIZATION TIPS\n",
            "----------------------------\n",
            "üíæ For large datasets:\n",
            "   - Use cache() only if data fits in memory\n",
            "   - Consider disk caching: cache('/path/to/cache')\n",
            "   - Use prefetch() to overlap I/O with computation\n",
            "   - Batch after expensive operations\n",
            "\n",
            "‚úÖ Performance optimization complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üöÄ 4.1 Performance Optimization Examples\n",
        "print(\"üöÄ PERFORMANCE OPTIMIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create synthetic dataset for performance testing\n",
        "def create_synthetic_data(n_samples=1000):\n",
        "    \"\"\"Create synthetic dataset for performance testing\"\"\"\n",
        "    data = tf.random.normal((n_samples, 100))  # 100 features\n",
        "    labels = tf.random.uniform((n_samples,), maxval=2, dtype=tf.int32)\n",
        "    return tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "\n",
        "# Expensive preprocessing simulation\n",
        "def expensive_preprocessing(features, label):\n",
        "    \"\"\"Simulate expensive preprocessing operation\"\"\"\n",
        "    # Simulate computational cost\n",
        "    processed_features = tf.nn.l2_normalize(features, axis=0)\n",
        "    processed_features = tf.math.sin(processed_features) * tf.math.cos(processed_features)\n",
        "    return processed_features, label\n",
        "\n",
        "print(\"üìä Created synthetic dataset (1000 samples, 100 features)\")\n",
        "\n",
        "print(\"\\n‚ùå BAD PIPELINE (Unoptimized)\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Bad pipeline - no optimization\n",
        "bad_pipeline = (create_synthetic_data()\n",
        "                .map(expensive_preprocessing)\n",
        "                .batch(32))\n",
        "\n",
        "print(\"‚úÖ Bad pipeline structure:\")\n",
        "print(\"   data ‚Üí map(expensive) ‚Üí batch\")\n",
        "print(\"   Issues: No caching, no prefetching, no parallelization\")\n",
        "\n",
        "print(\"\\n‚úÖ GOOD PIPELINE (Optimized)\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "# Good pipeline - fully optimized\n",
        "good_pipeline = (create_synthetic_data()\n",
        "                 .map(expensive_preprocessing,\n",
        "                      num_parallel_calls=tf.data.AUTOTUNE)  # Parallel processing\n",
        "                 .cache()                                   # Cache processed data\n",
        "                 .shuffle(buffer_size=1000)                 # Shuffle\n",
        "                 .batch(32)                                 # Batch\n",
        "                 .prefetch(tf.data.AUTOTUNE))              # Prefetch\n",
        "\n",
        "print(\"‚úÖ Good pipeline structure:\")\n",
        "print(\"   data ‚Üí map(expensive, parallel) ‚Üí cache ‚Üí shuffle ‚Üí batch ‚Üí prefetch\")\n",
        "\n",
        "print(\"\\n‚ö° PERFORMANCE COMPARISON\")\n",
        "print(\"-\" * 28)\n",
        "\n",
        "def time_dataset(dataset, name, num_batches=10):\n",
        "    \"\"\"Time dataset iteration\"\"\"\n",
        "    print(f\"\\nüïê Timing {name}:\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, batch in enumerate(dataset.take(num_batches)):\n",
        "        if i % 5 == 0:\n",
        "            print(f\"   Processed batch {i+1}\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"   ‚è±Ô∏è Time: {elapsed:.2f}s ({elapsed/num_batches:.3f}s per batch)\")\n",
        "    return elapsed\n",
        "\n",
        "# Time both pipelines\n",
        "bad_time = time_dataset(bad_pipeline, \"Bad Pipeline\", 10)\n",
        "good_time = time_dataset(good_pipeline, \"Good Pipeline\", 10)\n",
        "\n",
        "improvement = (bad_time - good_time) / bad_time * 100\n",
        "print(f\"\\nüéØ PERFORMANCE IMPROVEMENT: {improvement:.1f}%\")\n",
        "\n",
        "print(\"\\nüí° OPTIMIZATION TECHNIQUES BREAKDOWN\")\n",
        "print(\"-\" * 38)\n",
        "\n",
        "print(\"1Ô∏è‚É£ AUTOTUNE - Automatic optimization\")\n",
        "print(\"   tf.data.AUTOTUNE automatically determines optimal values\")\n",
        "print(\"   for buffer_size, num_parallel_calls, etc.\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ PREFETCHING - Overlap computation\")\n",
        "optimized_for_prefetch = (tf.data.Dataset.range(100)\n",
        "                         .map(lambda x: tf.cast(x, tf.float32))\n",
        "                         .batch(10)\n",
        "                         .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"   ‚úÖ Prefetch added - data loading overlaps with training\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ CACHING - Avoid recomputation\")\n",
        "cached_dataset = (tf.data.Dataset.range(100)\n",
        "                 .map(lambda x: x ** 2)  # Expensive operation\n",
        "                 .cache()                # Cache results\n",
        "                 .batch(10))\n",
        "\n",
        "print(\"   ‚úÖ Cache added - expensive operations computed once\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ PARALLEL MAP - Use multiple cores\")\n",
        "parallel_dataset = (tf.data.Dataset.range(100)\n",
        "                   .map(lambda x: tf.math.sin(tf.cast(x, tf.float32)),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                   .batch(10))\n",
        "\n",
        "print(\"   ‚úÖ Parallel processing - utilizes multiple CPU cores\")\n",
        "\n",
        "print(\"\\nüîß MEMORY OPTIMIZATION TIPS\")\n",
        "print(\"-\" * 28)\n",
        "print(\"üíæ For large datasets:\")\n",
        "print(\"   - Use cache() only if data fits in memory\")\n",
        "print(\"   - Consider disk caching: cache('/path/to/cache')\")\n",
        "print(\"   - Use prefetch() to overlap I/O with computation\")\n",
        "print(\"   - Batch after expensive operations\")\n",
        "\n",
        "print(\"\\n‚úÖ Performance optimization complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b9029b",
      "metadata": {
        "id": "08b9029b"
      },
      "source": [
        "---\n",
        "\n",
        "# üíæ 5. TFRecord Format\n",
        "\n",
        "## üéØ Mengapa TFRecord?\n",
        "\n",
        "**TFRecord** adalah binary format native TensorFlow dengan keunggulan:\n",
        "\n",
        "### ‚úÖ Keuntungan:\n",
        "- **üöÄ Performa** - Loading 2-3x lebih cepat dari CSV\n",
        "- **üì¶ Kompresi** - Built-in compression (GZIP, ZLIB)\n",
        "- **üîÑ Efisiensi** - Optimal untuk streaming data besar\n",
        "- **üèóÔ∏è Fleksibilitas** - Mendukung data kompleks (nested, variable-length)\n",
        "- **‚ö° Integrasi** - Perfect dengan tf.data pipeline\n",
        "\n",
        "### üìã Struktur TFRecord:\n",
        "```\n",
        "TFRecord File\n",
        "‚îú‚îÄ‚îÄ Example 1 (Protocol Buffer)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Feature 1 (bytes/float/int64)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Feature 2 (bytes/float/int64)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ Example 2\n",
        "‚îî‚îÄ‚îÄ ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5.1 Creating & Reading TFRecord üõ†Ô∏è\n",
        "\n",
        "TFRecord menggunakan **Protocol Buffers** untuk serialisasi data yang efisien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3b90f1dd",
      "metadata": {
        "id": "3b90f1dd",
        "outputId": "286c4346-7886-4248-edf2-48f1c513375d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ TFRECORD FORMAT\n",
            "==================================================\n",
            "üîß Helper functions created for TFRecord serialization\n",
            "\n",
            "1Ô∏è‚É£ CREATING TFRECORD FILE\n",
            "----------------------------\n",
            "‚úÖ Created TFRecord: sample_data.tfrecord\n",
            "   üìä Contains 3 examples\n",
            "   üíæ File size: 321 bytes\n",
            "\n",
            "2Ô∏è‚É£ READING TFRECORD FILE\n",
            "-------------------------\n",
            "‚úÖ TFRecord dataset loaded and parsed\n",
            "üìã Sample data:\n",
            "\n",
            "   Example 1:\n",
            "     ID: 1\n",
            "     Features: [1. 2. 3. 4.]\n",
            "     Label: positive\n",
            "     Score: 0.950\n",
            "\n",
            "   Example 2:\n",
            "     ID: 2\n",
            "     Features: [5. 6. 7. 8.]\n",
            "     Label: negative\n",
            "     Score: 0.120\n",
            "\n",
            "   Example 3:\n",
            "     ID: 3\n",
            "     Features: [ 9. 10. 11. 12.]\n",
            "     Label: positive\n",
            "     Score: 0.870\n",
            "\n",
            "3Ô∏è‚É£ TRAINING-READY PIPELINE\n",
            "------------------------------\n",
            "‚úÖ Training pipeline created:\n",
            "   TFRecord ‚Üí parse ‚Üí prepare ‚Üí shuffle ‚Üí batch ‚Üí prefetch\n",
            "\n",
            "üìä Training batches:\n",
            "   Batch 1:\n",
            "     Features shape: (2, 4)\n",
            "     Labels: [1 0]\n",
            "     Features: [[ 9. 10. 11. 12.]\n",
            " [ 5.  6.  7.  8.]]\n",
            "   Batch 2:\n",
            "     Features shape: (1, 4)\n",
            "     Labels: [1]\n",
            "     Features: [[1. 2. 3. 4.]]\n",
            "\n",
            "4Ô∏è‚É£ COMPRESSION BENEFITS\n",
            "-------------------------\n",
            "‚úÖ Compression results:\n",
            "   üìÑ Original: 321 bytes\n",
            "   üì¶ Compressed: 182 bytes\n",
            "   üíæ Compression: 43.3% reduction\n",
            "\n",
            "üßπ Files cleaned up\n",
            "\n",
            "‚úÖ TFRecord examples complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üíæ 5.1 TFRecord Complete Example\n",
        "print(\"üíæ TFRECORD FORMAT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Helper functions for TFRecord creation\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def _float_list_feature(values):\n",
        "    \"\"\"Returns a float_list from a list of floats.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
        "\n",
        "print(\"üîß Helper functions created for TFRecord serialization\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ CREATING TFRECORD FILE\")\n",
        "print(\"-\" * 28)\n",
        "\n",
        "# Sample structured data\n",
        "samples = [\n",
        "    {\n",
        "        'id': 1,\n",
        "        'features': [1.0, 2.0, 3.0, 4.0],\n",
        "        'label': 'positive',\n",
        "        'score': 0.95\n",
        "    },\n",
        "    {\n",
        "        'id': 2,\n",
        "        'features': [5.0, 6.0, 7.0, 8.0],\n",
        "        'label': 'negative',\n",
        "        'score': 0.12\n",
        "    },\n",
        "    {\n",
        "        'id': 3,\n",
        "        'features': [9.0, 10.0, 11.0, 12.0],\n",
        "        'label': 'positive',\n",
        "        'score': 0.87\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create TFRecord file\n",
        "tfrecord_filename = 'sample_data.tfrecord'\n",
        "\n",
        "with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n",
        "    for sample in samples:\n",
        "        # Create features dictionary\n",
        "        feature_dict = {\n",
        "            'id': _int64_feature(sample['id']),\n",
        "            'features': _float_list_feature(sample['features']),\n",
        "            'label': _bytes_feature(sample['label'].encode('utf-8')),\n",
        "            'score': _float_feature(sample['score'])\n",
        "        }\n",
        "\n",
        "        # Create example\n",
        "        example = tf.train.Example(\n",
        "            features=tf.train.Features(feature=feature_dict)\n",
        "        )\n",
        "\n",
        "        # Write to file\n",
        "        writer.write(example.SerializeToString())\n",
        "\n",
        "print(f\"‚úÖ Created TFRecord: {tfrecord_filename}\")\n",
        "print(f\"   üìä Contains {len(samples)} examples\")\n",
        "print(f\"   üíæ File size: {os.path.getsize(tfrecord_filename)} bytes\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ READING TFRECORD FILE\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Define feature parsing schema\n",
        "feature_description = {\n",
        "    'id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'features': tf.io.VarLenFeature(tf.float32),\n",
        "    'label': tf.io.FixedLenFeature([], tf.string),\n",
        "    'score': tf.io.FixedLenFeature([], tf.float32)\n",
        "}\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"Parse TFRecord example\"\"\"\n",
        "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    # Convert sparse tensor to dense\n",
        "    parsed['features'] = tf.sparse.to_dense(parsed['features'])\n",
        "\n",
        "    return parsed\n",
        "\n",
        "# Read TFRecord dataset\n",
        "tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_filename)\n",
        "parsed_dataset = tfrecord_dataset.map(parse_tfrecord)\n",
        "\n",
        "print(\"‚úÖ TFRecord dataset loaded and parsed\")\n",
        "print(\"üìã Sample data:\")\n",
        "for i, record in enumerate(parsed_dataset):\n",
        "    print(f\"\\n   Example {i+1}:\")\n",
        "    print(f\"     ID: {record['id'].numpy()}\")\n",
        "    print(f\"     Features: {record['features'].numpy()}\")\n",
        "    print(f\"     Label: {record['label'].numpy().decode('utf-8')}\")\n",
        "    print(f\"     Score: {record['score'].numpy():.3f}\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ TRAINING-READY PIPELINE\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def prepare_for_training(record):\n",
        "    \"\"\"Prepare parsed record for training\"\"\"\n",
        "    features = record['features']\n",
        "    # Convert string label to integer\n",
        "    label = tf.cond(\n",
        "        tf.equal(record['label'], b'positive'),\n",
        "        lambda: tf.constant(1, dtype=tf.int32),\n",
        "        lambda: tf.constant(0, dtype=tf.int32)\n",
        "    )\n",
        "    return features, label\n",
        "\n",
        "# Create training pipeline\n",
        "training_pipeline = (parsed_dataset\n",
        "                    .map(prepare_for_training)\n",
        "                    .shuffle(buffer_size=100)\n",
        "                    .batch(2)\n",
        "                    .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"‚úÖ Training pipeline created:\")\n",
        "print(\"   TFRecord ‚Üí parse ‚Üí prepare ‚Üí shuffle ‚Üí batch ‚Üí prefetch\")\n",
        "\n",
        "print(\"\\nüìä Training batches:\")\n",
        "for i, (batch_features, batch_labels) in enumerate(training_pipeline):\n",
        "    print(f\"   Batch {i+1}:\")\n",
        "    print(f\"     Features shape: {batch_features.shape}\")\n",
        "    print(f\"     Labels: {batch_labels.numpy()}\")\n",
        "    print(f\"     Features: {batch_features.numpy()}\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ COMPRESSION BENEFITS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Create compressed TFRecord\n",
        "compressed_filename = 'sample_data_compressed.tfrecord'\n",
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "\n",
        "with tf.io.TFRecordWriter(compressed_filename, options=options) as writer:\n",
        "    for sample in samples:\n",
        "        feature_dict = {\n",
        "            'id': _int64_feature(sample['id']),\n",
        "            'features': _float_list_feature(sample['features']),\n",
        "            'label': _bytes_feature(sample['label'].encode('utf-8')),\n",
        "            'score': _float_feature(sample['score'])\n",
        "        }\n",
        "\n",
        "        example = tf.train.Example(\n",
        "            features=tf.train.Features(feature=feature_dict)\n",
        "        )\n",
        "        writer.write(example.SerializeToString())\n",
        "\n",
        "# Compare file sizes\n",
        "original_size = os.path.getsize(tfrecord_filename)\n",
        "compressed_size = os.path.getsize(compressed_filename)\n",
        "compression_ratio = (1 - compressed_size / original_size) * 100\n",
        "\n",
        "print(f\"‚úÖ Compression results:\")\n",
        "print(f\"   üìÑ Original: {original_size} bytes\")\n",
        "print(f\"   üì¶ Compressed: {compressed_size} bytes\")\n",
        "print(f\"   üíæ Compression: {compression_ratio:.1f}% reduction\")\n",
        "\n",
        "# Cleanup\n",
        "os.unlink(tfrecord_filename)\n",
        "os.unlink(compressed_filename)\n",
        "print(f\"\\nüßπ Files cleaned up\")\n",
        "\n",
        "print(\"\\n‚úÖ TFRecord examples complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477b9a8d",
      "metadata": {
        "id": "477b9a8d"
      },
      "source": [
        "---\n",
        "\n",
        "# ü§ù 6. Integrasi dengan Keras\n",
        "\n",
        "## üîó Seamless Integration\n",
        "\n",
        "tf.data Dataset dapat langsung digunakan dengan Keras:\n",
        "\n",
        "```python\n",
        "# Dataset training\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "                .shuffle(1000)\n",
        "                .batch(32)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Train model\n",
        "model.fit(train_dataset, epochs=10)\n",
        "```\n",
        "\n",
        "### üéØ Keuntungan:\n",
        "- ‚úÖ No need untuk manual batching\n",
        "- ‚úÖ Automatic prefetching\n",
        "- ‚úÖ Memory efficient untuk dataset besar\n",
        "- ‚úÖ Reproducible dengan random seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "af18e3af",
      "metadata": {
        "id": "af18e3af",
        "outputId": "8c4d0cc7-6f0f-4f54-888c-0c3ce4c2cb7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ù KERAS INTEGRATION\n",
            "==================================================\n",
            "üìä Creating synthetic classification dataset...\n",
            "‚úÖ Dataset created: 1000 samples, 20 features, 3 classes\n",
            "üìã Train: 800 samples\n",
            "üìã Test:  200 samples\n",
            "\n",
            "1Ô∏è‚É£ CREATE OPTIMIZED DATASETS\n",
            "--------------------------------\n",
            "‚úÖ Datasets created:\n",
            "   üîÑ Train: shuffle ‚Üí batch ‚Üí prefetch\n",
            "   üìä Val:   batch ‚Üí prefetch\n",
            "\n",
            "2Ô∏è‚É£ CREATE SIMPLE MODEL\n",
            "-------------------------\n",
            "‚úÖ Model created:\n",
            "   üìä Architecture: 20 ‚Üí 64 ‚Üí 32 ‚Üí 3\n",
            "   üéØ Task: Multi-class classification\n",
            "\n",
            "3Ô∏è‚É£ TRAIN WITH tf.data\n",
            "----------------------\n",
            "üöÄ Training model...\n",
            "Epoch 1/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.3195 - loss: 1.2229 - val_accuracy: 0.3700 - val_loss: 1.1022\n",
            "Epoch 2/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3583 - loss: 1.1171 - val_accuracy: 0.3850 - val_loss: 1.0914\n",
            "Epoch 3/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3567 - loss: 1.0952 - val_accuracy: 0.3800 - val_loss: 1.0964\n",
            "\n",
            "‚úÖ Training complete!\n",
            "\n",
            "4Ô∏è‚É£ EVALUATE PERFORMANCE\n",
            "-------------------------\n",
            "üìä Test Results:\n",
            "   Loss: 1.0964\n",
            "   Accuracy: 0.3800\n",
            "\n",
            "5Ô∏è‚É£ BATCH PREDICTION\n",
            "--------------------\n",
            "üîÆ Batch predictions:\n",
            "   Batch size: 32\n",
            "   True labels: [0 2 2 1 1]\n",
            "   Predictions: [1 1 2 1 1]\n",
            "   Batch accuracy: 0.4375\n",
            "\n",
            "üí° KEY BENEFITS OF tf.data + Keras:\n",
            "-----------------------------------\n",
            "‚úÖ Automatic batching and prefetching\n",
            "‚úÖ Memory efficient for large datasets\n",
            "‚úÖ No need to load entire dataset in memory\n",
            "‚úÖ Seamless integration with model.fit()\n",
            "‚úÖ Support for validation_data parameter\n",
            "‚úÖ Built-in support for steps_per_epoch\n",
            "\n",
            "‚úÖ Keras integration complete!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ü§ù 6.1 Keras Integration Example\n",
        "print(\"ü§ù KERAS INTEGRATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create synthetic dataset for demo\n",
        "print(\"üìä Creating synthetic classification dataset...\")\n",
        "n_samples, n_features, n_classes = 1000, 20, 3\n",
        "\n",
        "# Generate synthetic data\n",
        "X_data = tf.random.normal((n_samples, n_features))\n",
        "y_data = tf.random.uniform((n_samples,), maxval=n_classes, dtype=tf.int32)\n",
        "\n",
        "print(f\"‚úÖ Dataset created: {n_samples} samples, {n_features} features, {n_classes} classes\")\n",
        "\n",
        "# Split data (80/20)\n",
        "split_idx = int(0.8 * n_samples)\n",
        "X_train, X_test = X_data[:split_idx], X_data[split_idx:]\n",
        "y_train, y_test = y_data[:split_idx], y_data[split_idx:]\n",
        "\n",
        "print(f\"üìã Train: {len(X_train)} samples\")\n",
        "print(f\"üìã Test:  {len(X_test)} samples\")\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£ CREATE OPTIMIZED DATASETS\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "# Training dataset with full pipeline\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "                .shuffle(buffer_size=1000, seed=42)\n",
        "                .batch(32)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Validation dataset (no shuffle needed)\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "              .batch(32)\n",
        "              .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"‚úÖ Datasets created:\")\n",
        "print(\"   üîÑ Train: shuffle ‚Üí batch ‚Üí prefetch\")\n",
        "print(\"   üìä Val:   batch ‚Üí prefetch\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ CREATE SIMPLE MODEL\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Simple neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model created:\")\n",
        "print(f\"   üìä Architecture: {n_features} ‚Üí 64 ‚Üí 32 ‚Üí {n_classes}\")\n",
        "print(\"   üéØ Task: Multi-class classification\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ TRAIN WITH tf.data\")\n",
        "print(\"-\" * 22)\n",
        "\n",
        "# Train model with tf.data datasets\n",
        "print(\"üöÄ Training model...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£ EVALUATE PERFORMANCE\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(val_dataset, verbose=0)\n",
        "print(f\"üìä Test Results:\")\n",
        "print(f\"   Loss: {test_loss:.4f}\")\n",
        "print(f\"   Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n5Ô∏è‚É£ BATCH PREDICTION\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Make predictions on a batch\n",
        "sample_batch = next(iter(val_dataset))\n",
        "batch_features, batch_labels = sample_batch\n",
        "\n",
        "predictions = model.predict(batch_features, verbose=0)\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"üîÆ Batch predictions:\")\n",
        "print(f\"   Batch size: {len(batch_labels)}\")\n",
        "print(f\"   True labels: {batch_labels.numpy()[:5]}\")\n",
        "print(f\"   Predictions: {predicted_classes.numpy()[:5]}\")\n",
        "\n",
        "# Calculate batch accuracy\n",
        "batch_accuracy = tf.reduce_mean(\n",
        "    tf.cast(tf.equal(predicted_classes, tf.cast(batch_labels, tf.int64)), tf.float32)\n",
        ")\n",
        "print(f\"   Batch accuracy: {batch_accuracy.numpy():.4f}\")\n",
        "\n",
        "print(\"\\nüí° KEY BENEFITS OF tf.data + Keras:\")\n",
        "print(\"-\" * 35)\n",
        "print(\"‚úÖ Automatic batching and prefetching\")\n",
        "print(\"‚úÖ Memory efficient for large datasets\")\n",
        "print(\"‚úÖ No need to load entire dataset in memory\")\n",
        "print(\"‚úÖ Seamless integration with model.fit()\")\n",
        "print(\"‚úÖ Support for validation_data parameter\")\n",
        "print(\"‚úÖ Built-in support for steps_per_epoch\")\n",
        "\n",
        "print(\"\\n‚úÖ Keras integration complete!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac24d43",
      "metadata": {
        "id": "aac24d43"
      },
      "source": [
        "---\n",
        "\n",
        "# üí° 7. Best Practices & Tips\n",
        "\n",
        "## üéØ Performance Best Practices\n",
        "\n",
        "### 1. **Pipeline Order** üîÑ\n",
        "```python\n",
        "# ‚úÖ OPTIMAL ORDER:\n",
        "dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
        "           .shuffle(buffer_size)     # 1. Shuffle first (if needed)\n",
        "           .map(preprocess_fn)       # 2. Apply transformations\n",
        "           .cache()                  # 3. Cache processed data\n",
        "           .batch(batch_size)        # 4. Batch data\n",
        "           .prefetch(AUTOTUNE))      # 5. Prefetch last\n",
        "```\n",
        "\n",
        "### 2. **Memory Management** üíæ\n",
        "- Use `.cache()` hanya jika data muat di memory\n",
        "- Gunakan `.cache('/path/to/disk')` untuk disk caching\n",
        "- Hindari shuffle pada dataset sangat besar\n",
        "\n",
        "### 3. **Parallelization** ‚ö°\n",
        "- Gunakan `num_parallel_calls=tf.data.AUTOTUNE`\n",
        "- Biarkan TensorFlow optimize secara otomatis\n",
        "- Monitor CPU utilization\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Common Pitfalls\n",
        "\n",
        "### ‚ùå Don't Do This:\n",
        "```python\n",
        "# BAD: Shuffle after batch\n",
        "dataset.batch(32).shuffle(1000)  \n",
        "\n",
        "# BAD: Cache before expensive operations  \n",
        "dataset.cache().map(expensive_fn)\n",
        "\n",
        "# BAD: No prefetching\n",
        "dataset.batch(32)  # Missing prefetch()\n",
        "```\n",
        "\n",
        "### ‚úÖ Do This Instead:\n",
        "```python\n",
        "# GOOD: Proper order\n",
        "dataset.shuffle(1000).map(expensive_fn).cache().batch(32).prefetch(AUTOTUNE)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Debugging Tips\n",
        "\n",
        "### 1. **Inspect Dataset**\n",
        "```python\n",
        "# Check first few samples\n",
        "for sample in dataset.take(3):\n",
        "    print(sample)\n",
        "\n",
        "# Check shapes\n",
        "print(dataset.element_spec)\n",
        "```\n",
        "\n",
        "### 2. **Performance Profiling**\n",
        "```python\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for batch in dataset.take(100):\n",
        "    pass\n",
        "print(f\"Time: {time.time() - start:.2f}s\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59042899",
      "metadata": {
        "id": "59042899"
      },
      "source": [
        "---\n",
        "\n",
        "# üéâ Chapter Summary\n",
        "\n",
        "## üìö Apa yang Telah Dipelajari\n",
        "\n",
        "Dalam Chapter 13 ini, kita telah mempelajari:\n",
        "\n",
        "### üî∞ 1. Data API Fundamentals\n",
        "- ‚úÖ Konsep tf.data.Dataset\n",
        "- ‚úÖ Membuat dataset dari berbagai sumber\n",
        "- ‚úÖ Transformasi dasar (map, filter, batch, shuffle)\n",
        "\n",
        "### üìÅ 2. Data Loading\n",
        "- ‚úÖ Loading dari CSV files  \n",
        "- ‚úÖ TextLineDataset untuk custom parsing\n",
        "- ‚úÖ Binary dan structured data formats\n",
        "\n",
        "### üîß 3. Preprocessing\n",
        "- ‚úÖ Numerical preprocessing (normalization, standardization)\n",
        "- ‚úÖ Text preprocessing (tokenization, padding, vectorization)\n",
        "- ‚úÖ Feature engineering techniques\n",
        "\n",
        "### üöÄ 4. Performance Optimization\n",
        "- ‚úÖ Prefetching dan caching\n",
        "- ‚úÖ Parallel processing\n",
        "- ‚úÖ Optimal pipeline ordering\n",
        "- ‚úÖ Memory management\n",
        "\n",
        "### üíæ 5. TFRecord Format\n",
        "- ‚úÖ Creating dan reading TFRecord files\n",
        "- ‚úÖ Protocol buffers serialization\n",
        "- ‚úÖ Compression benefits\n",
        "\n",
        "### ü§ù 6. Keras Integration\n",
        "- ‚úÖ Seamless integration dengan model.fit()\n",
        "- ‚úÖ Training dan validation pipelines\n",
        "- ‚úÖ Batch prediction workflow\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "\n",
        "### üí° **The Golden Pipeline:**\n",
        "```python\n",
        "optimal_pipeline = (\n",
        "    tf.data.Dataset.from_tensor_slices(data)\n",
        "    .shuffle(buffer_size)\n",
        "    .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)  \n",
        "    .cache()\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "```\n",
        "\n",
        "### üöÄ **Performance Mantra:**\n",
        "> \"Shuffle ‚Üí Map ‚Üí Cache ‚Üí Batch ‚Üí Prefetch\"\n",
        "\n",
        "### üíæ **Format Choice:**\n",
        "- üìä **CSV**: Prototyping dan dataset kecil\n",
        "- üíæ **TFRecord**: Production dan dataset besar  \n",
        "- üîÑ **tf.data**: Always untuk training pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## üîÆ Next Steps\n",
        "\n",
        "Setelah menguasai Chapter 13, Anda siap untuk:\n",
        "\n",
        "- üß† **Chapter 14**: Convolutional Neural Networks\n",
        "- üéØ **Chapter 15**: Processing Sequences using RNNs  \n",
        "- üöÄ **Advanced Topics**: Custom training loops, distributed training\n",
        "- üíº **Real Projects**: Apply tf.data pada dataset real-world\n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Congratulations!\n",
        "\n",
        "üéâ **Selamat!** Anda telah menguasai TensorFlow Data API - foundational skill untuk deep learning yang scalable dan efisien!\n",
        "\n",
        "**Remember**:\n",
        "> *\"Good data pipelines are the backbone of successful deep learning projects\"*\n",
        "\n",
        "---\n",
        "\n",
        "### üìñ Resources for Further Learning\n",
        "\n",
        "- üìö [TensorFlow Data Guide](https://www.tensorflow.org/guide/data)\n",
        "- üé• [tf.data Best Practices](https://www.tensorflow.org/guide/data_performance)\n",
        "- üíª [TensorFlow Datasets](https://www.tensorflow.org/datasets)\n",
        "- üî¨ [Advanced tf.data Techniques](https://www.tensorflow.org/guide/data_performance)\n",
        "\n",
        "**Happy Learning! üöÄ**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe980cc",
      "metadata": {
        "id": "dfe980cc"
      },
      "source": [
        "## 6. Integration dengan Keras ü§ù\n",
        "\n",
        "### 6.1 Dataset untuk Training\n",
        "\n",
        "tf.data.Dataset dapat langsung digunakan dengan:\n",
        "- `model.fit()` untuk training\n",
        "- `model.evaluate()` untuk evaluation  \n",
        "- `model.predict()` untuk prediction\n",
        "\n",
        "### 6.2 Preprocessing Layers\n",
        "\n",
        "Keras juga menyediakan preprocessing layers yang dapat diintegrasikan dalam model:\n",
        "- `tf.keras.layers.Normalization`\n",
        "- `tf.keras.layers.StringLookup`  \n",
        "- `tf.keras.layers.TextVectorization`\n",
        "- `tf.keras.layers.CategoryEncoding`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "870867a7",
      "metadata": {
        "id": "870867a7",
        "outputId": "d89a6664-77f1-43ee-c53a-f96ef4ed316e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== KERAS INTEGRATION ===\n",
            "=== PREPARING DATASET FOR KERAS ===\n",
            "Data shape: (1000, 4)\n",
            "Labels shape: (1000,)\n",
            "Class distribution: [504 496]\n",
            "Training batches: ~25\n",
            "Validation batches: ~6\n",
            "\n",
            "=== KERAS MODEL WITH tf.data ===\n",
            "Model architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ           \u001b[38;5;34m320\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_10 (\u001b[38;5;33mDense\u001b[0m)                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ         \u001b[38;5;34m2,080\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_11 (\u001b[38;5;33mDense\u001b[0m)                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              ‚îÇ            \u001b[38;5;34m33\u001b[0m ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              ‚îÇ            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,433\u001b[0m (9.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,433</span> (9.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,433\u001b[0m (9.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,433</span> (9.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING WITH tf.data ===\n",
            "Epoch 1/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5762 - loss: 0.6740 - val_accuracy: 0.9200 - val_loss: 0.5204\n",
            "Epoch 2/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8709 - loss: 0.4993 - val_accuracy: 0.9350 - val_loss: 0.3773\n",
            "Epoch 3/3\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9287 - loss: 0.3300 - val_accuracy: 0.9700 - val_loss: 0.2449\n",
            "‚úÖ Training completed!\n",
            "Final training accuracy: 0.9262\n",
            "Final validation accuracy: 0.9700\n",
            "\n",
            "=== PREPROCESSING LAYERS ===\n",
            "Text vectorization example:\n",
            "Original texts: ['I love machine learning', 'TensorFlow is great']\n",
            "Vectorized: [[ 9  8  7  2  0  0  0  0  0  0]\n",
            " [ 5  3 10  0  0  0  0  0  0  0]]\n",
            "\n",
            "=== NORMALIZATION LAYER ===\n",
            "Original data stats:\n",
            "  Mean: [3.7219698  4.47298011 3.80837349]\n",
            "  Std: [10.61660862 10.25860211  9.24816152]\n",
            "Normalized data (first 5 samples):\n",
            "[[-0.692962    0.02095655  0.14833167]\n",
            " [ 0.5655604  -1.2810286   0.76959074]\n",
            " [-2.4269414  -0.5626328  -0.39911845]\n",
            " [ 0.7169237   0.07343382 -1.3909796 ]\n",
            " [-0.8074049   0.2450451   0.04335492]]\n",
            "Normalized mean: [-0.5289648  -0.3008452  -0.16576415]\n",
            "Normalized std: [1.1363945  0.5605229  0.71733993]\n",
            "\n",
            "‚úÖ Keras integration examples completed!\n"
          ]
        }
      ],
      "source": [
        "# ü§ù Keras Integration Examples\n",
        "print(\"=== KERAS INTEGRATION ===\")\n",
        "\n",
        "# 1. Create sample dataset\n",
        "print(\"=== PREPARING DATASET FOR KERAS ===\")\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X_data = np.random.randn(1000, 4)  # 1000 samples, 4 features\n",
        "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int)  # Binary classification\n",
        "\n",
        "print(f\"Data shape: {X_data.shape}\")\n",
        "print(f\"Labels shape: {y_data.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y_data)}\")\n",
        "\n",
        "# Create tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_dataset = (dataset\n",
        "                       .shuffle(buffer_size=1000)\n",
        "                       .batch(32)\n",
        "                       .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Split into train/validation\n",
        "train_size = int(0.8 * len(X_data))\n",
        "train_dataset = preprocessed_dataset.take(train_size // 32)\n",
        "val_dataset = preprocessed_dataset.skip(train_size // 32)\n",
        "\n",
        "print(f\"Training batches: ~{train_size // 32}\")\n",
        "print(f\"Validation batches: ~{(len(X_data) - train_size) // 32}\")\n",
        "\n",
        "# 2. Create and train Keras model\n",
        "print(\"\\n=== KERAS MODEL WITH tf.data ===\")\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Train with tf.data.Dataset\n",
        "print(\"\\n=== TRAINING WITH tf.data ===\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training completed!\")\n",
        "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# 3. Demonstrate preprocessing layers\n",
        "print(\"\\n=== PREPROCESSING LAYERS ===\")\n",
        "\n",
        "# Example with text data\n",
        "text_data = [\n",
        "    \"I love machine learning\",\n",
        "    \"TensorFlow is great\",\n",
        "    \"Deep learning rocks\",\n",
        "    \"AI is the future\"\n",
        "]\n",
        "\n",
        "# Create text vectorization layer\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=1000,\n",
        "    output_sequence_length=10\n",
        ")\n",
        "\n",
        "# Adapt to text data\n",
        "text_vectorizer.adapt(text_data)\n",
        "\n",
        "print(\"Text vectorization example:\")\n",
        "print(\"Original texts:\", text_data[:2])\n",
        "vectorized = text_vectorizer(text_data[:2])\n",
        "print(\"Vectorized:\", vectorized.numpy())\n",
        "\n",
        "# Example with normalization layer\n",
        "print(\"\\n=== NORMALIZATION LAYER ===\")\n",
        "normalizer = tf.keras.layers.Normalization()\n",
        "\n",
        "# Adapt to data\n",
        "sample_data = np.random.randn(100, 3) * 10 + 5  # Mean‚âà5, Std‚âà10\n",
        "normalizer.adapt(sample_data)\n",
        "\n",
        "print(\"Original data stats:\")\n",
        "print(f\"  Mean: {np.mean(sample_data, axis=0)}\")\n",
        "print(f\"  Std: {np.std(sample_data, axis=0)}\")\n",
        "\n",
        "normalized = normalizer(sample_data[:5])\n",
        "print(\"Normalized data (first 5 samples):\")\n",
        "print(normalized.numpy())\n",
        "print(f\"Normalized mean: {np.mean(normalized.numpy(), axis=0)}\")\n",
        "print(f\"Normalized std: {np.std(normalized.numpy(), axis=0)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Keras integration examples completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c61f96e",
      "metadata": {
        "id": "4c61f96e"
      },
      "source": [
        "## 7. Best Practices & Summary üìã\n",
        "\n",
        "### 7.1 Data Pipeline Best Practices\n",
        "\n",
        "1. **Urutan Optimal Transformasi**:\n",
        "   ```python\n",
        "   dataset = (tf.data.Dataset.from_generator(...)\n",
        "              .map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
        "              .cache()  # Cache after expensive operations\n",
        "              .shuffle(buffer_size)\n",
        "              .batch(batch_size)\n",
        "              .prefetch(AUTOTUNE))\n",
        "   ```\n",
        "\n",
        "2. **Performance Tips**:\n",
        "   - Gunakan `num_parallel_calls=AUTOTUNE` untuk operasi map\n",
        "   - Implementasikan `prefetch()` di akhir pipeline\n",
        "   - Cache dataset setelah operasi mahal, sebelum shuffle\n",
        "   - Gunakan TFRecord untuk dataset besar\n",
        "   - Batch sebelum expensive transformations jika memungkinkan\n",
        "\n",
        "3. **Memory Management**:\n",
        "   - Hindari `.cache()` untuk dataset yang terlalu besar\n",
        "   - Gunakan generator untuk data yang tidak muat di memory\n",
        "   - Pertimbangkan `.cache(filename)` untuk cache ke disk\n",
        "\n",
        "### 7.2 Common Patterns\n",
        "\n",
        "- **Image Data**: `map(decode_image) ‚Üí cache() ‚Üí shuffle() ‚Üí batch() ‚Üí prefetch()`\n",
        "- **Text Data**: `map(tokenize) ‚Üí padded_batch() ‚Üí prefetch()`\n",
        "- **Structured Data**: `map(normalize) ‚Üí batch() ‚Üí prefetch()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9c10fd17",
      "metadata": {
        "id": "9c10fd17",
        "outputId": "fb89a0a3-dda7-4df8-ac51-5ab394be081b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CHAPTER 13 SUMMARY ===\n",
            "üéØ Loading and Preprocessing Data with TensorFlow\n",
            "\n",
            "üìö KEY CONCEPTS LEARNED:\n",
            "   ‚úÖ 1. tf.data.Dataset fundamentals and creation methods\n",
            "   ‚úÖ 2. Dataset transformations (map, filter, batch, shuffle)\n",
            "   ‚úÖ 3. Loading data from various sources (CSV, TFRecord, etc.)\n",
            "   ‚úÖ 4. Data preprocessing and feature engineering\n",
            "   ‚úÖ 5. Text preprocessing and vectorization\n",
            "   ‚úÖ 6. Performance optimization (prefetch, cache, parallel)\n",
            "   ‚úÖ 7. TFRecord format for efficient storage\n",
            "   ‚úÖ 8. Integration with Keras models and preprocessing layers\n",
            "\n",
            "üöÄ PERFORMANCE OPTIMIZATION CHECKLIST:\n",
            "   üîß Use num_parallel_calls=AUTOTUNE for map operations\n",
            "   üîß Implement prefetch(AUTOTUNE) at end of pipeline\n",
            "   üîß Cache expensive operations with cache()\n",
            "   üîß Shuffle with appropriate buffer_size\n",
            "   üîß Use TFRecord for large datasets\n",
            "   üîß Batch data for efficient processing\n",
            "   üîß Consider preprocessing layers in model\n",
            "\n",
            "üìä COMMON DATA PIPELINE PATTERN:\n",
            "\n",
            "   dataset = (tf.data.Dataset.from_source(...)\n",
            "              .map(preprocessing_fn, num_parallel_calls=AUTOTUNE)\n",
            "              .cache()\n",
            "              .shuffle(buffer_size=1000)\n",
            "              .batch(batch_size)\n",
            "              .prefetch(AUTOTUNE))\n",
            "\n",
            "üéâ NEXT STEPS:\n",
            "   üìà Practice with real-world datasets\n",
            "   üìà Experiment with different preprocessing techniques\n",
            "   üìà Benchmark pipeline performance\n",
            "   üìà Explore advanced tf.data features\n",
            "   üìà Integrate with complex model architectures\n",
            "\n",
            "==================================================\n",
            "üéä CONGRATULATIONS! You've completed Chapter 13!\n",
            "   You now understand TensorFlow Data API and\n",
            "   can build efficient data pipelines for deep learning!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# üìã Chapter 13 Summary & Best Practices\n",
        "print(\"=== CHAPTER 13 SUMMARY ===\")\n",
        "print(\"üéØ Loading and Preprocessing Data with TensorFlow\")\n",
        "print()\n",
        "\n",
        "print(\"üìö KEY CONCEPTS LEARNED:\")\n",
        "concepts = [\n",
        "    \"1. tf.data.Dataset fundamentals and creation methods\",\n",
        "    \"2. Dataset transformations (map, filter, batch, shuffle)\",\n",
        "    \"3. Loading data from various sources (CSV, TFRecord, etc.)\",\n",
        "    \"4. Data preprocessing and feature engineering\",\n",
        "    \"5. Text preprocessing and vectorization\",\n",
        "    \"6. Performance optimization (prefetch, cache, parallel)\",\n",
        "    \"7. TFRecord format for efficient storage\",\n",
        "    \"8. Integration with Keras models and preprocessing layers\"\n",
        "]\n",
        "\n",
        "for concept in concepts:\n",
        "    print(f\"   ‚úÖ {concept}\")\n",
        "\n",
        "print(\"\\nüöÄ PERFORMANCE OPTIMIZATION CHECKLIST:\")\n",
        "optimizations = [\n",
        "    \"Use num_parallel_calls=AUTOTUNE for map operations\",\n",
        "    \"Implement prefetch(AUTOTUNE) at end of pipeline\",\n",
        "    \"Cache expensive operations with cache()\",\n",
        "    \"Shuffle with appropriate buffer_size\",\n",
        "    \"Use TFRecord for large datasets\",\n",
        "    \"Batch data for efficient processing\",\n",
        "    \"Consider preprocessing layers in model\"\n",
        "]\n",
        "\n",
        "for opt in optimizations:\n",
        "    print(f\"   üîß {opt}\")\n",
        "\n",
        "print(\"\\nüìä COMMON DATA PIPELINE PATTERN:\")\n",
        "print(\"\"\"\n",
        "   dataset = (tf.data.Dataset.from_source(...)\n",
        "              .map(preprocessing_fn, num_parallel_calls=AUTOTUNE)\n",
        "              .cache()\n",
        "              .shuffle(buffer_size=1000)\n",
        "              .batch(batch_size)\n",
        "              .prefetch(AUTOTUNE))\n",
        "\"\"\")\n",
        "\n",
        "print(\"üéâ NEXT STEPS:\")\n",
        "next_steps = [\n",
        "    \"Practice with real-world datasets\",\n",
        "    \"Experiment with different preprocessing techniques\",\n",
        "    \"Benchmark pipeline performance\",\n",
        "    \"Explore advanced tf.data features\",\n",
        "    \"Integrate with complex model architectures\"\n",
        "]\n",
        "\n",
        "for step in next_steps:\n",
        "    print(f\"   üìà {step}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéä CONGRATULATIONS! You've completed Chapter 13!\")\n",
        "print(\"   You now understand TensorFlow Data API and\")\n",
        "print(\"   can build efficient data pipelines for deep learning!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}