{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ffff68",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning\n",
    "# Reinforcement Learning\n",
    "\n",
    "## Pengantar\n",
    "\n",
    "Reinforcement Learning (RL) adalah paradigma machine learning yang berbeda dari \n",
    "supervised dan unsupervised learning. Dalam RL, agent belajar melalui interaksi \n",
    "dengan environment untuk memaksimalkan reward.\n",
    "\n",
    "**Konsep Kunci:**\n",
    "- **Agent**: Yang membuat keputusan\n",
    "- **Environment**: Dunia tempat agent beroperasi\n",
    "- **State**: Situasi current dari environment\n",
    "- **Action**: Pilihan yang bisa diambil agent\n",
    "- **Reward**: Feedback dari environment\n",
    "- **Policy**: Strategi agent dalam memilih action\n",
    "\n",
    "**Aplikasi:**\n",
    "- Game playing (AlphaGo, OpenAI Five)\n",
    "- Robotics\n",
    "- Autonomous vehicles\n",
    "- Trading algorithms\n",
    "- Resource allocation\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd64691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Keras version: 3.10.0\n",
      "Gymnasium version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "# Setup dan Import Libraries untuk Reinforcement Learning\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, Model, Input, optimizers, losses, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Gymnasium version:\", gym.__version__)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5112c",
   "metadata": {},
   "source": [
    "## Mengapa RL Penting?\n",
    "\n",
    "Reinforcement Learning adalah paradigma pembelajaran di mana seorang **agent** (agen) belajar mengambil **actions** (tindakan) dalam suatu **environment** (lingkungan) untuk memaksimalkan **rewards** (hadiah) yang diterima. Konsep ini terinspirasi dari cara manusia dan hewan belajar melalui trial and error.\n",
    "\n",
    "**Aplikasi Real-World:**\n",
    "- **Game Playing**: AlphaGo, OpenAI Five, StarCraft II\n",
    "- **Robotics**: Robot navigation, manipulation, autonomous vehicles\n",
    "- **Finance**: Trading algorithms, portfolio optimization\n",
    "- **Healthcare**: Treatment optimization, drug discovery\n",
    "- **Recommendation Systems**: Personalized content delivery\n",
    "- **Resource Management**: Cloud computing, energy optimization\n",
    "\n",
    "**Perbedaan dengan Supervised Learning:**\n",
    "- **No labels**: Agent harus menemukan strategy sendiri\n",
    "- **Sequential decision making**: Actions mempengaruhi future states\n",
    "- **Delayed rewards**: Feedback mungkin tidak immediate\n",
    "- **Exploration vs Exploitation**: Balance antara trying new things vs using known good strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa9987",
   "metadata": {},
   "source": [
    "## 1. Environment Setup - CartPole\n",
    "\n",
    "Mari mulai dengan **CartPole environment** dari OpenAI Gym - salah satu environment klasik untuk belajar RL. Tujuannya adalah menjaga pole tetap seimbang di atas cart yang bergerak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f0faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action Space: Discrete(2)\n",
      "\n",
      "Observation Components:\n",
      "0: Cart Position (-4.8 to 4.8)\n",
      "1: Cart Velocity (-Inf to Inf)\n",
      "2: Pole Angle (-0.418 to 0.418 radians, ~24 degrees)\n",
      "3: Pole Angular Velocity (-Inf to Inf)\n",
      "\n",
      "Action Space:\n",
      "0: Push cart to the LEFT\n",
      "1: Push cart to the RIGHT\n",
      "\n",
      "Testing environment...\n",
      "Initial observation: [ 0.000318    0.03244925 -0.01243256  0.04073001]\n",
      "Step 1: Action=1, Reward=1.0, Done=False\n",
      "  Observation: [ 0.00096699  0.22774725 -0.01161796 -0.25584945]\n",
      "Step 2: Action=1, Reward=1.0, Done=False\n",
      "  Observation: [ 0.00552193  0.42303315 -0.01673494 -0.5521741 ]\n",
      "Step 3: Action=1, Reward=1.0, Done=False\n",
      "  Observation: [ 0.0139826   0.6183861  -0.02777843 -0.8500823 ]\n",
      "Step 4: Action=1, Reward=1.0, Done=False\n",
      "  Observation: [ 0.02635032  0.81387556 -0.04478007 -1.1513692 ]\n",
      "Step 5: Action=1, Reward=1.0, Done=False\n",
      "  Observation: [ 0.04262783  1.0095522  -0.06780746 -1.457751  ]\n"
     ]
    }
   ],
   "source": [
    "# Setup CartPole Environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Explore environment properties\n",
    "print(\"Environment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "\n",
    "# Observation space explanation\n",
    "print(\"\\nObservation Components:\")\n",
    "print(\"0: Cart Position (-4.8 to 4.8)\")\n",
    "print(\"1: Cart Velocity (-Inf to Inf)\") \n",
    "print(\"2: Pole Angle (-0.418 to 0.418 radians, ~24 degrees)\")\n",
    "print(\"3: Pole Angular Velocity (-Inf to Inf)\")\n",
    "\n",
    "print(\"\\nAction Space:\")\n",
    "print(\"0: Push cart to the LEFT\")\n",
    "print(\"1: Push cart to the RIGHT\")\n",
    "\n",
    "# Test environment\n",
    "print(\"\\nTesting environment...\")\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation: {obs}\")\n",
    "\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(f\"Step {i+1}: Action={action}, Reward={reward}, Done={done}\")\n",
    "    print(f\"  Observation: {obs}\")\n",
    "    \n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "        print(\"  Episode terminated, environment reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39291",
   "metadata": {},
   "source": [
    "## 2. Policy Search - Random vs Basic Policies\n",
    "\n",
    "Sebelum menggunakan algoritma RL yang sophisticated, mari kita mulai dengan policies sederhana untuk memahami baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e98181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Policy Implementation\n",
    "def run_random_episode(env):\n",
    "    \"\"\"Run single episode dengan random policy\"\"\"\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(200):  # Max 200 steps\n",
    "        # Random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Execute action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward, step + 1\n",
    "\n",
    "# Test random policy\n",
    "rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(100):\n",
    "    reward, length = run_random_episode(env)\n",
    "    rewards.append(reward)\n",
    "    episode_lengths.append(length)\n",
    "\n",
    "print(f\"Random Policy Results:\")\n",
    "print(f\"Average reward: {np.mean(rewards):.2f} (+/- {np.std(rewards):.2f})\")\n",
    "print(f\"Average episode length: {np.mean(episode_lengths):.2f}\")\n",
    "print(f\"Best performance: {np.max(rewards)}\")\n",
    "\n",
    "# Basic Policy Implementation\n",
    "def basic_policy(obs):\n",
    "    \"\"\"Simple heuristic policy based on pole angle\"\"\"\n",
    "    pole_angle = obs[2]\n",
    "    \n",
    "    # If pole is leaning left, push cart left\n",
    "    # If pole is leaning right, push cart right\n",
    "    if pole_angle < 0:\n",
    "        return 0  # Push left\n",
    "    else:\n",
    "        return 1  # Push right\n",
    "\n",
    "def test_policy(env, policy_func, num_episodes=100):\n",
    "    \"\"\"Test policy function\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = policy_func(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test basic policy\n",
    "basic_rewards = test_policy(env, basic_policy)\n",
    "print(f\"\\nBasic Policy Results:\")\n",
    "print(f\"Average reward: {np.mean(basic_rewards):.2f} (+/- {np.std(basic_rewards):.2f})\")\n",
    "print(f\"Best performance: {np.max(basic_rewards)}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(rewards, bins=20, alpha=0.7, label='Random Policy')\n",
    "plt.hist(basic_rewards, bins=20, alpha=0.7, label='Basic Policy')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Policy Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([rewards, basic_rewards], labels=['Random', 'Basic'])\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Performance Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723c895",
   "metadata": {},
   "source": [
    "## 3. Neural Network Policies\n",
    "\n",
    "Sekarang mari kita implementasikan policy menggunakan neural network yang dapat belajar dari pengalaman. Ini adalah foundation untuk policy gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf85293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Policy Implementation\n",
    "class NeuralNetworkPolicy:\n",
    "    \"\"\"Simple neural network policy\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs=4, n_hidden=5, n_outputs=2):\n",
    "        self.model = keras.models.Sequential([\n",
    "            keras.layers.Dense(n_hidden, activation=\"relu\", input_shape=[n_inputs]),\n",
    "            keras.layers.Dense(n_outputs, activation=\"softmax\")  # Probability distribution over actions\n",
    "        ])\n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        \"\"\"Choose action based on policy probabilities\"\"\"\n",
    "        obs_batch = obs.reshape(1, -1)\n",
    "        probabilities = self.model.predict(obs_batch, verbose=0)[0]\n",
    "        return np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "# Create neural network policy dengan random weights\n",
    "nn_policy = NeuralNetworkPolicy(n_inputs=4, n_hidden=5, n_outputs=2)\n",
    "\n",
    "def test_nn_policy(env, policy, num_episodes=100):\n",
    "    \"\"\"Test neural network policy\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = policy.choose_action(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test dengan random weights\n",
    "random_nn_rewards = test_nn_policy(env, nn_policy)\n",
    "print(f\"Random NN Policy Results:\")\n",
    "print(f\"Average reward: {np.mean(random_nn_rewards):.2f} (+/- {np.std(random_nn_rewards):.2f})\")\n",
    "print(f\"Best performance: {np.max(random_nn_rewards)}\")\n",
    "\n",
    "# Visualize neural network architecture\n",
    "print(\"\\nNeural Network Architecture:\")\n",
    "nn_policy.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e5610",
   "metadata": {},
   "source": [
    "## 4. Policy Gradients Implementation\n",
    "\n",
    "### Credit Assignment Problem\n",
    "\n",
    "Masalah utama dalam RL adalah **credit assignment problem**: ketika agent mendapat reward, sulit menentukan action mana yang berkontribusi terhadap reward tersebut.\n",
    "\n",
    "**Solusi**: Menggunakan **discounted rewards** dan **advantage estimation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discounted Rewards Implementation\n",
    "def compute_discounted_rewards(rewards, discount_factor=0.95):\n",
    "    \"\"\"\n",
    "    Compute discounted rewards (returns) untuk setiap timestep\n",
    "    \n",
    "    Args:\n",
    "        rewards: list of rewards untuk satu episode\n",
    "        discount_factor: gamma, faktor diskon untuk future rewards\n",
    "        \n",
    "    Returns:\n",
    "        discounted_rewards: array of discounted returns\n",
    "    \"\"\"\n",
    "    discounted = np.array(rewards)\n",
    "    \n",
    "    # Compute discounted rewards dari belakang\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[i] = rewards[i] + discount_factor * discounted[i + 1]\n",
    "    \n",
    "    return discounted\n",
    "\n",
    "# Test discounted rewards\n",
    "test_rewards = [10, 0, -50]\n",
    "discounted = compute_discounted_rewards(test_rewards, discount_factor=0.8)\n",
    "print(f\"Original rewards: {test_rewards}\")\n",
    "print(f\"Discounted rewards: {discounted}\")\n",
    "\n",
    "# Visualize effect of different discount factors\n",
    "rewards_example = [1, 1, 1, 1, 10]  # Small rewards followed by big reward\n",
    "discount_factors = [0.5, 0.8, 0.95, 0.99]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, gamma in enumerate(discount_factors):\n",
    "    discounted_rewards = compute_discounted_rewards(rewards_example, gamma)\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    x = range(len(rewards_example))\n",
    "    plt.bar(x, rewards_example, alpha=0.6, label='Original Rewards')\n",
    "    plt.plot(x, discounted_rewards, 'ro-', label='Discounted Returns')\n",
    "    plt.title(f'Discount Factor Œ≥ = {gamma}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Reward/Return')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPenjelasan Discounted Rewards:\")\n",
    "print(\"- Konsep: Action yang dilakukan lebih awal dalam episode memiliki pengaruh lebih besar terhadap outcome\")\n",
    "print(\"- Formula: R_t = r_t + Œ≥¬∑r_{t+1} + Œ≥¬≤¬∑r_{t+2} + ...\")\n",
    "print(\"- Discount factor (Œ≥): Menentukan seberapa penting future rewards (0.9-0.99 umumnya)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bfae4",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm Implementation\n",
    "\n",
    "REINFORCE adalah algoritma policy gradient yang sederhana namun powerful. Algoritma ini menggunakan Monte Carlo sampling untuk estimate policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE Algorithm Implementation\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE Algorithm untuk Policy Gradient\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs=4, n_hidden=5, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize REINFORCE agent\n",
    "        \n",
    "        Args:\n",
    "            n_inputs: input dimension (observation space)\n",
    "            n_hidden: hidden layer size  \n",
    "            learning_rate: learning rate untuk optimizer\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        \n",
    "        # Build neural network policy\n",
    "        self.model = keras.models.Sequential([\n",
    "            keras.layers.Dense(n_hidden, activation=\"elu\", input_shape=[n_inputs]),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        ])\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Storage untuk training data\n",
    "        self.episode_states = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using current policy\"\"\"\n",
    "        state_batch = state.reshape(1, -1)\n",
    "        prob_left = self.model.predict(state_batch, verbose=0)[0, 0]\n",
    "        \n",
    "        # Sample action berdasarkan probability\n",
    "        action = 0 if np.random.random() < prob_left else 1\n",
    "        \n",
    "        # Store untuk training\n",
    "        self.episode_states.append(state)\n",
    "        self.episode_actions.append(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward untuk current timestep\"\"\"\n",
    "        self.episode_rewards.append(reward)\n",
    "    \n",
    "    def train_episode(self, discount_factor=0.95):\n",
    "        \"\"\"Train policy menggunakan data dari satu episode\"\"\"\n",
    "        if len(self.episode_rewards) == 0:\n",
    "            return\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = compute_discounted_rewards(\n",
    "            self.episode_rewards, discount_factor\n",
    "        )\n",
    "        \n",
    "        # Normalize rewards (advantage estimation)\n",
    "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (\n",
    "            np.std(discounted_rewards) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Convert ke tensors\n",
    "        states = tf.constant(self.episode_states, dtype=tf.float32)\n",
    "        actions = tf.constant(self.episode_actions, dtype=tf.float32)\n",
    "        advantages = tf.constant(discounted_rewards, dtype=tf.float32)\n",
    "        \n",
    "        # Training step\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            left_probabilities = self.model(states)[:, 0]\n",
    "            \n",
    "            # Compute action probabilities berdasarkan chosen actions\n",
    "            action_probabilities = tf.where(\n",
    "                actions == 0,\n",
    "                left_probabilities,      # P(left) jika action = 0\n",
    "                1 - left_probabilities   # P(right) jika action = 1  \n",
    "            )\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            # Loss = -log(œÄ(a|s)) * A(s,a)\n",
    "            loss = -tf.reduce_mean(\n",
    "                tf.math.log(action_probabilities + 1e-8) * advantages\n",
    "            )\n",
    "        \n",
    "        # Compute gradients dan update weights\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.episode_states.clear()\n",
    "        self.episode_actions.clear() \n",
    "        self.episode_rewards.clear()\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "# Buat REINFORCE agent\n",
    "agent = REINFORCEAgent(n_inputs=4, n_hidden=5, learning_rate=0.01)\n",
    "\n",
    "print(\"REINFORCE Agent created!\")\n",
    "print(\"Model Architecture:\")\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb19ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training REINFORCE Agent\n",
    "def train_reinforce_agent(env, agent, num_episodes=500, print_every=100):\n",
    "    \"\"\"\n",
    "    Train REINFORCE agent\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: REINFORCEAgent instance\n",
    "        num_episodes: jumlah training episodes\n",
    "        print_every: interval untuk print progress\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards: list of total rewards per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train agent pada akhir episode\n",
    "        loss = agent.train_episode()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            print(f\"Episode {episode + 1}: Average Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Train agent\n",
    "print(\"Training REINFORCE Agent...\")\n",
    "training_rewards = train_reinforce_agent(env, agent, num_episodes=500)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_rewards)\n",
    "plt.title(\"Training Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# Moving average untuk smoothing\n",
    "window_size = 50\n",
    "if len(training_rewards) >= window_size:\n",
    "    moving_avg = np.convolve(training_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title(f\"Moving Average (window={window_size})\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final 100 episodes average: {np.mean(training_rewards[-100:]):.2f}\")\n",
    "\n",
    "# Test trained agent\n",
    "test_rewards = []\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        # Use policy tanpa exploration (deterministic)\n",
    "        state_batch = state.reshape(1, -1)\n",
    "        prob_left = agent.model.predict(state_batch, verbose=0)[0, 0]\n",
    "        action = 0 if prob_left > 0.5 else 1  # deterministic choice\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    test_rewards.append(total_reward)\n",
    "\n",
    "print(f\"\\nTrained Agent Performance:\")\n",
    "print(f\"Average reward: {np.mean(test_rewards):.2f} (+/- {np.std(test_rewards):.2f})\")\n",
    "print(f\"Best performance: {np.max(test_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe7361",
   "metadata": {},
   "source": [
    "## 5. Q-Learning dan Deep Q-Networks (DQN)\n",
    "\n",
    "### Markov Decision Processes (MDP)\n",
    "\n",
    "Sebelum membahas Q-Learning, penting memahami **Markov Decision Process** yang merupakan framework matematika untuk RL:\n",
    "\n",
    "**Komponen MDP:**\n",
    "- **States (S)**: Himpunan semua possible states\n",
    "- **Actions (A)**: Himpunan semua possible actions  \n",
    "- **Transition Probabilities (P)**: P(s'|s,a) - probabilitas transisi ke state s' dari state s dengan action a\n",
    "- **Rewards (R)**: R(s,a,s') - reward yang diterima untuk transisi tertentu\n",
    "- **Discount Factor (Œ≥)**: Faktor untuk mengurangi importance future rewards\n",
    "\n",
    "### Q-Learning Theory\n",
    "\n",
    "**Q-Function**: Q(s,a) merepresentasikan expected total discounted reward jika mengambil action a di state s dan kemudian mengikuti optimal policy.\n",
    "\n",
    "**Bellman Equation**: Q*(s,a) = E[r + Œ≥ * max Q*(s',a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network (DQN) Implementation\n",
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=4, action_size=2, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize DQN agent\n",
    "        \n",
    "        Args:\n",
    "            state_size: dimensi observation space\n",
    "            action_size: jumlah possible actions\n",
    "            learning_rate: learning rate untuk neural network\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.epsilon = 1.0          # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = 0.95          # discount factor\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Neural network\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build neural network untuk Q-function approximation\"\"\"\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(32, activation='relu', input_shape=(self.state_size,)),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=self.lr),\n",
    "            loss='mse'\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience dalam replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Experience replay training\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample random batch dari memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.array([experience[0] for experience in batch])\n",
    "        actions = np.array([experience[1] for experience in batch])\n",
    "        rewards = np.array([experience[2] for experience in batch])\n",
    "        next_states = np.array([experience[3] for experience in batch])\n",
    "        dones = np.array([experience[4] for experience in batch])\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Next Q-values\n",
    "        next_q_values = self.model.predict(next_states, verbose=0)\n",
    "        \n",
    "        # Update Q-values\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                current_q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                current_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Buat DQN agent\n",
    "dqn_agent = DQNAgent(state_size=4, action_size=2)\n",
    "\n",
    "print(\"DQN Agent created!\")\n",
    "print(\"Model Architecture:\")\n",
    "dqn_agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DQN Agent\n",
    "def train_dqn_agent(env, agent, num_episodes=500, print_every=100):\n",
    "    \"\"\"\n",
    "    Train DQN agent\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI Gym environment\n",
    "        agent: DQNAgent instance\n",
    "        num_episodes: jumlah training episodes\n",
    "        print_every: interval untuk print progress\n",
    "        \n",
    "    Returns:\n",
    "        episode_rewards: list of total rewards per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train agent\n",
    "        if len(agent.memory) > agent.batch_size:\n",
    "            agent.replay()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            print(f\"Episode {episode + 1}: Average Reward = {avg_reward:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Train DQN agent\n",
    "print(\"Training DQN Agent...\")\n",
    "dqn_rewards = train_dqn_agent(env, dqn_agent, num_episodes=500)\n",
    "\n",
    "# Plot DQN training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dqn_rewards)\n",
    "plt.title(\"DQN Training Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "\n",
    "# Moving average\n",
    "window_size = 50\n",
    "if len(dqn_rewards) >= window_size:\n",
    "    dqn_moving_avg = np.convolve(dqn_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(dqn_moving_avg)\n",
    "    plt.title(f\"DQN Moving Average (window={window_size})\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDQN Training completed!\")\n",
    "print(f\"Final 100 episodes average: {np.mean(dqn_rewards[-100:]):.2f}\")\n",
    "print(f\"Final epsilon: {dqn_agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d02c46",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation dan Comparison\n",
    "\n",
    "Mari kita bandingkan performa semua agent yang telah kita implementasikan: Random Policy, Basic Policy, REINFORCE, dan DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a91a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "def test_agent_performance(env, policy_func, num_episodes=100, agent_name=\"Agent\"):\n",
    "    \"\"\"Test agent performance dengan policy function\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = policy_func(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    print(f\"{agent_name} Performance:\")\n",
    "    print(f\"  Average reward: {np.mean(rewards):.2f} (+/- {np.std(rewards):.2f})\")\n",
    "    print(f\"  Best performance: {np.max(rewards)}\")\n",
    "    print(f\"  Success rate (reward >= 195): {np.mean(np.array(rewards) >= 195)*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test basic policy\n",
    "basic_test_rewards = test_agent_performance(env, basic_policy, agent_name=\"Basic Policy\")\n",
    "\n",
    "# Test trained REINFORCE agent\n",
    "def reinforce_policy(state):\n",
    "    state_batch = state.reshape(1, -1)\n",
    "    prob_left = agent.model.predict(state_batch, verbose=0)[0, 0]\n",
    "    return 0 if prob_left > 0.5 else 1\n",
    "\n",
    "reinforce_test_rewards = test_agent_performance(env, reinforce_policy, agent_name=\"REINFORCE\")\n",
    "\n",
    "# Test trained DQN agent  \n",
    "def dqn_policy(state):\n",
    "    q_values = dqn_agent.model.predict(state.reshape(1, -1), verbose=0)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "dqn_test_rewards = test_agent_performance(env, dqn_policy, agent_name=\"DQN\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(basic_test_rewards, bins=20, alpha=0.7, label='Basic Policy')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Basic Policy Performance')\n",
    "plt.axvline(np.mean(basic_test_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(basic_test_rewards):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(reinforce_test_rewards, bins=20, alpha=0.7, label='REINFORCE', color='orange')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('REINFORCE Performance')\n",
    "plt.axvline(np.mean(reinforce_test_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(reinforce_test_rewards):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(dqn_test_rewards, bins=20, alpha=0.7, label='DQN', color='green')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency') \n",
    "plt.title('DQN Performance')\n",
    "plt.axvline(np.mean(dqn_test_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(dqn_test_rewards):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"=== PERFORMANCE SUMMARY ===\")\n",
    "print(f\"Random Policy:     {np.mean(rewards):.1f} ¬± {np.std(rewards):.1f}\")\n",
    "print(f\"Basic Policy:      {np.mean(basic_test_rewards):.1f} ¬± {np.std(basic_test_rewards):.1f}\")\n",
    "print(f\"REINFORCE:         {np.mean(reinforce_test_rewards):.1f} ¬± {np.std(reinforce_test_rewards):.1f}\")\n",
    "print(f\"DQN:               {np.mean(dqn_test_rewards):.1f} ¬± {np.std(dqn_test_rewards):.1f}\")\n",
    "\n",
    "# Learning curves comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_rewards, label='REINFORCE', alpha=0.7)\n",
    "plt.plot(dqn_rewards, label='DQN', alpha=0.7)\n",
    "plt.title('Training Curves Comparison')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Moving averages\n",
    "reinforce_ma = np.convolve(training_rewards, np.ones(50)/50, mode='valid')\n",
    "dqn_ma = np.convolve(dqn_rewards, np.ones(50)/50, mode='valid')\n",
    "plt.plot(reinforce_ma, label='REINFORCE (MA)', linewidth=2)\n",
    "plt.plot(dqn_ma, label='DQN (MA)', linewidth=2)\n",
    "plt.title('Training Curves (Moving Average)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bffb7",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics\n",
    "\n",
    "### Double DQN\n",
    "\n",
    "Double DQN mengatasi masalah overestimation pada standard DQN dengan menggunakan dua network: main network untuk action selection dan target network untuk Q-value estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Implementation\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"Double DQN Agent - mengatasi overestimation bias\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=4, action_size=2, learning_rate=0.001):\n",
    "        super().__init__(state_size, action_size, learning_rate)\n",
    "        \n",
    "        # Target network untuk stabilitas\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_frequency = 100\n",
    "        self.train_step = 0\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights dari main network ke target network\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Experience replay dengan Double DQN update\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.array([e[0] for e in batch])\n",
    "        actions = np.array([e[1] for e in batch])\n",
    "        rewards = np.array([e[2] for e in batch])\n",
    "        next_states = np.array([e[3] for e in batch])\n",
    "        dones = np.array([e[4] for e in batch])\n",
    "        \n",
    "        # Double DQN: gunakan main network untuk action selection\n",
    "        next_q_values_main = self.model.predict(next_states, verbose=0)\n",
    "        next_actions = np.argmax(next_q_values_main, axis=1)\n",
    "        \n",
    "        # Gunakan target network untuk Q-value estimation\n",
    "        next_q_values_target = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                current_q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Double DQN update\n",
    "                current_q_values[i][actions[i]] = rewards[i] + self.gamma * next_q_values_target[i][next_actions[i]]\n",
    "        \n",
    "        self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.train_step += 1\n",
    "        if self.train_step % self.update_target_frequency == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Demo Double DQN (menggunakan epochs yang lebih sedikit untuk demo)\n",
    "print(\"Double DQN Implementation:\")\n",
    "double_dqn_agent = DoubleDQNAgent(state_size=4, action_size=2)\n",
    "print(\"Double DQN agent created with target network!\")\n",
    "\n",
    "# Initialize target network\n",
    "double_dqn_agent.update_target_network()\n",
    "print(\"Target network initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05095082",
   "metadata": {},
   "source": [
    "## 8. Best Practices dan Kesimpulan\n",
    "\n",
    "### Kapan Menggunakan Algoritma Mana?\n",
    "\n",
    "**Policy Gradients (REINFORCE):**\n",
    "- ‚úÖ **Cocok untuk**: Continuous action spaces, stochastic policies\n",
    "- ‚úÖ **Keunggulan**: Sederhana, dapat handle high-dimensional action spaces\n",
    "- ‚ùå **Kelemahan**: Sample inefficient, high variance, slow convergence\n",
    "\n",
    "**Deep Q-Networks (DQN):**\n",
    "- ‚úÖ **Cocok untuk**: Discrete action spaces, deterministic environments\n",
    "- ‚úÖ **Keunggulan**: Sample efficient, stable learning, proven performance\n",
    "- ‚ùå **Kelemahan**: Hanya untuk discrete actions, dapat overestimate Q-values\n",
    "\n",
    "**Actor-Critic Methods:**\n",
    "- ‚úÖ **Cocok untuk**: Balance antara policy gradients dan value-based methods\n",
    "- ‚úÖ **Keunggulan**: Lower variance than PG, dapat handle continuous actions\n",
    "- ‚ùå **Kelemahan**: Lebih kompleks, butuh tuning dua networks\n",
    "\n",
    "### Best Practices untuk RL\n",
    "\n",
    "1. **Environment Design**:\n",
    "   - Pastikan reward signal informatif dan tidak sparse\n",
    "   - Normalisasi observations jika diperlukan\n",
    "   - Set proper episode termination conditions\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Learning rate: mulai dari 1e-3, adjust berdasarkan convergence\n",
    "   - Discount factor: 0.95-0.99 untuk most tasks\n",
    "   - Exploration: start high, decay gradually\n",
    "\n",
    "3. **Training Stability**:\n",
    "   - Monitor training metrics (reward, loss, epsilon)\n",
    "   - Use moving averages untuk smooth evaluation\n",
    "   - Implement early stopping jika performance plateau\n",
    "\n",
    "4. **Debugging Tips**:\n",
    "   - Test random policy baseline\n",
    "   - Visualize episode trajectories\n",
    "   - Check for reward hacking atau unintended behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup dan Summary\n",
    "env.close()\n",
    "\n",
    "print(\"=== CHAPTER 18 SUMMARY ===\")\n",
    "print(\"\\nReinforcement Learning Concepts Covered:\")\n",
    "print(\"1. ‚úÖ Basic RL Concepts: Agent, Environment, State, Action, Reward, Policy\")\n",
    "print(\"2. ‚úÖ Policy Search: Random, Hard-coded, Neural Network policies\")\n",
    "print(\"3. ‚úÖ Policy Gradients: REINFORCE algorithm dan credit assignment\")\n",
    "print(\"4. ‚úÖ Value-Based Methods: Q-Learning dan Deep Q-Networks\")\n",
    "print(\"5. ‚úÖ Advanced Techniques: Double DQN dan target networks\")\n",
    "print(\"6. ‚úÖ Practical Implementation: Training loops, evaluation, comparison\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"- RL berbeda fundamental dari supervised learning - belajar melalui interaction\")\n",
    "print(\"- Trade-off antara exploration vs exploitation adalah central problem dalam RL\")\n",
    "print(\"- Deep Learning memungkinkan RL untuk tackle complex, high-dimensional problems\")\n",
    "print(\"- Choice of algorithm tergantung pada characteristics dari problem\")\n",
    "print(\"- RL membutuhkan careful engineering dan hyperparameter tuning untuk stability\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Explore advanced algorithms: PPO, SAC, A3C\")\n",
    "print(\"- Learn about multi-agent RL\")\n",
    "print(\"- Study real-world applications: robotics, game AI, autonomous systems\")\n",
    "print(\"- Understand RL safety dan alignment\")\n",
    "\n",
    "print(\"\\nüéØ Chapter 18 completed successfully!\")\n",
    "print(\"You now have a solid foundation in Reinforcement Learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
