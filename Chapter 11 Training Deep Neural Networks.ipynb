{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e0921a",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks\n",
    "# Hands-On Machine Learning - Implementasi dan Penjelasan Teoretis\n",
    "\n",
    "## Daftar Isi\n",
    "1. [Pengantar dan Tantangan Deep Networks](#1-pengantar)\n",
    "2. [Masalah Vanishing/Exploding Gradients](#2-vanishing-exploding-gradients)\n",
    "3. [Inisialisasi Weight yang Tepat](#3-inisialisasi-weight)\n",
    "4. [Fungsi Aktivasi Non-Saturating](#4-fungsi-aktivasi)\n",
    "5. [Batch Normalization](#5-batch-normalization)\n",
    "6. [Gradient Clipping](#6-gradient-clipping)\n",
    "7. [Transfer Learning](#7-transfer-learning)\n",
    "8. [Optimizer Modern](#8-optimizer-modern)\n",
    "9. [Learning Rate Scheduling](#9-learning-rate-scheduling)\n",
    "10. [Teknik Regularisasi](#10-regularisasi)\n",
    "11. [Best Practices dan Workflow](#11-best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pengantar dan Tantangan Deep Networks {#1-pengantar}\n",
    "\n",
    "### Mengapa Deep Networks Sulit Dilatih?\n",
    "\n",
    "Deep Neural Networks (DNN) dengan banyak hidden layers menghadapi tantangan khusus:\n",
    "\n",
    "**🔥 Masalah Utama:**\n",
    "- **Vanishing/Exploding Gradients**: Gradients menjadi terlalu kecil atau besar\n",
    "- **Overfitting**: Model mudah memorize data training\n",
    "- **Training Instability**: Konvergensi tidak stabil\n",
    "- **Computational Cost**: Membutuhkan resources besar\n",
    "\n",
    "**🎯 Solusi yang Akan Dipelajari:**\n",
    "- Teknik inisialisasi weight yang tepat\n",
    "- Fungsi aktivasi modern\n",
    "- Batch normalization untuk stabilitas\n",
    "- Regularisasi untuk mencegah overfitting\n",
    "- Optimizer dan scheduling yang efektif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dan Import Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Setup plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"📚 Chapter 11: Training Deep Neural Networks\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(\"\\n🎯 Siap untuk mempelajari teknik training deep networks yang efektif!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1de30f",
   "metadata": {},
   "source": [
    "## 2. Masalah Vanishing/Exploding Gradients {#2-vanishing-exploding-gradients}\n",
    "\n",
    "### Teori\n",
    "Vanishing gradients terjadi ketika gradients menjadi sangat kecil saat backpropagation, menyebabkan layer awal hampir tidak belajar. Exploding gradients adalah kebalikannya - gradients menjadi sangat besar.\n",
    "\n",
    "### Penyebab:\n",
    "1. **Activation Functions**: Sigmoid/tanh yang saturate\n",
    "2. **Weight Initialization**: Inisialisasi yang tidak tepat\n",
    "3. **Network Depth**: Semakin dalam, semakin parah masalahnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrasi Vanishing/Exploding Gradients\n",
    "print(\"=== Demonstrasi Vanishing/Exploding Gradients ===\")\n",
    "\n",
    "# Load dan preprocess California Housing data\n",
    "housing = fetch_california_housing()\n",
    "X_full, y_full = housing.data, housing.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "\n",
    "def create_deep_network(n_layers, activation='sigmoid', initializer='random_normal'):\n",
    "    \"\"\"Buat deep network untuk demonstrasi gradient problems\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=[X_train_scaled.shape[1]]))\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        model.add(keras.layers.Dense(50, activation=activation, kernel_initializer=initializer))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))  # Output layer\n",
    "    return model\n",
    "\n",
    "def analyze_gradients(model, X_batch, y_batch):\n",
    "    \"\"\"Analisis magnitude gradients di setiap layer\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X_batch, training=True)\n",
    "        loss = tf.reduce_mean(tf.square(predictions - y_batch))\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for grad in gradients:\n",
    "        if grad is not None:\n",
    "            norm = tf.norm(grad).numpy()\n",
    "            gradient_norms.append(norm)\n",
    "    \n",
    "    return gradient_norms\n",
    "\n",
    "# Test dengan berbagai kedalaman\n",
    "depths = [3, 5, 8, 10]\n",
    "gradient_results = {}\n",
    "\n",
    "X_batch = X_train_scaled[:32].astype(np.float32)\n",
    "y_batch = y_train[:32].astype(np.float32)\n",
    "\n",
    "for depth in depths:\n",
    "    print(f\"\\nAnalyzing {depth}-layer network...\")\n",
    "    \n",
    "    # Model dengan sigmoid (prone to vanishing gradients)\n",
    "    model = create_deep_network(depth, activation='sigmoid', initializer='random_normal')\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    \n",
    "    grad_norms = analyze_gradients(model, X_batch, y_batch)\n",
    "    gradient_results[depth] = grad_norms\n",
    "    \n",
    "    print(f\"First layer gradient norm: {grad_norms[0]:.6f}\")\n",
    "    print(f\"Last layer gradient norm: {grad_norms[-2]:.6f}\")  # -2 karena bias\n",
    "    if grad_norms[0] > 0:\n",
    "        print(f\"Ratio (first/last): {grad_norms[0]/grad_norms[-2]:.2f}\")\n",
    "\n",
    "# Visualisasi\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, depth in enumerate(depths):\n",
    "    layer_indices = range(0, len(gradient_results[depth]), 2)  # Skip bias layers\n",
    "    weight_gradients = [gradient_results[depth][j] for j in layer_indices]\n",
    "    \n",
    "    axes[i].plot(layer_indices, weight_gradients, 'o-', linewidth=2, markersize=8)\n",
    "    axes[i].set_title(f'{depth}-Layer Network\\nGradient Magnitudes')\n",
    "    axes[i].set_xlabel('Layer Index')\n",
    "    axes[i].set_ylabel('Gradient Norm')\n",
    "    axes[i].set_yscale('log')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Observasi:\")\n",
    "print(\"• Semakin dalam network, gradient semakin kecil di layer awal\")\n",
    "print(\"• Ini adalah masalah VANISHING GRADIENT\")\n",
    "print(\"• Layer awal tidak belajar karena gradient terlalu kecil\")\n",
    "print(\"• Solusi: Inisialisasi weight yang tepat dan activation function yang baik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697dcced",
   "metadata": {},
   "source": [
    "## 3. Inisialisasi Weight yang Tepat {#3-inisialisasi-weight}\n",
    "\n",
    "### Teori\n",
    "Inisialisasi weight yang tepat crucial untuk training yang stabil. Terlalu kecil menyebabkan vanishing gradients, terlalu besar menyebabkan exploding gradients.\n",
    "\n",
    "### Strategi Inisialisasi:\n",
    "\n",
    "1. **Xavier/Glorot Initialization**:\n",
    "   - Untuk sigmoid/tanh: `Var(w) = 1/fan_in` atau `Var(w) = 2/(fan_in + fan_out)`\n",
    "   - Menjaga variance aktivasi tetap konsisten\n",
    "\n",
    "2. **He Initialization**:\n",
    "   - Untuk ReLU: `Var(w) = 2/fan_in`\n",
    "   - Mengkompensasi \"dead\" neurons di ReLU\n",
    "\n",
    "3. **LeCun Initialization**:\n",
    "   - Untuk SELU: `Var(w) = 1/fan_in`\n",
    "   - Untuk self-normalizing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perbandingan Teknik Inisialisasi Weight\n",
    "print(\"=== Perbandingan Inisialisasi Weight ===\")\n",
    "\n",
    "def create_model_with_init(initializer, activation='relu'):\n",
    "    \"\"\"Buat model dengan inisialisasi tertentu\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=[X_train_scaled.shape[1]]),\n",
    "        keras.layers.Dense(128, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(64, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(32, activation=activation, kernel_initializer=initializer),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test berbagai inisialisasi\n",
    "initializers = {\n",
    "    'Random Normal (σ=0.01)': keras.initializers.RandomNormal(stddev=0.01),\n",
    "    'Glorot Normal': keras.initializers.GlorotNormal(),\n",
    "    'He Normal': keras.initializers.HeNormal(),\n",
    "    'LeCun Normal': keras.initializers.LecunNormal()\n",
    "}\n",
    "\n",
    "init_results = {}\n",
    "\n",
    "for name, initializer in initializers.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    model = create_model_with_init(initializer)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    init_results[name] = {\n",
    "        'train_loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        'min_val_loss': min(history.history['val_loss'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Final validation loss: {init_results[name]['final_val_loss']:.4f}\")\n",
    "    print(f\"Best validation loss: {init_results[name]['min_val_loss']:.4f}\")\n",
    "\n",
    "# Visualisasi hasil\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Loss curves\n",
    "for name in initializers.keys():\n",
    "    axes[0].plot(init_results[name]['train_loss'], '--', alpha=0.7, label=f'{name} (train)')\n",
    "    axes[1].plot(init_results[name]['val_loss'], '-', label=f'{name} (val)')\n",
    "\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance comparison\n",
    "names = list(initializers.keys())\n",
    "final_losses = [init_results[name]['final_val_loss'] for name in names]\n",
    "best_losses = [init_results[name]['min_val_loss'] for name in names]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "axes[2].bar(x - width/2, final_losses, width, label='Final Val Loss', alpha=0.8)\n",
    "axes[2].bar(x + width/2, best_losses, width, label='Best Val Loss', alpha=0.8)\n",
    "axes[2].set_title('Performance Comparison')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(names, rotation=45, ha='right')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Inisialisasi:\")\n",
    "print(\"✅ He Normal: Terbaik untuk ReLU networks\")\n",
    "print(\"✅ Glorot Normal: Bagus untuk sigmoid/tanh\")\n",
    "print(\"✅ LeCun Normal: Khusus untuk SELU\")\n",
    "print(\"❌ Random Normal: Tidak optimal untuk deep networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efaa0e4",
   "metadata": {},
   "source": [
    "## 4. Fungsi Aktivasi Non-Saturating {#4-fungsi-aktivasi}\n",
    "\n",
    "### Masalah Activation Functions Klasik\n",
    "Sigmoid dan tanh **saturate** di nilai ekstrem, menyebabkan gradients mendekati 0.\n",
    "\n",
    "### Solusi: Non-Saturating Activations\n",
    "\n",
    "1. **ReLU**: `f(x) = max(0, x)`\n",
    "   - ✅ Simple, fast, no vanishing gradient\n",
    "   - ❌ Dead ReLU problem\n",
    "\n",
    "2. **Leaky ReLU**: `f(x) = max(αx, x)`\n",
    "   - ✅ Solves dead ReLU\n",
    "   - α typically 0.01\n",
    "\n",
    "3. **ELU**: Exponential Linear Unit\n",
    "   - ✅ Smooth, mean ≈ 0\n",
    "   - ❌ Slower computation\n",
    "\n",
    "4. **Swish/SiLU**: `f(x) = x * sigmoid(x)`\n",
    "   - ✅ Self-gated, works well in practice\n",
    "   - State-of-the-art in many applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perbandingan Fungsi Aktivasi\n",
    "print(\"=== Perbandingan Fungsi Aktivasi ===\")\n",
    "\n",
    "# Visualisasi fungsi aktivasi\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "\n",
    "# Define activation functions\n",
    "activations = {\n",
    "    'Sigmoid': lambda x: 1 / (1 + np.exp(-x)),\n",
    "    'Tanh': lambda x: np.tanh(x),\n",
    "    'ReLU': lambda x: np.maximum(0, x),\n",
    "    'Leaky ReLU': lambda x: np.where(x > 0, x, 0.01 * x),\n",
    "    'ELU': lambda x: np.where(x > 0, x, np.exp(x) - 1),\n",
    "    'Swish': lambda x: x * (1 / (1 + np.exp(-x)))\n",
    "}\n",
    "\n",
    "# Plot aktivasi dan derivatives\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot activation functions\n",
    "for i, (name, func) in enumerate(activations.items()):\n",
    "    row, col = i // 3, i % 3\n",
    "    y = func(x)\n",
    "    axes[row, col].plot(x, y, linewidth=3, label=name)\n",
    "    axes[row, col].set_title(f'{name} Activation')\n",
    "    axes[row, col].set_xlabel('x')\n",
    "    axes[row, col].set_ylabel('f(x)')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[row, col].axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Add derivative (approximate)\n",
    "    dx = x[1] - x[0]\n",
    "    dy = np.gradient(y, dx)\n",
    "    axes[row, col].plot(x, dy, '--', alpha=0.7, label=f\"{name}'\")\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test performa berbagai aktivasi\n",
    "print(\"\\nTesting activation functions performance...\")\n",
    "\n",
    "activation_configs = {\n",
    "    'ReLU': 'relu',\n",
    "    'Leaky ReLU': keras.layers.LeakyReLU(alpha=0.01),\n",
    "    'ELU': 'elu',\n",
    "    'Swish': 'swish'\n",
    "}\n",
    "\n",
    "activation_results = {}\n",
    "\n",
    "for name, activation in activation_configs.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    if name == 'Leaky ReLU':\n",
    "        # Special handling for LeakyReLU\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Input(shape=[X_train_scaled.shape[1]]),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "            keras.layers.LeakyReLU(alpha=0.01),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_normal'),\n",
    "            keras.layers.LeakyReLU(alpha=0.01),\n",
    "            keras.layers.Dense(32, kernel_initializer='he_normal'),\n",
    "            keras.layers.LeakyReLU(alpha=0.01),\n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "    else:\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Input(shape=[X_train_scaled.shape[1]]),\n",
    "            keras.layers.Dense(128, activation=activation, kernel_initializer='he_normal'),\n",
    "            keras.layers.Dense(64, activation=activation, kernel_initializer='he_normal'),\n",
    "            keras.layers.Dense(32, activation=activation, kernel_initializer='he_normal'),\n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    activation_results[name] = {\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        'min_val_loss': min(history.history['val_loss'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Final validation loss: {activation_results[name]['final_val_loss']:.4f}\")\n",
    "    print(f\"Best validation loss: {activation_results[name]['min_val_loss']:.4f}\")\n",
    "\n",
    "# Visualisasi hasil\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Learning curves\n",
    "for name in activation_configs.keys():\n",
    "    axes[0].plot(activation_results[name]['val_loss'], label=name, linewidth=2)\n",
    "\n",
    "axes[0].set_title('Validation Loss by Activation Function')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance\n",
    "names = list(activation_configs.keys())\n",
    "final_losses = [activation_results[name]['final_val_loss'] for name in names]\n",
    "best_losses = [activation_results[name]['min_val_loss'] for name in names]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, final_losses, width, label='Final', alpha=0.8)\n",
    "axes[1].bar(x + width/2, best_losses, width, label='Best', alpha=0.8)\n",
    "axes[1].set_title('Performance Comparison')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(names)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Aktivasi:\")\n",
    "print(\"✅ ReLU: Default choice, simple dan efektif\")\n",
    "print(\"✅ Swish: Sering memberikan hasil terbaik\")\n",
    "print(\"✅ Leaky ReLU: Alternatif yang baik untuk ReLU\")\n",
    "print(\"✅ ELU: Bagus untuk deep networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a76704",
   "metadata": {},
   "source": [
    "## 5. Batch Normalization {#5-batch-normalization}\n",
    "\n",
    "### Konsep\n",
    "Batch Normalization menormalkan input setiap layer untuk mengurangi **internal covariate shift** dan mempercepat training.\n",
    "\n",
    "### Formula:\n",
    "1. **Normalize**: `x_norm = (x - μ) / √(σ² + ε)`\n",
    "2. **Scale & Shift**: `y = γ * x_norm + β`\n",
    "\n",
    "### Keuntungan:\n",
    "- ✅ Training lebih cepat dan stabil\n",
    "- ✅ Mengurangi sensitivity terhadap initialization\n",
    "- ✅ Memungkinkan learning rate yang lebih tinggi\n",
    "- ✅ Berfungsi sebagai regularizer\n",
    "- ✅ Mengurangi vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrasi Batch Normalization\n",
    "print(\"=== Batch Normalization Demo ===\")\n",
    "\n",
    "# Load CIFAR-10 untuk demonstrasi yang lebih realistis\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocessing\n",
    "X_train_cifar = X_train_cifar.astype('float32') / 255.0\n",
    "X_test_cifar = X_test_cifar.astype('float32') / 255.0\n",
    "y_train_cifar = keras.utils.to_categorical(y_train_cifar, 10)\n",
    "y_test_cifar = keras.utils.to_categorical(y_test_cifar, 10)\n",
    "\n",
    "# Subset untuk demo cepat\n",
    "X_train_small = X_train_cifar[:5000]\n",
    "y_train_small = y_train_cifar[:5000]\n",
    "X_test_small = X_test_cifar[:1000]\n",
    "y_test_small = y_test_cifar[:1000]\n",
    "\n",
    "print(f\"Data shape: {X_train_small.shape}\")\n",
    "\n",
    "def create_model_without_bn():\n",
    "    \"\"\"Model tanpa Batch Normalization\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def create_model_with_bn():\n",
    "    \"\"\"Model dengan Batch Normalization\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "        keras.layers.Dense(256, kernel_initializer='he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(64, kernel_initializer='he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Perbandingan model\n",
    "models = {\n",
    "    'Without BatchNorm': create_model_without_bn(),\n",
    "    'With BatchNorm': create_model_with_bn()\n",
    "}\n",
    "\n",
    "bn_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Training dengan early stopping\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        patience=5, \n",
    "        restore_best_weights=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_small, y_train_small,\n",
    "        validation_data=(X_test_small, y_test_small),\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    bn_results[name] = {\n",
    "        'train_acc': history.history['accuracy'],\n",
    "        'val_acc': history.history['val_accuracy'],\n",
    "        'train_loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'best_val_acc': max(history.history['val_accuracy']),\n",
    "        'epochs_trained': len(history.history['loss'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Final validation accuracy: {bn_results[name]['final_val_acc']:.4f}\")\n",
    "    print(f\"Best validation accuracy: {bn_results[name]['best_val_acc']:.4f}\")\n",
    "    print(f\"Epochs trained: {bn_results[name]['epochs_trained']}\")\n",
    "\n",
    "# Visualisasi perbandingan\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Training curves\n",
    "for name in models.keys():\n",
    "    axes[0, 0].plot(bn_results[name]['train_acc'], label=f'{name} (train)', linewidth=2)\n",
    "    axes[0, 1].plot(bn_results[name]['val_acc'], label=f'{name} (val)', linewidth=2)\n",
    "    axes[1, 0].plot(bn_results[name]['train_loss'], label=f'{name} (train)', linewidth=2)\n",
    "    axes[1, 1].plot(bn_results[name]['val_loss'], label=f'{name} (val)', linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Training Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Validation Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_title('Training Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_title('Validation Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "metrics = ['Final Acc', 'Best Acc', 'Epochs']\n",
    "without_bn = [bn_results['Without BatchNorm']['final_val_acc'],\n",
    "              bn_results['Without BatchNorm']['best_val_acc'],\n",
    "              bn_results['Without BatchNorm']['epochs_trained']]\n",
    "with_bn = [bn_results['With BatchNorm']['final_val_acc'],\n",
    "           bn_results['With BatchNorm']['best_val_acc'],\n",
    "           bn_results['With BatchNorm']['epochs_trained']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 2].bar(x - width/2, without_bn, width, label='Without BN', alpha=0.8)\n",
    "axes[0, 2].bar(x + width/2, with_bn, width, label='With BN', alpha=0.8)\n",
    "axes[0, 2].set_title('Performance Summary')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels(metrics)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence speed\n",
    "target_acc = 0.4  # Target accuracy for comparison\n",
    "convergence_epochs = []\n",
    "for name in models.keys():\n",
    "    val_accs = bn_results[name]['val_acc']\n",
    "    epoch_reached = next((i for i, acc in enumerate(val_accs) if acc >= target_acc), len(val_accs))\n",
    "    convergence_epochs.append(epoch_reached)\n",
    "\n",
    "axes[1, 2].bar(['Without BN', 'With BN'], convergence_epochs, color=['red', 'blue'], alpha=0.8)\n",
    "axes[1, 2].set_title(f'Epochs to Reach {target_acc:.1f} Accuracy')\n",
    "axes[1, 2].set_ylabel('Epochs')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Batch Normalization:\")\n",
    "print(\"✅ Batch Normalization mempercepat training secara signifikan\")\n",
    "print(\"✅ Mengurangi epochs yang dibutuhkan untuk konvergensi\")\n",
    "print(\"✅ Menghasilkan performa yang lebih baik dan konsisten\")\n",
    "print(\"✅ Membuat training lebih stabil dan robust\")\n",
    "print(\"✅ Mengurangi overfitting (regularization effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058eb86",
   "metadata": {},
   "source": [
    "## 6. Gradient Clipping {#6-gradient-clipping}\n",
    "\n",
    "### Konsep\n",
    "Gradient clipping mencegah **exploding gradients** dengan membatasi magnitude gradients selama backpropagation.\n",
    "\n",
    "### Metode:\n",
    "1. **Clip by Value**: `grad = clip(grad, -threshold, threshold)`\n",
    "2. **Clip by Norm**: `grad = grad * (threshold / ||grad||)` jika `||grad|| > threshold`\n",
    "3. **Global Norm Clipping**: Clip berdasarkan total norm semua gradients\n",
    "\n",
    "### Kapan Digunakan:\n",
    "- RNNs (sangat prone ke exploding gradients)\n",
    "- Very deep networks\n",
    "- Ketika training tidak stabil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrasi Gradient Clipping\n",
    "print(\"=== Gradient Clipping Demo ===\")\n",
    "\n",
    "def create_unstable_model():\n",
    "    \"\"\"Model yang prone ke exploding gradients\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Input(shape=[X_train_scaled.shape[1]]),\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='random_normal'),\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='random_normal'),\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='random_normal'),\n",
    "        keras.layers.Dense(200, activation='relu', kernel_initializer='random_normal'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Test berbagai gradient clipping strategies\n",
    "clipping_configs = {\n",
    "    'No Clipping': keras.optimizers.Adam(learning_rate=0.01),\n",
    "    'Clip by Value (0.5)': keras.optimizers.Adam(learning_rate=0.01, clipvalue=0.5),\n",
    "    'Clip by Norm (1.0)': keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0),\n",
    "    'Global Clip (1.0)': keras.optimizers.Adam(learning_rate=0.01, global_clipnorm=1.0)\n",
    "}\n",
    "\n",
    "clipping_results = {}\n",
    "\n",
    "for name, optimizer in clipping_configs.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    model = create_unstable_model()\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    try:\n",
    "        # Training dengan subset kecil\n",
    "        history = model.fit(\n",
    "            X_train_scaled[:1000], y_train[:1000],\n",
    "            validation_data=(X_val_scaled[:200], y_val[:200]),\n",
    "            epochs=15,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        clipping_results[name] = {\n",
    "            'train_loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'final_val_loss': history.history['val_loss'][-1],\n",
    "            'converged': True,\n",
    "            'stable': not any(np.isnan(history.history['loss']) or np.isinf(history.history['loss']))\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Converged - Final val loss: {clipping_results[name]['final_val_loss']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {str(e)}\")\n",
    "        clipping_results[name] = {\n",
    "            'converged': False,\n",
    "            'stable': False\n",
    "        }\n",
    "\n",
    "# Visualisasi hasil\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Training curves\n",
    "for name in clipping_configs.keys():\n",
    "    if clipping_results[name]['converged']:\n",
    "        axes[0].plot(clipping_results[name]['train_loss'], label=f'{name} (train)', linewidth=2)\n",
    "        axes[1].plot(clipping_results[name]['val_loss'], label=f'{name} (val)', linewidth=2)\n",
    "\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Convergence status\n",
    "methods = list(clipping_configs.keys())\n",
    "converged = [clipping_results[method]['converged'] for method in methods]\n",
    "colors = ['green' if c else 'red' for c in converged]\n",
    "\n",
    "axes[2].bar(methods, converged, color=colors, alpha=0.7)\n",
    "axes[2].set_title('Training Success')\n",
    "axes[2].set_ylabel('Converged (1) or Failed (0)')\n",
    "axes[2].set_xticklabels(methods, rotation=45, ha='right')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Gradient Clipping:\")\n",
    "print(\"✅ Gradient clipping mencegah exploding gradients\")\n",
    "print(\"✅ Global norm clipping sering paling efektif\")\n",
    "print(\"✅ Clip by norm lebih baik dari clip by value\")\n",
    "print(\"✅ Essential untuk RNNs dan very deep networks\")\n",
    "print(\"⚠️  Threshold harus disesuaikan dengan model dan data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b93dd22",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning {#7-transfer-learning}\n",
    "\n",
    "### Konsep\n",
    "Transfer Learning menggunakan model yang sudah dilatih pada dataset besar (seperti ImageNet) sebagai starting point untuk task yang berbeda.\n",
    "\n",
    "### Keuntungan:\n",
    "- ✅ **Reduced Training Time**: Tidak perlu training dari scratch\n",
    "- ✅ **Less Data Required**: Butuh lebih sedikit data untuk task baru  \n",
    "- ✅ **Better Performance**: Sering menghasilkan hasil lebih baik\n",
    "- ✅ **Computational Efficiency**: Menghemat resources\n",
    "\n",
    "### Strategi:\n",
    "1. **Feature Extraction**: Freeze pre-trained layers, train classifier only\n",
    "2. **Fine-tuning**: Unfreeze sebagian/semua layers dengan LR rendah  \n",
    "3. **Gradual Unfreezing**: Bertahap unfreeze dari top ke bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrasi Transfer Learning\n",
    "print(\"=== Transfer Learning Demo ===\")\n",
    "\n",
    "# Load Fashion MNIST untuk demonstrasi transfer learning\n",
    "(X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Preprocessing untuk transfer learning\n",
    "X_train_fashion = X_train_fashion.astype('float32') / 255.0\n",
    "X_test_fashion = X_test_fashion.astype('float32') / 255.0\n",
    "\n",
    "# Convert ke RGB (pre-trained models expect 3 channels)\n",
    "X_train_fashion = np.stack([X_train_fashion] * 3, axis=-1)\n",
    "X_test_fashion = np.stack([X_test_fashion] * 3, axis=-1)\n",
    "\n",
    "# Resize ke 32x32 untuk simplicity (dalam praktik gunakan 224x224)\n",
    "X_train_fashion = tf.image.resize(X_train_fashion, [32, 32]).numpy()\n",
    "X_test_fashion = tf.image.resize(X_test_fashion, [32, 32]).numpy()\n",
    "\n",
    "# Subset untuk demo\n",
    "X_train_small = X_train_fashion[:5000]\n",
    "y_train_small = y_train_fashion[:5000]\n",
    "X_test_small = X_test_fashion[:1000]\n",
    "y_test_small = y_test_fashion[:1000]\n",
    "\n",
    "print(f\"Data shape: {X_train_small.shape}\")\n",
    "\n",
    "# 1. Model from scratch (baseline)\n",
    "def create_cnn_from_scratch():\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Input(shape=[32, 32, 3]),\n",
    "        keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        keras.layers.MaxPooling2D(),\n",
    "        keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# 2. Transfer Learning - Feature Extraction\n",
    "def create_transfer_model_frozen():\n",
    "    # Load pre-trained MobileNetV2 (lighter than VGG untuk demo)\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=[32, 32, 3]\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    return keras.Sequential([\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# 3. Transfer Learning - Fine-tuning\n",
    "def create_transfer_model_finetuned():\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=[32, 32, 3]\n",
    "    )\n",
    "    \n",
    "    # Unfreeze top layers\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze bottom layers\n",
    "    fine_tune_at = 100  # Unfreeze last 50+ layers\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return keras.Sequential([\n",
    "        base_model,\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Test semua strategi\n",
    "transfer_strategies = {\n",
    "    'From Scratch': create_cnn_from_scratch(),\n",
    "    'Transfer - Frozen': create_transfer_model_frozen(),\n",
    "    'Transfer - Fine-tuned': create_transfer_model_finetuned()\n",
    "}\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "for name, model in transfer_strategies.items():\n",
    "    print(f\"\\n=== Training: {name} ===\")\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if 'Fine-tuned' in name:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.0001)  # Lower LR for fine-tuning\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Training\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_small, y_train_small,\n",
    "        validation_data=(X_test_small, y_test_small),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test_small, y_test_small, verbose=0)\n",
    "    \n",
    "    transfer_results[name] = {\n",
    "        'train_acc': history.history['accuracy'],\n",
    "        'val_acc': history.history['val_accuracy'],\n",
    "        'test_acc': test_acc,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'trainable_params': trainable_params\n",
    "    }\n",
    "    \n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Epochs trained: {transfer_results[name]['epochs_trained']}\")\n",
    "\n",
    "# Visualisasi hasil\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training curves\n",
    "for name in transfer_strategies.keys():\n",
    "    axes[0, 0].plot(transfer_results[name]['train_acc'], label=f'{name} (train)', linewidth=2)\n",
    "    axes[0, 1].plot(transfer_results[name]['val_acc'], label=f'{name} (val)', linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Training Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Validation Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "strategies = list(transfer_strategies.keys())\n",
    "test_accs = [transfer_results[s]['test_acc'] for s in strategies]\n",
    "epochs = [transfer_results[s]['epochs_trained'] for s in strategies]\n",
    "\n",
    "axes[1, 0].bar(strategies, test_accs, color=['red', 'blue', 'green'], alpha=0.8)\n",
    "axes[1, 0].set_title('Test Accuracy')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(strategies, epochs, color=['red', 'blue', 'green'], alpha=0.8)\n",
    "axes[1, 1].set_title('Training Epochs')\n",
    "axes[1, 1].set_ylabel('Epochs')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Transfer Learning:\")\n",
    "print(\"✅ Transfer learning konvergen lebih cepat dari training from scratch\")\n",
    "print(\"✅ Feature extraction cocok untuk dataset kecil\")\n",
    "print(\"✅ Fine-tuning dapat memberikan akurasi terbaik\")\n",
    "print(\"✅ Trainable parameters jauh lebih sedikit dengan frozen approach\")\n",
    "print(\"✅ Sangat efektif untuk domains dengan data terbatas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc50b12",
   "metadata": {},
   "source": [
    "## 8. Optimizer Modern {#8-optimizer-modern}\n",
    "\n",
    "### Evolusi Optimizers\n",
    "\n",
    "**Gradient Descent Evolution:**\n",
    "```\n",
    "SGD → SGD + Momentum → RMSprop → Adam → AdamW → Modern variants\n",
    "```\n",
    "\n",
    "### Optimizer Populer:\n",
    "\n",
    "1. **SGD + Momentum**:\n",
    "   - `v = βv + (1-β)∇θ`\n",
    "   - `θ = θ - αv`\n",
    "   - Simple, reliable, good for fine-tuning\n",
    "\n",
    "2. **RMSprop**:\n",
    "   - Adaptive learning rate per parameter\n",
    "   - `s = βs + (1-β)∇θ²`\n",
    "   - `θ = θ - α∇θ/√(s + ε)`\n",
    "\n",
    "3. **Adam**:\n",
    "   - Combines momentum + RMSprop\n",
    "   - Most popular untuk deep learning\n",
    "   - Adaptive learning rates + momentum\n",
    "\n",
    "4. **AdamW**:\n",
    "   - Adam dengan proper weight decay\n",
    "   - Better regularization\n",
    "   - State-of-the-art untuk many tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d657c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perbandingan Optimizer Modern\n",
    "print(\"=== Perbandingan Optimizer Modern ===\")\n",
    "\n",
    "def create_optimizer_test_model():\n",
    "    \"\"\"Model konsisten untuk test optimizer\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Input(shape=[X_train_scaled.shape[1]]),\n",
    "        keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Test berbagai optimizer\n",
    "optimizers_to_test = {\n",
    "    'SGD': keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'AdamW': keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    'Nadam': keras.optimizers.Nadam(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for name, optimizer in optimizers_to_test.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    model = create_optimizer_test_model()\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    optimizer_results[name] = {\n",
    "        'train_loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        'min_val_loss': min(history.history['val_loss']),\n",
    "        'convergence_epoch': np.argmin(history.history['val_loss'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Final validation loss: {optimizer_results[name]['final_val_loss']:.4f}\")\n",
    "    print(f\"Best validation loss: {optimizer_results[name]['min_val_loss']:.4f}\")\n",
    "    print(f\"Best at epoch: {optimizer_results[name]['convergence_epoch'] + 1}\")\n",
    "\n",
    "# Visualisasi perbandingan optimizer\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Training loss curves\n",
    "for name in optimizers_to_test.keys():\n",
    "    axes[0, 0].plot(optimizer_results[name]['train_loss'], label=name, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Validation loss curves\n",
    "for name in optimizers_to_test.keys():\n",
    "    axes[0, 1].plot(optimizer_results[name]['val_loss'], label=name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Validation Loss Comparison')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Final performance\n",
    "names = list(optimizers_to_test.keys())\n",
    "final_losses = [optimizer_results[name]['final_val_loss'] for name in names]\n",
    "best_losses = [optimizer_results[name]['min_val_loss'] for name in names]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 2].bar(x - width/2, final_losses, width, label='Final', alpha=0.8)\n",
    "axes[0, 2].bar(x + width/2, best_losses, width, label='Best', alpha=0.8)\n",
    "axes[0, 2].set_title('Performance Comparison')\n",
    "axes[0, 2].set_ylabel('Validation Loss')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels(names, rotation=45)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence speed\n",
    "convergence_epochs = [optimizer_results[name]['convergence_epoch'] for name in names]\n",
    "axes[1, 0].bar(names, convergence_epochs, color=plt.cm.viridis(np.linspace(0, 1, len(names))))\n",
    "axes[1, 0].set_title('Epochs to Best Performance')\n",
    "axes[1, 0].set_ylabel('Epochs')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Early training comparison (first 10 epochs)\n",
    "for name in ['SGD', 'Adam', 'AdamW']:\n",
    "    if name in optimizer_results:\n",
    "        axes[1, 1].plot(optimizer_results[name]['val_loss'][:10], \n",
    "                       label=name, linewidth=3, marker='o')\n",
    "\n",
    "axes[1, 1].set_title('Early Training Comparison (First 10 Epochs)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Validation Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Stability analysis (standard deviation of loss in last 10 epochs)\n",
    "stability_scores = []\n",
    "for name in names:\n",
    "    last_10_losses = optimizer_results[name]['val_loss'][-10:]\n",
    "    stability = np.std(last_10_losses)\n",
    "    stability_scores.append(stability)\n",
    "\n",
    "axes[1, 2].bar(names, stability_scores, color=plt.cm.plasma(np.linspace(0, 1, len(names))))\n",
    "axes[1, 2].set_title('Training Stability (Lower = More Stable)')\n",
    "axes[1, 2].set_ylabel('Std Dev of Last 10 Epochs')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Optimizer:\")\n",
    "print(\"✅ Adam: Default choice, balance antara speed dan stability\")\n",
    "print(\"✅ AdamW: Better regularization, good untuk overfitting\")\n",
    "print(\"✅ SGD + Momentum: Reliable, good untuk fine-tuning\")\n",
    "print(\"✅ Nadam: Often faster convergence than Adam\")\n",
    "print(\"✅ RMSprop: Good untuk RNNs dan non-stationary problems\")\n",
    "print(\"⚠️  Pilihan optimizer bergantung pada data dan arsitektur model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b609679",
   "metadata": {},
   "source": [
    "## 9. Learning Rate Scheduling {#9-learning-rate-scheduling}\n",
    "\n",
    "### Mengapa LR Scheduling?\n",
    "Learning rate yang tepat crucial untuk konvergensi optimal. Terlalu tinggi → overshooting, terlalu rendah → konvergensi lambat.\n",
    "\n",
    "### Strategi Scheduling:\n",
    "\n",
    "1. **Exponential Decay**: `lr = initial_lr * decay_rate^(step/decay_steps)`\n",
    "2. **Step Decay**: Turunkan LR pada epoch tertentu\n",
    "3. **Cosine Annealing**: LR mengikuti pola cosine, smooth decay\n",
    "4. **Reduce on Plateau**: Adaptive, turunkan LR saat plateau\n",
    "5. **Cyclical LR**: Oscillate LR antara min dan max bounds\n",
    "\n",
    "## 10. Teknik Regularisasi {#10-regularisasi}\n",
    "\n",
    "### Mencegah Overfitting\n",
    "Deep networks dengan jutaan parameter mudah overfitting. Regularisasi essential untuk generalisasi yang baik.\n",
    "\n",
    "### Teknik Utama:\n",
    "\n",
    "1. **Dropout**: Randomly set neurons ke 0 during training\n",
    "2. **Early Stopping**: Stop saat validation loss mulai naik\n",
    "3. **L1/L2 Regularization**: Penalty pada weight magnitude\n",
    "4. **Data Augmentation**: Increase dataset diversity\n",
    "5. **Batch Normalization**: Juga berfungsi sebagai regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrasi Learning Rate Scheduling dan Regularisasi\n",
    "print(\"=== Learning Rate Scheduling & Regularization Demo ===\")\n",
    "\n",
    "# Load Fashion MNIST untuk demo regularisasi\n",
    "(X_train_fmnist, y_train_fmnist), (X_test_fmnist, y_test_fmnist) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_fmnist = X_train_fmnist.astype('float32') / 255.0\n",
    "X_test_fmnist = X_test_fmnist.astype('float32') / 255.0\n",
    "\n",
    "# Subset untuk demo cepat\n",
    "X_train_reg = X_train_fmnist[:10000]\n",
    "y_train_reg = y_train_fmnist[:10000]\n",
    "X_test_reg = X_test_fmnist[:2000]\n",
    "y_test_reg = y_test_fmnist[:2000]\n",
    "\n",
    "print(f\"Regularization demo data shape: {X_train_reg.shape}\")\n",
    "\n",
    "# Learning Rate Schedulers\n",
    "def exponential_decay(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "def step_decay(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return 0.001\n",
    "    elif epoch < 20:\n",
    "        return 0.0005\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "# Model creation functions\n",
    "def create_baseline_model():\n",
    "    \"\"\"Model prone to overfitting\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def create_regularized_model():\n",
    "    \"\"\"Model dengan regularisasi lengkap\"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Test berbagai konfigurasi\n",
    "configs = {\n",
    "    'Baseline (No Reg)': {\n",
    "        'model_fn': create_baseline_model,\n",
    "        'optimizer': keras.optimizers.Adam(0.001),\n",
    "        'callbacks': []\n",
    "    },\n",
    "    'Early Stopping': {\n",
    "        'model_fn': create_baseline_model,\n",
    "        'optimizer': keras.optimizers.Adam(0.001),\n",
    "        'callbacks': [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    },\n",
    "    'LR Scheduling': {\n",
    "        'model_fn': create_baseline_model,\n",
    "        'optimizer': keras.optimizers.Adam(0.001),\n",
    "        'callbacks': [keras.callbacks.LearningRateScheduler(exponential_decay)]\n",
    "    },\n",
    "    'Full Regularization': {\n",
    "        'model_fn': create_regularized_model,\n",
    "        'optimizer': keras.optimizers.Adam(0.001),\n",
    "        'callbacks': [\n",
    "            keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for name, config in configs.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    model = config['model_fn']()\n",
    "    model.compile(\n",
    "        optimizer=config['optimizer'],\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train_reg, y_train_reg,\n",
    "        validation_data=(X_test_reg, y_test_reg),\n",
    "        epochs=25,\n",
    "        batch_size=128,\n",
    "        callbacks=config['callbacks'],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test_reg, y_test_reg, verbose=0)\n",
    "    \n",
    "    reg_results[name] = {\n",
    "        'train_acc': history.history['accuracy'],\n",
    "        'val_acc': history.history['val_accuracy'],\n",
    "        'train_loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "        'test_acc': test_acc,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'overfitting_gap': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Overfitting gap: {reg_results[name]['overfitting_gap']:.4f}\")\n",
    "    print(f\"Epochs trained: {reg_results[name]['epochs_trained']}\")\n",
    "\n",
    "# Visualisasi comprehensive\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Create a 3x4 grid\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "for name in configs.keys():\n",
    "    ax1.plot(reg_results[name]['train_acc'], label=name, linewidth=2)\n",
    "ax1.set_title('Training Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for name in configs.keys():\n",
    "    ax2.plot(reg_results[name]['val_acc'], label=name, linewidth=2)\n",
    "ax2.set_title('Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "for name in configs.keys():\n",
    "    ax3.plot(reg_results[name]['train_loss'], label=name, linewidth=2)\n",
    "ax3.set_title('Training Loss')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "for name in configs.keys():\n",
    "    ax4.plot(reg_results[name]['val_loss'], label=name, linewidth=2)\n",
    "ax4.set_title('Validation Loss')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Loss')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy comparison\n",
    "ax5 = fig.add_subplot(gs[1, 0])\n",
    "names = list(configs.keys())\n",
    "test_accs = [reg_results[name]['test_acc'] for name in names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
    "ax5.bar(names, test_accs, color=colors, alpha=0.8)\n",
    "ax5.set_title('Test Accuracy')\n",
    "ax5.set_ylabel('Accuracy')\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis\n",
    "ax6 = fig.add_subplot(gs[1, 1])\n",
    "overfitting_gaps = [reg_results[name]['overfitting_gap'] for name in names]\n",
    "colors = ['red' if gap > 0.1 else 'green' for gap in overfitting_gaps]\n",
    "ax6.bar(names, overfitting_gaps, color=colors, alpha=0.8)\n",
    "ax6.set_title('Overfitting Gap (Train - Val Acc)')\n",
    "ax6.set_ylabel('Accuracy Gap')\n",
    "ax6.tick_params(axis='x', rotation=45)\n",
    "ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Training efficiency (epochs)\n",
    "ax7 = fig.add_subplot(gs[1, 2])\n",
    "epochs_trained = [reg_results[name]['epochs_trained'] for name in names]\n",
    "ax7.bar(names, epochs_trained, color=plt.cm.plasma(np.linspace(0, 1, len(names))), alpha=0.8)\n",
    "ax7.set_title('Training Epochs')\n",
    "ax7.set_ylabel('Epochs')\n",
    "ax7.tick_params(axis='x', rotation=45)\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curves comparison (detailed)\n",
    "ax8 = fig.add_subplot(gs[1, 3])\n",
    "for name in ['Baseline (No Reg)', 'Full Regularization']:\n",
    "    if name in reg_results:\n",
    "        ax8.plot(reg_results[name]['train_acc'], '--', alpha=0.7, label=f'{name} (train)')\n",
    "        ax8.plot(reg_results[name]['val_acc'], '-', linewidth=2, label=f'{name} (val)')\n",
    "ax8.set_title('Detailed Comparison: Baseline vs Regularized')\n",
    "ax8.set_xlabel('Epoch')\n",
    "ax8.set_ylabel('Accuracy')\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary metrics\n",
    "ax9 = fig.add_subplot(gs[2, :])\n",
    "metrics = ['Test Acc', 'Overfitting Gap', 'Epochs', 'Efficiency Score']\n",
    "\n",
    "# Calculate efficiency score (test_acc / epochs_trained)\n",
    "efficiency_scores = [reg_results[name]['test_acc'] / reg_results[name]['epochs_trained'] \n",
    "                    for name in names]\n",
    "\n",
    "# Normalize metrics for comparison\n",
    "test_accs_norm = [(acc - min(test_accs)) / (max(test_accs) - min(test_accs)) for acc in test_accs]\n",
    "gaps_norm = [1 - (gap + 0.2) / 0.4 for gap in overfitting_gaps]  # Inverted (lower gap = better)\n",
    "epochs_norm = [1 - (ep - min(epochs_trained)) / (max(epochs_trained) - min(epochs_trained)) \n",
    "               for ep in epochs_trained]  # Inverted (fewer epochs = better)\n",
    "efficiency_norm = [(eff - min(efficiency_scores)) / (max(efficiency_scores) - min(efficiency_scores)) \n",
    "                   for eff in efficiency_scores]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.2\n",
    "\n",
    "ax9.bar(x - 1.5*width, test_accs_norm, width, label='Test Accuracy', alpha=0.8)\n",
    "ax9.bar(x - 0.5*width, gaps_norm, width, label='Generalization', alpha=0.8)\n",
    "ax9.bar(x + 0.5*width, epochs_norm, width, label='Training Speed', alpha=0.8)\n",
    "ax9.bar(x + 1.5*width, efficiency_norm, width, label='Efficiency', alpha=0.8)\n",
    "\n",
    "ax9.set_title('Normalized Performance Metrics (Higher = Better)')\n",
    "ax9.set_ylabel('Normalized Score')\n",
    "ax9.set_xticks(x)\n",
    "ax9.set_xticklabels(names, rotation=45)\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Kesimpulan Learning Rate Scheduling & Regularization:\")\n",
    "print(\"✅ Early Stopping: Simple dan efektif untuk mencegah overfitting\")\n",
    "print(\"✅ LR Scheduling: Membantu fine-tuning di akhir training\")\n",
    "print(\"✅ Full Regularization: Kombinasi teknik memberikan hasil terbaik\")\n",
    "print(\"✅ Batch Normalization: Mempercepat training dan regularisasi\")\n",
    "print(\"✅ Dropout: Efektif mencegah overfitting pada dense layers\")\n",
    "print(\"✅ L2 Regularization: Menghasilkan weights yang lebih stabil\")\n",
    "print(\"⚠️  Balance antara regularization dan model capacity penting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1cd88",
   "metadata": {},
   "source": [
    "## 11. Best Practices dan Workflow {#11-best-practices}\n",
    "\n",
    "### 🎯 Workflow Systematic untuk Deep Learning\n",
    "\n",
    "#### Phase 1: Setup & Baseline\n",
    "```python\n",
    "# 1. Data Preparation\n",
    "✓ Normalize/standardize input data\n",
    "✓ Split data properly (train/val/test)\n",
    "✓ Check for data quality issues\n",
    "\n",
    "# 2. Create Baseline Model\n",
    "✓ Start simple (shallow network)\n",
    "✓ Use standard components (Dense layers, ReLU)\n",
    "✓ Establish performance baseline\n",
    "```\n",
    "\n",
    "#### Phase 2: Architecture Design\n",
    "```python\n",
    "# 3. Improve Architecture\n",
    "✓ Add depth gradually\n",
    "✓ Use appropriate activation functions\n",
    "✓ Add batch normalization\n",
    "✓ Consider skip connections for very deep networks\n",
    "```\n",
    "\n",
    "#### Phase 3: Training Optimization\n",
    "```python\n",
    "# 4. Optimize Training\n",
    "✓ Choose appropriate optimizer (Adam as default)\n",
    "✓ Implement early stopping\n",
    "✓ Add learning rate scheduling\n",
    "✓ Monitor training curves\n",
    "```\n",
    "\n",
    "#### Phase 4: Regularization\n",
    "```python\n",
    "# 5. Prevent Overfitting\n",
    "✓ Add dropout layers\n",
    "✓ Use L2 regularization if needed\n",
    "✓ Implement data augmentation (for images)\n",
    "✓ Cross-validate results\n",
    "```\n",
    "\n",
    "### 📋 Checklist untuk Deep Learning Success\n",
    "\n",
    "#### ✅ Data & Preprocessing\n",
    "- [ ] Data quality check (outliers, missing values)\n",
    "- [ ] Proper train/validation/test split\n",
    "- [ ] Feature scaling/normalization\n",
    "- [ ] Data augmentation (if applicable)\n",
    "\n",
    "#### ✅ Model Architecture\n",
    "- [ ] Start with simple baseline\n",
    "- [ ] Use He initialization for ReLU networks\n",
    "- [ ] Add batch normalization after linear layers\n",
    "- [ ] Choose appropriate activation functions\n",
    "- [ ] Add regularization (dropout, L2)\n",
    "\n",
    "#### ✅ Training Configuration\n",
    "- [ ] Adam optimizer sebagai starting point\n",
    "- [ ] Learning rate ∈ [0.0001, 0.01]\n",
    "- [ ] Batch size ∈ [32, 256]\n",
    "- [ ] Early stopping (patience 5-10)\n",
    "- [ ] Learning rate scheduling\n",
    "\n",
    "#### ✅ Monitoring & Debugging\n",
    "- [ ] Plot training/validation curves\n",
    "- [ ] Monitor overfitting gap\n",
    "- [ ] Check gradient magnitudes\n",
    "- [ ] Validate on separate test set\n",
    "\n",
    "### 🛠️ Troubleshooting Guide\n",
    "\n",
    "| Problem | Symptoms | Solutions |\n",
    "|---------|----------|-----------|\n",
    "| **Vanishing Gradients** | Layer awal tidak belajar | • He initialization<br>• Batch normalization<br>• ReLU/Swish activation |\n",
    "| **Exploding Gradients** | Loss menjadi NaN/Inf | • Gradient clipping<br>• Lower learning rate<br>• Better initialization |\n",
    "| **Overfitting** | Val loss > Train loss | • Dropout<br>• Early stopping<br>• L2 regularization<br>• Data augmentation |\n",
    "| **Underfitting** | Poor performance overall | • Increase model capacity<br>• Longer training<br>• Lower regularization |\n",
    "| **Slow Convergence** | Training sangat lambat | • Higher learning rate<br>• Batch normalization<br>• Better optimizer |\n",
    "\n",
    "### 🎯 Domain-Specific Best Practices\n",
    "\n",
    "#### Computer Vision\n",
    "```python\n",
    "# CNN dengan data augmentation\n",
    "model = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    # ... more layers\n",
    "])\n",
    "```\n",
    "\n",
    "#### Natural Language Processing\n",
    "```python\n",
    "# RNN/LSTM dengan gradient clipping\n",
    "optimizer = keras.optimizers.Adam(clipnorm=1.0)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 128),\n",
    "    keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "#### Time Series\n",
    "```python\n",
    "# LSTM untuk sequence prediction\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(50, return_sequences=True),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.LSTM(50),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 📊 Performance Optimization Tips\n",
    "\n",
    "#### Memory Optimization\n",
    "- Mixed precision training (float16)\n",
    "- Gradient accumulation\n",
    "- Model checkpointing\n",
    "- Batch size tuning\n",
    "\n",
    "#### Speed Optimization\n",
    "- tf.data for data loading\n",
    "- Prefetch dan parallel processing\n",
    "- GPU utilization monitoring\n",
    "- Compile dengan XLA\n",
    "\n",
    "### 🏆 Key Takeaways\n",
    "\n",
    "1. **Start Simple**: Begin dengan baseline, tambah complexity secara bertahap\n",
    "2. **Monitor Everything**: Training curves, overfitting, gradient norms\n",
    "3. **Regularize Properly**: Balance antara model capacity dan generalization\n",
    "4. **Be Systematic**: Change one thing at a time, document experiments\n",
    "5. **Validate Properly**: Never use test set untuk model selection\n",
    "\n",
    "### 🎓 Final Recommendations\n",
    "\n",
    "**Untuk Pemula:**\n",
    "- Mulai dengan pre-trained models (transfer learning)\n",
    "- Gunakan standard architectures yang proven\n",
    "- Focus pada data quality dan preprocessing\n",
    "\n",
    "**Untuk Intermediate:**\n",
    "- Experiment dengan different optimizers dan schedulers\n",
    "- Implement custom training loops\n",
    "- Explore advanced regularization techniques\n",
    "\n",
    "**Untuk Advanced:**\n",
    "- Design custom architectures\n",
    "- Implement paper reproductions\n",
    "- Contribute ke open source projects\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Selamat! \n",
    "\n",
    "Anda telah menyelesaikan **Chapter 11: Training Deep Neural Networks**!\n",
    "\n",
    "**Yang telah Anda pelajari:**\n",
    "✅ Masalah fundamental dalam training deep networks  \n",
    "✅ Teknik-teknik untuk mengatasi vanishing/exploding gradients  \n",
    "✅ Weight initialization dan activation functions yang tepat  \n",
    "✅ Batch normalization untuk stabilitas training  \n",
    "✅ Gradient clipping dan transfer learning  \n",
    "✅ Optimizer modern dan learning rate scheduling  \n",
    "✅ Teknik regularisasi untuk mencegah overfitting  \n",
    "✅ Best practices dan workflow yang systematic  \n",
    "\n",
    "**Next Steps:**\n",
    "- Practice dengan dataset real-world\n",
    "- Experiment dengan different architectures\n",
    "- Explore advanced topics seperti attention mechanisms\n",
    "- Read latest papers dan implement cutting-edge techniques\n",
    "\n",
    "**Happy Deep Learning! 🚀**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template Praktis untuk Training Deep Neural Networks\n",
    "print(\"=== Template Praktis Deep Learning ===\")\n",
    "\n",
    "class DeepLearningTemplate:\n",
    "    \"\"\"\n",
    "    Template comprehensive untuk training deep neural networks\n",
    "    dengan built-in best practices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, problem_type='classification'):\n",
    "        self.problem_type = problem_type\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def create_model(self, input_shape, output_dim, architecture='balanced'):\n",
    "        \"\"\"\n",
    "        Buat model dengan best practices built-in\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape input data\n",
    "            output_dim: Dimensi output\n",
    "            architecture: 'simple', 'balanced', atau 'deep'\n",
    "        \"\"\"\n",
    "        \n",
    "        if architecture == 'simple':\n",
    "            self.model = keras.Sequential([\n",
    "                keras.layers.Input(shape=input_shape),\n",
    "                keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dropout(0.3),\n",
    "                keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dropout(0.3),\n",
    "                keras.layers.Dense(output_dim, activation=self._get_output_activation())\n",
    "            ])\n",
    "            \n",
    "        elif architecture == 'balanced':\n",
    "            self.model = keras.Sequential([\n",
    "                keras.layers.Input(shape=input_shape),\n",
    "                keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dropout(0.3),\n",
    "                keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dropout(0.3),\n",
    "                keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.Dropout(0.2),\n",
    "                keras.layers.Dense(output_dim, activation=self._get_output_activation())\n",
    "            ])\n",
    "            \n",
    "        elif architecture == 'deep':\n",
    "            layers = [keras.layers.Input(shape=input_shape)]\n",
    "            for i, units in enumerate([256, 128, 128, 64, 64, 32]):\n",
    "                layers.extend([\n",
    "                    keras.layers.Dense(units, activation='relu', \n",
    "                                     kernel_initializer='he_normal',\n",
    "                                     kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.Dropout(0.3 if i < 3 else 0.2)\n",
    "                ])\n",
    "            layers.append(keras.layers.Dense(output_dim, activation=self._get_output_activation()))\n",
    "            self.model = keras.Sequential(layers)\n",
    "            \n",
    "        return self.model\n",
    "    \n",
    "    def _get_output_activation(self):\n",
    "        \"\"\"Get appropriate output activation\"\"\"\n",
    "        if self.problem_type == 'binary_classification':\n",
    "            return 'sigmoid'\n",
    "        elif self.problem_type == 'multiclass_classification':\n",
    "            return 'softmax'\n",
    "        else:  # regression\n",
    "            return None\n",
    "    \n",
    "    def compile_and_train(self, X_train, y_train, X_val, y_val, \n",
    "                         optimizer='adam', learning_rate=0.001,\n",
    "                         epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compile dan train model dengan best practices\n",
    "        \"\"\"\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if optimizer == 'adam':\n",
    "            opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        elif optimizer == 'adamw':\n",
    "            opt = keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=0.01)\n",
    "        else:\n",
    "            opt = optimizer\n",
    "            \n",
    "        # Setup loss dan metrics\n",
    "        if self.problem_type == 'binary_classification':\n",
    "            loss = 'binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        elif self.problem_type == 'multiclass_classification':\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        else:  # regression\n",
    "            loss = 'mse'\n",
    "            metrics = ['mae']\n",
    "            \n",
    "        # Compile model\n",
    "        self.model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.h5', monitor='val_loss', save_best_only=True, verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(\"🚀 Starting Training...\")\n",
    "        print(f\"Model parameters: {self.model.count_params():,}\")\n",
    "        print(f\"Training samples: {len(X_train):,}\")\n",
    "        print(f\"Validation samples: {len(X_val):,}\")\n",
    "        \n",
    "        # Training\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate_and_plot(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model dan plot training history\n",
    "        \"\"\"\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_metric = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        metric_name = 'accuracy' if 'classification' in self.problem_type else 'mae'\n",
    "        \n",
    "        print(f\"\\n📊 Final Results:\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test {metric_name}: {test_metric:.4f}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        if self.history:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Loss plot\n",
    "            axes[0].plot(self.history.history['loss'], label='Training Loss', linewidth=2)\n",
    "            axes[0].plot(self.history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "            axes[0].set_title('Model Loss')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Metric plot\n",
    "            metric_key = 'accuracy' if 'classification' in self.problem_type else 'mae'\n",
    "            axes[1].plot(self.history.history[metric_key], label=f'Training {metric_name}', linewidth=2)\n",
    "            axes[1].plot(self.history.history[f'val_{metric_key}'], label=f'Validation {metric_name}', linewidth=2)\n",
    "            axes[1].set_title(f'Model {metric_name}')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel(metric_name)\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Training analysis\n",
    "            final_train_metric = self.history.history[metric_key][-1]\n",
    "            final_val_metric = self.history.history[f'val_{metric_key}'][-1]\n",
    "            \n",
    "            if 'accuracy' in metric_key:\n",
    "                overfitting_gap = final_train_metric - final_val_metric\n",
    "            else:  # mae\n",
    "                overfitting_gap = final_val_metric - final_train_metric\n",
    "                \n",
    "            print(f\"\\n📈 Training Analysis:\")\n",
    "            print(f\"Final overfitting gap: {overfitting_gap:.4f}\")\n",
    "            print(f\"Training epochs: {len(self.history.history['loss'])}\")\n",
    "            \n",
    "            if abs(overfitting_gap) < 0.05:\n",
    "                print(\"✅ Model shows good generalization\")\n",
    "            elif overfitting_gap > 0.05:\n",
    "                print(\"⚠️  Model might be overfitting\")\n",
    "            else:\n",
    "                print(\"⚠️  Model might be underfitting\")\n",
    "        \n",
    "        return test_loss, test_metric\n",
    "\n",
    "# Demonstrasi penggunaan template\n",
    "print(\"\\n=== Demo Template Usage ===\")\n",
    "\n",
    "# Example 1: Regression\n",
    "print(\"\\n1. Regression Example:\")\n",
    "regression_template = DeepLearningTemplate(problem_type='regression')\n",
    "\n",
    "# Create sample data for demonstration if variables don't exist\n",
    "try:\n",
    "    regression_model = regression_template.create_model(\n",
    "        input_shape=[X_train_scaled.shape[1]], \n",
    "        output_dim=1, \n",
    "        architecture='balanced'\n",
    "    )\n",
    "    print(f\"Regression model created with {regression_model.count_params():,} parameters\")\n",
    "    \n",
    "    # Quick training demo\n",
    "    print(\"\\n2. Quick Training Demo (Subset):\")\n",
    "    history = regression_template.compile_and_train(\n",
    "        X_train_scaled[:1000], y_train[:1000],\n",
    "        X_val_scaled[:200], y_val[:200],\n",
    "        epochs=15,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_mae = regression_template.evaluate_and_plot(\n",
    "        X_test_scaled[:200], y_test[:200]\n",
    "    )\n",
    "    \n",
    "except NameError:\n",
    "    print(\"⚠️  Training data variables not found. Please run data preparation cells first.\")\n",
    "    print(\"Creating model structure demonstration:\")\n",
    "    \n",
    "    # Demo with synthetic data\n",
    "    import numpy as np\n",
    "    demo_model = regression_template.create_model(\n",
    "        input_shape=[8], \n",
    "        output_dim=1, \n",
    "        architecture='balanced'\n",
    "    )\n",
    "    demo_model.summary()\n",
    "\n",
    "print(\"\\n🎯 Template Usage Summary:\")\n",
    "print(\"1. Instantiate template dengan problem_type\")\n",
    "print(\"2. Create model dengan architecture preference\")\n",
    "print(\"3. Compile dan train dengan built-in best practices\")\n",
    "print(\"4. Evaluate dan analyze hasil training\")\n",
    "\n",
    "print(\"\\n✅ Template Features:\")\n",
    "print(\"• Automatic best practices (BatchNorm, Dropout, Early Stopping)\")\n",
    "print(\"• Proper weight initialization (He Normal)\")\n",
    "print(\"• Learning rate scheduling (ReduceLROnPlateau)\")\n",
    "print(\"• Comprehensive monitoring dan analysis\")\n",
    "print(\"• Easy to customize dan extend\")\n",
    "\n",
    "print(\"\\n🎓 Chapter 11 COMPLETED!\")\n",
    "print(\"Anda sekarang memiliki foundation yang solid untuk training deep networks yang efektif!\")\n",
    "print(\"🚀 Ready untuk tackle real-world deep learning problems!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
