{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6987b835",
   "metadata": {},
   "source": [
    "# ğŸ”„ Chapter 15: Processing Sequences Using RNNs and CNNs\n",
    "# Memproses Urutan Data Menggunakan RNN dan CNN\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Tujuan Pembelajaran\n",
    "\n",
    "Setelah menyelesaikan chapter ini, Anda akan mampu:\n",
    "- âœ… Memahami konsep RNN dan sequence processing\n",
    "- âœ… Mengimplementasikan Simple RNN, LSTM, dan GRU\n",
    "- âœ… Menangani berbagai jenis sequence problems\n",
    "- âœ… Menggunakan CNN untuk sequence data\n",
    "- âœ… Menerapkan teknik untuk time series forecasting\n",
    "- âœ… Membangun text classification dan sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Outline Chapter\n",
    "\n",
    "1. **Pengantar Sequence Processing** ğŸ§ \n",
    "2. **Simple RNN - Basics** ğŸ”„\n",
    "3. **LSTM - Long Short-Term Memory** ğŸ§ ğŸ’¾\n",
    "4. **GRU - Gated Recurrent Unit** ğŸšª\n",
    "5. **CNN untuk Sequences** ğŸ”\n",
    "6. **Time Series Forecasting** ğŸ“ˆ\n",
    "7. **Text Processing dengan RNN** ğŸ“\n",
    "8. **Advanced Techniques** ğŸš€\n",
    "9. **Best Practices & Tips** ğŸ’¡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Pengantar: Dunia Data Sekuensial\n",
    "\n",
    "### ğŸ”„ **Mengapa Sequence Processing?**\n",
    "\n",
    "**Data sekuensial** ada di mana-mana dalam kehidupan nyata:\n",
    "- ğŸ“ˆ **Time Series** - Stock prices, weather data, sensor readings\n",
    "- ğŸ“ **Text** - Natural language, machine translation\n",
    "- ğŸµ **Audio** - Speech recognition, music generation\n",
    "- ğŸ¬ **Video** - Action recognition, video analysis\n",
    "- ğŸ§¬ **Biological** - DNA sequences, protein folding\n",
    "\n",
    "### ğŸ§  **Challenge Traditional Neural Networks**\n",
    "\n",
    "**Feedforward networks** memiliki keterbatasan:\n",
    "- âŒ **Fixed Input Size** - Tidak bisa handle variable length\n",
    "- âŒ **No Memory** - Tidak mengingat context sebelumnya\n",
    "- âŒ **Position Blind** - Tidak peduli urutan data\n",
    "- âŒ **Independence Assumption** - Setiap input dianggap independent\n",
    "\n",
    "### ğŸš€ **RNN Solution**\n",
    "\n",
    "**Recurrent Neural Networks (RNN)** mengatasi masalah ini dengan:\n",
    "- âœ… **Memory Mechanism** - Hidden state menyimpan informasi masa lalu\n",
    "- âœ… **Variable Length** - Handle sequences dengan panjang berbeda\n",
    "- âœ… **Parameter Sharing** - Same weights across time steps\n",
    "- âœ… **Temporal Dependencies** - Capture patterns over time\n",
    "\n",
    "### ğŸ”§ **Konsep Matematika RNN**\n",
    "\n",
    "**RNN Core Equation:**\n",
    "```\n",
    "h(t) = tanh(W_hh * h(t-1) + W_xh * x(t) + b_h)\n",
    "y(t) = W_hy * h(t) + b_y\n",
    "```\n",
    "\n",
    "**Dimana:**\n",
    "- `h(t)` = Hidden state pada waktu t\n",
    "- `x(t)` = Input pada waktu t  \n",
    "- `y(t)` = Output pada waktu t\n",
    "- `W_hh`, `W_xh`, `W_hy` = Weight matrices\n",
    "- `b_h`, `b_y` = Bias vectors\n",
    "\n",
    "### ğŸ¯ **Types of Sequence Problems**\n",
    "\n",
    "1. **One-to-One** ğŸ¯ - Traditional neural network\n",
    "2. **One-to-Many** ğŸ¯â¡ï¸ğŸ“Š - Image captioning, music generation\n",
    "3. **Many-to-One** ğŸ“Šâ¡ï¸ğŸ¯ - Sentiment analysis, sequence classification\n",
    "4. **Many-to-Many** ğŸ“Šâ¡ï¸ğŸ“Š - Machine translation, video analysis\n",
    "5. **Many-to-Many (synced)** ğŸ“Šâ¬ŒğŸ“Š - Video frame labeling, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a055078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”„ CHAPTER 15: Processing Sequences Using RNNs and CNNs\n",
      "======================================================================\n",
      "ğŸ“¦ TensorFlow version: 2.19.0\n",
      "ğŸ“¦ Keras version: 3.10.0\n",
      "ğŸ“¦ NumPy version: 2.1.3\n",
      "ğŸ“¦ Pandas version: 2.3.0\n",
      "ğŸ’» Running on CPU\n",
      "\n",
      "âœ… Setup complete! Ready to explore Sequence Processing with RNNs\n",
      "======================================================================\n",
      "ğŸ“¦ TensorFlow version: 2.19.0\n",
      "ğŸ“¦ Keras version: 3.10.0\n",
      "ğŸ“¦ NumPy version: 2.1.3\n",
      "ğŸ“¦ Pandas version: 2.3.0\n",
      "ğŸ’» Running on CPU\n",
      "\n",
      "âœ… Setup complete! Ready to explore Sequence Processing with RNNs\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Setup & Import Libraries\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”„ CHAPTER 15: Processing Sequences Using RNNs and CNNs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Display versions\n",
    "print(f\"ğŸ“¦ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ“¦ Keras version: {keras.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")\n",
    "print(f\"ğŸ“¦ Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ğŸ® GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "    print(\"ğŸš€ CUDA enabled - Ready for accelerated RNN training!\")\n",
    "else:\n",
    "    print(\"ğŸ’» Running on CPU\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nâœ… Setup complete! Ready to explore Sequence Processing with RNNs\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2d2b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”„ 1. Simple RNN - Building Blocks\n",
    "\n",
    "## ğŸ§  Anatomy of Simple RNN\n",
    "\n",
    "**Simple RNN** adalah bentuk paling dasar dari recurrent neural networks:\n",
    "\n",
    "### ğŸ”§ **Core Components**:\n",
    "- **Hidden State** ğŸ’¾ - Memory yang menyimpan informasi dari timestep sebelumnya\n",
    "- **Input Gate** ğŸ“¥ - Menerima input pada setiap timestep\n",
    "- **Output Gate** ğŸ“¤ - Menghasilkan output (optional)\n",
    "- **Recurrent Connection** ğŸ”„ - Menghubungkan hidden state antar waktu\n",
    "\n",
    "### âš¡ **Information Flow**:\n",
    "```\n",
    "x(0) â†’ [RNN] â†’ h(0) â†’ y(0)\n",
    "         â†“       â†“\n",
    "x(1) â†’ [RNN] â†’ h(1) â†’ y(1)\n",
    "         â†“       â†“\n",
    "x(2) â†’ [RNN] â†’ h(2) â†’ y(2)\n",
    "         â†“       â†“\n",
    "       ...     ...\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Key Characteristics**:\n",
    "- **Short-term Memory** - Informasi lama cenderung hilang\n",
    "- **Vanishing Gradient** - Gradients become very small over long sequences\n",
    "- **Simple Architecture** - Easy to understand dan implement\n",
    "- **Fast Training** - Fewer parameters dibanding LSTM/GRU\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Problems with Simple RNN\n",
    "\n",
    "### âŒ **Vanishing Gradient Problem**:\n",
    "- **Gradients shrink** exponentially melalui backpropagation through time\n",
    "- **Long dependencies** sulit dipelajari\n",
    "- **Early information** tends to be forgotten\n",
    "\n",
    "### âŒ **Limited Memory**:\n",
    "- **Short context window** - Only recent information retained\n",
    "- **Sequential bias** - Recent inputs have more influence\n",
    "- **Information bottleneck** - Single hidden state must encode everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ 1.1 Simple RNN - Practical Implementation\n",
    "print(\"ğŸ”„ SIMPLE RNN DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate synthetic time series data (sine wave)\n",
    "def generate_sine_wave(length=1000, frequency=0.1, noise=0.1):\n",
    "    \"\"\"Generate synthetic sine wave data for demonstration\"\"\"\n",
    "    t = np.linspace(0, length * frequency, length)\n",
    "    data = np.sin(2 * np.pi * t) + noise * np.random.randn(length)\n",
    "    return data\n",
    "\n",
    "# Create dataset\n",
    "print(\"ğŸ“Š Generating synthetic time series data...\")\n",
    "time_series = generate_sine_wave(length=1000, frequency=0.1, noise=0.05)\n",
    "print(f\"âœ… Generated time series with {len(time_series)} points\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(time_series[:200], label='Sine Wave', linewidth=2)\n",
    "plt.title('ğŸ”„ Synthetic Time Series Data (First 200 points)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ”§ PREPARING SEQUENCE DATA\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "def create_sequences(data, seq_length, prediction_steps=1):\n",
    "    \"\"\"Create sequences for RNN training\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - prediction_steps + 1):\n",
    "        # Input sequence\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        # Target (next value(s))\n",
    "        if prediction_steps == 1:\n",
    "            y.append(data[i + seq_length])\n",
    "        else:\n",
    "            y.append(data[i + seq_length:i + seq_length + prediction_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 20  # Look back 20 steps\n",
    "X, y = create_sequences(time_series, sequence_length)\n",
    "\n",
    "print(f\"âœ… Sequence preparation:\")\n",
    "print(f\"   Sequence length: {sequence_length}\")\n",
    "print(f\"   Input shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Number of sequences: {len(X)}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"   Training sequences: {len(X_train)}\")\n",
    "print(f\"   Test sequences: {len(X_test)}\")\n",
    "\n",
    "# Reshape for RNN (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"   Reshaped for RNN: {X_train.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING SIMPLE RNN MODEL\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Build Simple RNN model\n",
    "simple_rnn_model = models.Sequential([\n",
    "    layers.SimpleRNN(50, \n",
    "                     activation='tanh',\n",
    "                     return_sequences=False,  # Only return last output\n",
    "                     input_shape=(sequence_length, 1),\n",
    "                     name='simple_rnn'),\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… Simple RNN Architecture:\")\n",
    "simple_rnn_model.summary()\n",
    "\n",
    "# Compile model\n",
    "simple_rnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING SIMPLE RNN\")\n",
    "print(\"-\" * 23)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Simple RNN...\")\n",
    "rnn_history = simple_rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š SIMPLE RNN RESULTS\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Evaluate model\n",
    "rnn_loss, rnn_mae = simple_rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"ğŸ¯ Test Results:\")\n",
    "print(f\"   Loss (MSE): {rnn_loss:.6f}\")\n",
    "print(f\"   MAE: {rnn_mae:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "print(f\"\\nğŸ”® MAKING PREDICTIONS\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "y_pred = simple_rnn_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate additional metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"ğŸ“Š Prediction Metrics:\")\n",
    "print(f\"   RMSE: {rmse:.6f}\")\n",
    "print(f\"   MAE: {mae:.6f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Training history\n",
    "ax1.plot(rnn_history.history['loss'], label='Training Loss', marker='o')\n",
    "ax1.plot(rnn_history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax1.set_title('ğŸš€ Simple RNN Training Progress', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual\n",
    "test_range = range(len(y_test))\n",
    "ax2.plot(test_range[:100], y_test[:100], label='Actual', linewidth=2, alpha=0.8)\n",
    "ax2.plot(test_range[:100], y_pred[:100], label='Predicted', linewidth=2, alpha=0.8)\n",
    "ax2.set_title('ğŸ”® Simple RNN Predictions vs Actual (First 100 test points)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Test Sample')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ SIMPLE RNN INSIGHTS:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"ğŸ”¸ Simple RNN can learn basic temporal patterns\")\n",
    "print(\"ğŸ”¸ Performance depends heavily on sequence length\")\n",
    "print(\"ğŸ”¸ May struggle with long-term dependencies\")\n",
    "print(\"ğŸ”¸ Good baseline for understanding RNN concepts\")\n",
    "print(\"ğŸ”¸ Vanishing gradient limits learning capability\")\n",
    "\n",
    "print(f\"\\nâœ… Simple RNN demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be750e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ§ ğŸ’¾ 2. LSTM - Long Short-Term Memory\n",
    "\n",
    "## ğŸ¯ Mengapa LSTM?\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** dikembangkan untuk mengatasi masalah **vanishing gradient** pada Simple RNN.\n",
    "\n",
    "### âœ… **Keunggulan LSTM**:\n",
    "- **ğŸ”’ Long-term Memory** - Dapat mengingat informasi jangka panjang\n",
    "- **ğŸšª Gated Architecture** - Kontrol informasi yang masuk, keluar, dan dilupakan\n",
    "- **ğŸ“ˆ Better Gradients** - Gradients tidak hilang secepat Simple RNN\n",
    "- **ğŸ¯ Selective Memory** - Decide what to remember dan forget\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Arsitektur LSTM\n",
    "\n",
    "### ğŸšª **The Three Gates**:\n",
    "\n",
    "#### 1. **Forget Gate** ğŸ—‘ï¸\n",
    "```\n",
    "f(t) = Ïƒ(W_f * [h(t-1), x(t)] + b_f)\n",
    "```\n",
    "- **Decide** what information to discard dari cell state\n",
    "- **Output** between 0 (completely forget) dan 1 (completely keep)\n",
    "\n",
    "#### 2. **Input Gate** ğŸ“¥\n",
    "```\n",
    "i(t) = Ïƒ(W_i * [h(t-1), x(t)] + b_i)\n",
    "CÌƒ(t) = tanh(W_C * [h(t-1), x(t)] + b_C)\n",
    "```\n",
    "- **Input gate** decides what new information to store\n",
    "- **Candidate values** create new information that could be added\n",
    "\n",
    "#### 3. **Output Gate** ğŸ“¤\n",
    "```\n",
    "o(t) = Ïƒ(W_o * [h(t-1), x(t)] + b_o)\n",
    "```\n",
    "- **Decide** what parts of cell state to output\n",
    "- **Control** what information goes to hidden state\n",
    "\n",
    "### ğŸ”„ **Complete LSTM Flow**:\n",
    "```\n",
    "1. Forget: C(t) = f(t) * C(t-1)\n",
    "2. Input: C(t) = C(t) + i(t) * CÌƒ(t)  \n",
    "3. Output: h(t) = o(t) * tanh(C(t))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **LSTM vs Simple RNN**\n",
    "\n",
    "| Aspect | Simple RNN | LSTM |\n",
    "|--------|------------|------|\n",
    "| **Memory** | Short-term | Long & Short-term |\n",
    "| **Parameters** | Fewer | More (4x) |\n",
    "| **Training** | Faster | Slower |\n",
    "| **Gradient Flow** | Vanishing | Better preserved |\n",
    "| **Use Cases** | Simple patterns | Complex sequences |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ **LSTM Variants**\n",
    "\n",
    "### ğŸ“Š **Return Sequences**:\n",
    "- **`return_sequences=False`** - Only last output (many-to-one)\n",
    "- **`return_sequences=True`** - All outputs (many-to-many)\n",
    "\n",
    "### ğŸ”„ **Bidirectional LSTM**:\n",
    "- **Forward + Backward** processing\n",
    "- **Richer representations** by seeing future context\n",
    "- **2x parameters** but better performance\n",
    "\n",
    "### ğŸ“š **Stacked LSTM**:\n",
    "- **Multiple LSTM layers** stacked vertically\n",
    "- **Hierarchical feature learning**\n",
    "- **Deeper representations** for complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16015506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ ğŸ’¾ 2.1 LSTM - Practical Implementation\n",
    "print(\"ğŸ§ ğŸ’¾ LSTM DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ—ï¸ BUILDING LSTM MODEL\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = models.Sequential([\n",
    "    layers.LSTM(50, \n",
    "                return_sequences=False,  # Many-to-one\n",
    "                input_shape=(sequence_length, 1),\n",
    "                name='lstm'),\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.2, name='dropout'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… LSTM Architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING LSTM MODEL\")\n",
    "print(\"-\" * 24)\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"Training LSTM...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š LSTM RESULTS\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_loss, lstm_mae = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"ğŸ¯ LSTM Test Results:\")\n",
    "print(f\"   Loss (MSE): {lstm_loss:.6f}\")\n",
    "print(f\"   MAE: {lstm_mae:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "lstm_pred = lstm_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))\n",
    "lstm_mae_calc = mean_absolute_error(y_test, lstm_pred)\n",
    "\n",
    "print(f\"ğŸ“Š LSTM Prediction Metrics:\")\n",
    "print(f\"   RMSE: {lstm_rmse:.6f}\")\n",
    "print(f\"   MAE: {lstm_mae_calc:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ” BIDIRECTIONAL LSTM\")\n",
    "print(\"-\" * 24)\n",
    "\n",
    "# Build Bidirectional LSTM\n",
    "bidirectional_model = models.Sequential([\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(25, return_sequences=False),  # 25*2 = 50 total units\n",
    "        input_shape=(sequence_length, 1),\n",
    "        name='bidirectional_lstm'\n",
    "    ),\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.2, name='dropout'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… Bidirectional LSTM Architecture:\")\n",
    "bidirectional_model.summary()\n",
    "\n",
    "# Compile and train\n",
    "bidirectional_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Training Bidirectional LSTM...\")\n",
    "bi_history = bidirectional_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=15,  # Fewer epochs due to complexity\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Bidirectional LSTM\n",
    "bi_loss, bi_mae = bidirectional_model.evaluate(X_test, y_test, verbose=0)\n",
    "bi_pred = bidirectional_model.predict(X_test, verbose=0)\n",
    "bi_rmse = np.sqrt(mean_squared_error(y_test, bi_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š COMPREHENSIVE COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Compare all models\n",
    "comparison_data = {\n",
    "    'Model': ['Simple RNN', 'LSTM', 'Bidirectional LSTM'],\n",
    "    'MSE Loss': [rnn_loss, lstm_loss, bi_loss],\n",
    "    'RMSE': [rmse, lstm_rmse, bi_rmse],\n",
    "    'MAE': [mae, lstm_mae_calc, bi_mae]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.round(6))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison_df['RMSE'].idxmin()\n",
    "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nğŸ† Best Model: {best_model}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ VISUALIZATION COMPARISON\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Training histories\n",
    "axes[0, 0].plot(rnn_history.history['val_loss'], label='Simple RNN', marker='o')\n",
    "axes[0, 0].plot(lstm_history.history['val_loss'], label='LSTM', marker='s')\n",
    "axes[0, 0].plot(bi_history.history['val_loss'], label='Bidirectional LSTM', marker='^')\n",
    "axes[0, 0].set_title('ğŸš€ Training Progress - Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions comparison (first 50 points)\n",
    "test_range = range(50)\n",
    "axes[0, 1].plot(test_range, y_test[:50], label='Actual', linewidth=3, alpha=0.8)\n",
    "axes[0, 1].plot(test_range, y_pred[:50], label='Simple RNN', linewidth=2, alpha=0.7)\n",
    "axes[0, 1].plot(test_range, lstm_pred[:50], label='LSTM', linewidth=2, alpha=0.7)\n",
    "axes[0, 1].plot(test_range, bi_pred[:50], label='Bidirectional LSTM', linewidth=2, alpha=0.7)\n",
    "axes[0, 1].set_title('ğŸ”® Predictions Comparison (First 50 points)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Test Sample')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error analysis\n",
    "models_names = ['Simple RNN', 'LSTM', 'Bidirectional LSTM']\n",
    "rmse_values = [rmse, lstm_rmse, bi_rmse]\n",
    "axes[1, 0].bar(models_names, rmse_values, color=['skyblue', 'lightgreen', 'orange'])\n",
    "axes[1, 0].set_title('ğŸ“Š RMSE Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('RMSE')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Model complexity (parameter count)\n",
    "param_counts = [\n",
    "    simple_rnn_model.count_params(),\n",
    "    lstm_model.count_params(), \n",
    "    bidirectional_model.count_params()\n",
    "]\n",
    "axes[1, 1].bar(models_names, param_counts, color=['skyblue', 'lightgreen', 'orange'])\n",
    "axes[1, 1].set_title('ğŸ”§ Model Complexity (Parameters)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Parameters')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ LSTM INSIGHTS:\")\n",
    "print(\"-\" * 18)\n",
    "print(\"ğŸ”¸ LSTM typically outperforms Simple RNN on complex sequences\")\n",
    "print(\"ğŸ”¸ Bidirectional LSTM can provide better context understanding\")\n",
    "print(\"ğŸ”¸ More parameters mean longer training time but better performance\")\n",
    "print(\"ğŸ”¸ Gated architecture helps with long-term dependency learning\")\n",
    "print(\"ğŸ”¸ LSTM is more stable for gradient flow during training\")\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 25)\n",
    "for i, model_name in enumerate(models_names):\n",
    "    print(f\"   {model_name:18}: RMSE={rmse_values[i]:.6f}, Params={param_counts[i]:,}\")\n",
    "\n",
    "print(f\"\\nâœ… LSTM demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c84a79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸšª 3. GRU - Gated Recurrent Unit\n",
    "\n",
    "## ğŸ¯ Mengapa GRU?\n",
    "\n",
    "**Gated Recurrent Unit (GRU)** adalah versi **simplified** dari LSTM yang dikembangkan untuk:\n",
    "- âœ… **Fewer Parameters** - Lebih sederhana dari LSTM\n",
    "- âœ… **Faster Training** - Karena arsitektur yang lebih streamlined  \n",
    "- âœ… **Good Performance** - Mendekati performa LSTM dengan kompleksitas lebih rendah\n",
    "- âœ… **Less Overfitting** - Fewer parameters berarti less prone to overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Arsitektur GRU\n",
    "\n",
    "### ğŸšª **The Two Gates** (vs 3 gates di LSTM):\n",
    "\n",
    "#### 1. **Reset Gate** ğŸ”„\n",
    "```\n",
    "r(t) = Ïƒ(W_r * [h(t-1), x(t)] + b_r)\n",
    "```\n",
    "- **Controls** how much past information to forget\n",
    "- **Similar** to forget gate di LSTM\n",
    "- **Range**: 0 (forget all) to 1 (keep all)\n",
    "\n",
    "#### 2. **Update Gate** ğŸ“Š\n",
    "```\n",
    "z(t) = Ïƒ(W_z * [h(t-1), x(t)] + b_z)\n",
    "```\n",
    "- **Controls** how much new information to add\n",
    "- **Combines** input dan forget gates dari LSTM\n",
    "- **Range**: 0 (ignore new) to 1 (completely update)\n",
    "\n",
    "### ğŸ”„ **Complete GRU Flow**:\n",
    "```\n",
    "1. Reset: hÌƒ(t) = tanh(W_h * [r(t) * h(t-1), x(t)] + b_h)\n",
    "2. Update: h(t) = (1 - z(t)) * h(t-1) + z(t) * hÌƒ(t)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†š **LSTM vs GRU Comparison**\n",
    "\n",
    "| Aspect | LSTM | GRU |\n",
    "|--------|------|-----|\n",
    "| **Gates** | 3 (forget, input, output) | 2 (reset, update) |\n",
    "| **Parameters** | More | Fewer (~25% less) |\n",
    "| **Training Speed** | Slower | Faster |\n",
    "| **Memory Usage** | Higher | Lower |\n",
    "| **Performance** | Slightly better on complex tasks | Good on most tasks |\n",
    "| **Interpretability** | More complex | Simpler |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **When to Use GRU?**\n",
    "\n",
    "### âœ… **GRU is Better For**:\n",
    "- **Limited Data** - Less prone to overfitting\n",
    "- **Resource Constraints** - Faster training, less memory\n",
    "- **Simple to Medium Complexity** - Good performance with less overhead\n",
    "- **Real-time Applications** - Faster inference\n",
    "\n",
    "### âœ… **LSTM is Better For**:\n",
    "- **Complex Patterns** - More sophisticated gating mechanism\n",
    "- **Large Datasets** - Can leverage additional parameters\n",
    "- **Critical Applications** - Slight performance edge\n",
    "- **Long Sequences** - Better long-term memory retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ba506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸšª 3.1 GRU - Practical Implementation\n",
    "print(\"ğŸšª GRU DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ—ï¸ BUILDING GRU MODEL\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Build GRU model\n",
    "gru_model = models.Sequential([\n",
    "    layers.GRU(50, \n",
    "               return_sequences=False,  # Many-to-one\n",
    "               input_shape=(sequence_length, 1),\n",
    "               name='gru'),\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.2, name='dropout'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… GRU Architecture:\")\n",
    "gru_model.summary()\n",
    "\n",
    "# Compile model\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING GRU MODEL\")\n",
    "print(\"-\" * 21)\n",
    "\n",
    "# Train GRU model\n",
    "print(\"Training GRU...\")\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š GRU RESULTS\")\n",
    "print(\"-\" * 14)\n",
    "\n",
    "# Evaluate GRU\n",
    "gru_loss, gru_mae = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"ğŸ¯ GRU Test Results:\")\n",
    "print(f\"   Loss (MSE): {gru_loss:.6f}\")\n",
    "print(f\"   MAE: {gru_mae:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "gru_pred = gru_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "gru_rmse = np.sqrt(mean_squared_error(y_test, gru_pred))\n",
    "gru_mae_calc = mean_absolute_error(y_test, gru_pred)\n",
    "\n",
    "print(f\"ğŸ“Š GRU Prediction Metrics:\")\n",
    "print(f\"   RMSE: {gru_rmse:.6f}\")\n",
    "print(f\"   MAE: {gru_mae_calc:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ” STACKED GRU\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "# Build Stacked GRU (2 layers)\n",
    "stacked_gru_model = models.Sequential([\n",
    "    layers.GRU(50,\n",
    "               return_sequences=True,  # Return sequences for next GRU layer\n",
    "               input_shape=(sequence_length, 1),\n",
    "               name='gru_layer1'),\n",
    "    layers.Dropout(0.2, name='dropout1'),\n",
    "    layers.GRU(25,\n",
    "               return_sequences=False,  # Final layer returns only last output\n",
    "               name='gru_layer2'),\n",
    "    layers.Dropout(0.2, name='dropout2'),\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… Stacked GRU Architecture:\")\n",
    "stacked_gru_model.summary()\n",
    "\n",
    "# Compile and train\n",
    "stacked_gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Training Stacked GRU...\")\n",
    "stacked_history = stacked_gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=15,  # Fewer epochs due to complexity\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Stacked GRU\n",
    "stacked_loss, stacked_mae = stacked_gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "stacked_pred = stacked_gru_model.predict(X_test, verbose=0)\n",
    "stacked_rmse = np.sqrt(mean_squared_error(y_test, stacked_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š COMPREHENSIVE RNN COMPARISON\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Compare all RNN models\n",
    "rnn_comparison_data = {\n",
    "    'Model': ['Simple RNN', 'LSTM', 'Bidirectional LSTM', 'GRU', 'Stacked GRU'],\n",
    "    'MSE Loss': [rnn_loss, lstm_loss, bi_loss, gru_loss, stacked_loss],\n",
    "    'RMSE': [rmse, lstm_rmse, bi_rmse, gru_rmse, stacked_rmse],\n",
    "    'MAE': [mae, lstm_mae_calc, bi_mae, gru_mae_calc, stacked_mae],\n",
    "    'Parameters': [\n",
    "        simple_rnn_model.count_params(),\n",
    "        lstm_model.count_params(),\n",
    "        bidirectional_model.count_params(),\n",
    "        gru_model.count_params(),\n",
    "        stacked_gru_model.count_params()\n",
    "    ]\n",
    "}\n",
    "\n",
    "rnn_comparison_df = pd.DataFrame(rnn_comparison_data)\n",
    "print(rnn_comparison_df.round(6))\n",
    "\n",
    "# Find best model\n",
    "best_rnn_idx = rnn_comparison_df['RMSE'].idxmin()\n",
    "best_rnn = rnn_comparison_df.loc[best_rnn_idx, 'Model']\n",
    "print(f\"\\nğŸ† Best RNN Model: {best_rnn}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ TRAINING TIME COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Training time analysis (epochs comparison)\n",
    "print(\"Training Epochs Used:\")\n",
    "print(f\"   Simple RNN: 20 epochs\")\n",
    "print(f\"   LSTM: 20 epochs\")  \n",
    "print(f\"   Bidirectional LSTM: 15 epochs\")\n",
    "print(f\"   GRU: 20 epochs\")\n",
    "print(f\"   Stacked GRU: 15 epochs\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ADVANCED VISUALIZATION\")\n",
    "print(\"-\" * 27)\n",
    "\n",
    "# Create comprehensive RNN comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Training progress comparison\n",
    "axes[0, 0].plot(rnn_history.history['val_loss'][:15], label='Simple RNN', marker='o', alpha=0.8)\n",
    "axes[0, 0].plot(lstm_history.history['val_loss'][:15], label='LSTM', marker='s', alpha=0.8)\n",
    "axes[0, 0].plot(bi_history.history['val_loss'], label='Bidirectional LSTM', marker='^', alpha=0.8)\n",
    "axes[0, 0].plot(gru_history.history['val_loss'][:15], label='GRU', marker='d', alpha=0.8)\n",
    "axes[0, 0].plot(stacked_history.history['val_loss'], label='Stacked GRU', marker='*', alpha=0.8)\n",
    "axes[0, 0].set_title('ğŸš€ Training Progress Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE comparison\n",
    "model_names = rnn_comparison_df['Model']\n",
    "rmse_values = rnn_comparison_df['RMSE']\n",
    "bars = axes[0, 1].bar(range(len(model_names)), rmse_values, \n",
    "                      color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "axes[0, 1].set_title('ğŸ“Š RMSE Performance Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(range(len(model_names)))\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, rmse_values):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Parameter count comparison\n",
    "param_counts = rnn_comparison_df['Parameters']\n",
    "bars2 = axes[0, 2].bar(range(len(model_names)), param_counts,\n",
    "                       color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "axes[0, 2].set_title('ğŸ”§ Model Complexity (Parameters)', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Number of Parameters')\n",
    "axes[0, 2].set_xticks(range(len(model_names)))\n",
    "axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars2, param_counts):\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,\n",
    "                    f'{value:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 4. Predictions comparison (detailed view)\n",
    "test_range = range(30)  # First 30 points for clarity\n",
    "axes[1, 0].plot(test_range, y_test[:30], label='Actual', linewidth=3, alpha=0.9)\n",
    "axes[1, 0].plot(test_range, y_pred[:30], label='Simple RNN', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].plot(test_range, lstm_pred[:30], label='LSTM', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].plot(test_range, gru_pred[:30], label='GRU', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].set_title('ğŸ”® Detailed Predictions (First 30 points)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Test Sample')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Error distribution\n",
    "all_errors = {\n",
    "    'Simple RNN': np.abs(y_test - y_pred.flatten()),\n",
    "    'LSTM': np.abs(y_test - lstm_pred.flatten()),\n",
    "    'GRU': np.abs(y_test - gru_pred.flatten())\n",
    "}\n",
    "\n",
    "for i, (model, errors) in enumerate(all_errors.items()):\n",
    "    axes[1, 1].hist(errors, bins=30, alpha=0.6, label=model, density=True)\n",
    "\n",
    "axes[1, 1].set_title('ğŸ“Š Error Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Absolute Error')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance vs Complexity scatter plot\n",
    "axes[1, 2].scatter(param_counts, rmse_values, s=100, alpha=0.7,\n",
    "                   c=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])\n",
    "for i, model in enumerate(model_names):\n",
    "    axes[1, 2].annotate(model, (param_counts[i], rmse_values[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 2].set_title('âš–ï¸ Performance vs Complexity', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Number of Parameters')\n",
    "axes[1, 2].set_ylabel('RMSE')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ GRU INSIGHTS:\")\n",
    "print(\"-\" * 17)\n",
    "print(\"ğŸ”¸ GRU often provides similar performance to LSTM with fewer parameters\")\n",
    "print(\"ğŸ”¸ Training time is typically faster due to simpler architecture\")\n",
    "print(\"ğŸ”¸ Good choice when computational resources are limited\")\n",
    "print(\"ğŸ”¸ Stacked GRUs can capture more complex patterns\")\n",
    "print(\"ğŸ”¸ Less prone to overfitting due to fewer parameters\")\n",
    "\n",
    "print(f\"\\nğŸ¯ MODEL SELECTION GUIDELINES:\")\n",
    "print(\"-\" * 34)\n",
    "print(\"ğŸ”¸ Simple RNN: Basic patterns, fast prototyping\")\n",
    "print(\"ğŸ”¸ LSTM: Complex long-term dependencies, large datasets\")\n",
    "print(\"ğŸ”¸ GRU: Good balance of performance and efficiency\")\n",
    "print(\"ğŸ”¸ Bidirectional: When future context is available\")\n",
    "print(\"ğŸ”¸ Stacked: Very complex patterns, sufficient data\")\n",
    "\n",
    "print(f\"\\nğŸ† WINNER ANALYSIS:\")\n",
    "print(\"-\" * 20)\n",
    "best_performance = rnn_comparison_df.loc[rnn_comparison_df['RMSE'].idxmin()]\n",
    "print(f\"   Best Performance: {best_performance['Model']}\")\n",
    "print(f\"   RMSE: {best_performance['RMSE']:.6f}\")\n",
    "print(f\"   Parameters: {best_performance['Parameters']:,}\")\n",
    "\n",
    "# Find best efficiency (performance/parameter ratio)\n",
    "efficiency = rnn_comparison_df['RMSE'] / (rnn_comparison_df['Parameters'] / 1000)\n",
    "best_efficiency_idx = efficiency.idxmin()\n",
    "best_efficient = rnn_comparison_df.loc[best_efficiency_idx, 'Model']\n",
    "print(f\"   Best Efficiency: {best_efficient}\")\n",
    "\n",
    "print(f\"\\nâœ… GRU demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca95ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ” 4. CNN untuk Sequences\n",
    "\n",
    "## ğŸ¤” CNN untuk Sequential Data?\n",
    "\n",
    "**Convolutional Neural Networks (CNN)** tidak hanya untuk image processing! CNN juga powerful untuk **sequence data**:\n",
    "\n",
    "### âœ… **Keunggulan CNN untuk Sequences**:\n",
    "- **ğŸš€ Parallel Processing** - Tidak sequential seperti RNN\n",
    "- **âš¡ Faster Training** - Parallel computation vs sequential RNN\n",
    "- **ğŸ¯ Local Patterns** - Excellent untuk mendeteksi local patterns\n",
    "- **ğŸ“ Fixed Computation** - Tidak tergantung sequence length\n",
    "- **ğŸ” Feature Extraction** - Good untuk extracting hierarchical features\n",
    "\n",
    "### ğŸ¯ **Use Cases CNN untuk Sequences**:\n",
    "- **ğŸ“ Text Classification** - Document classification, sentiment analysis\n",
    "- **ğŸ”Š Audio Processing** - Speech recognition, music genre classification  \n",
    "- **ğŸ“ˆ Time Series** - Pattern recognition dalam financial data\n",
    "- **ğŸ§¬ Biological Sequences** - DNA/protein sequence analysis\n",
    "- **ğŸ“Š Sensor Data** - IoT sensor data analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Arsitektur CNN untuk Sequences\n",
    "\n",
    "### ğŸ” **1D Convolution Operation**:\n",
    "```\n",
    "Conv1D(filters, kernel_size, strides, padding)\n",
    "```\n",
    "\n",
    "**Example**: Kernel size 3 pada text\n",
    "```\n",
    "Input:  [w1, w2, w3, w4, w5, w6, w7]\n",
    "Kernel: [k1, k2, k3]\n",
    "\n",
    "Output: [f1, f2, f3, f4, f5]  # f1 = w1*k1 + w2*k2 + w3*k3\n",
    "```\n",
    "\n",
    "### ğŸ“ **Key Parameters**:\n",
    "- **Filters** - Number of feature maps to create\n",
    "- **Kernel Size** - Width of convolutional window\n",
    "- **Strides** - Step size for sliding window\n",
    "- **Padding** - Handle boundaries (same/valid)\n",
    "\n",
    "### ğŸ—ï¸ **Typical CNN Sequence Architecture**:\n",
    "```\n",
    "Input Sequence\n",
    "    â†“\n",
    "Conv1D + ReLU\n",
    "    â†“\n",
    "MaxPooling1D\n",
    "    â†“\n",
    "Conv1D + ReLU\n",
    "    â†“\n",
    "MaxPooling1D\n",
    "    â†“\n",
    "GlobalMaxPooling1D / Flatten\n",
    "    â†“\n",
    "Dense + Dropout\n",
    "    â†“\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†š **CNN vs RNN untuk Sequences**\n",
    "\n",
    "| Aspect | CNN | RNN |\n",
    "|--------|-----|-----|\n",
    "| **Processing** | Parallel | Sequential |\n",
    "| **Speed** | Faster training | Slower training |\n",
    "| **Memory** | Fixed | Grows with sequence |\n",
    "| **Long Dependencies** | Limited by kernel size | Better (especially LSTM) |\n",
    "| **Local Patterns** | Excellent | Good |\n",
    "| **Variable Length** | Requires padding | Native support |\n",
    "| **Interpretability** | Feature maps | Hidden states |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ **Hybrid Approaches**\n",
    "\n",
    "### ğŸ¤ **CNN + RNN**:\n",
    "```\n",
    "CNN (local features) â†’ RNN (sequential dependencies)\n",
    "```\n",
    "\n",
    "### ğŸ¤ **RNN + CNN**:\n",
    "```\n",
    "RNN (sequence processing) â†’ CNN (feature extraction)\n",
    "```\n",
    "\n",
    "### ğŸ¤ **Parallel CNN-RNN**:\n",
    "```\n",
    "         Input\n",
    "        /     \\\n",
    "      CNN     RNN\n",
    "        \\     /\n",
    "       Concatenate\n",
    "          â†“\n",
    "        Dense\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aa49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” 4.1 CNN untuk Sequences - Practical Implementation\n",
    "print(\"ğŸ” CNN FOR SEQUENCES DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“Š PREPARING DATA FOR CNN\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "# For CNN, we need to pad sequences to make them fixed length\n",
    "# Let's create a more complex synthetic dataset\n",
    "def generate_complex_sequence(length=1000):\n",
    "    \"\"\"Generate more complex sequence data with multiple patterns\"\"\"\n",
    "    t = np.linspace(0, 10, length)\n",
    "    \n",
    "    # Combine multiple patterns\n",
    "    signal1 = np.sin(2 * np.pi * 0.5 * t)  # Slow oscillation\n",
    "    signal2 = 0.5 * np.sin(2 * np.pi * 2 * t)  # Fast oscillation  \n",
    "    trend = 0.1 * t  # Linear trend\n",
    "    noise = 0.1 * np.random.randn(length)\n",
    "    \n",
    "    combined = signal1 + signal2 + trend + noise\n",
    "    return combined\n",
    "\n",
    "# Generate complex sequence\n",
    "print(\"Generating complex synthetic data...\")\n",
    "complex_sequence = generate_complex_sequence(1000)\n",
    "\n",
    "# Visualize complex data\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(complex_sequence[:200], linewidth=2)\n",
    "plt.title('ğŸ” Complex Synthetic Sequence (Multiple Patterns)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create sequences for CNN (same function as before)\n",
    "X_complex, y_complex = create_sequences(complex_sequence, sequence_length)\n",
    "\n",
    "# Train/test split\n",
    "train_size_complex = int(0.8 * len(X_complex))\n",
    "X_train_cnn = X_complex[:train_size_complex]\n",
    "X_test_cnn = X_complex[train_size_complex:]\n",
    "y_train_cnn = y_complex[:train_size_complex]\n",
    "y_test_cnn = y_complex[train_size_complex:]\n",
    "\n",
    "# Reshape for CNN (samples, timesteps, features)\n",
    "X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))\n",
    "X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n",
    "\n",
    "print(f\"âœ… CNN Data Preparation:\")\n",
    "print(f\"   Training shape: {X_train_cnn.shape}\")\n",
    "print(f\"   Test shape: {X_test_cnn.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING CNN MODEL\")\n",
    "print(\"-\" * 23)\n",
    "\n",
    "# Build CNN model for sequences\n",
    "cnn_model = models.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu', \n",
    "                  input_shape=(sequence_length, 1), name='conv1d_1'),\n",
    "    layers.MaxPooling1D(pool_size=2, name='maxpool1d_1'),\n",
    "    \n",
    "    # Second convolutional block  \n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'),\n",
    "    layers.MaxPooling1D(pool_size=2, name='maxpool1d_2'),\n",
    "    \n",
    "    # Global pooling to handle variable length\n",
    "    layers.GlobalMaxPooling1D(name='global_maxpool'),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(50, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… CNN Architecture:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Compile model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING CNN MODEL\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Train CNN model\n",
    "print(\"Training CNN for sequences...\")\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    batch_size=32,\n",
    "    epochs=25,\n",
    "    validation_data=(X_test_cnn, y_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_loss, cnn_mae = cnn_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "cnn_pred = cnn_model.predict(X_test_cnn, verbose=0)\n",
    "cnn_rmse = np.sqrt(mean_squared_error(y_test_cnn, cnn_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š CNN RESULTS\")\n",
    "print(\"-\" * 14)\n",
    "print(f\"ğŸ¯ CNN Test Results:\")\n",
    "print(f\"   Loss (MSE): {cnn_loss:.6f}\")\n",
    "print(f\"   RMSE: {cnn_rmse:.6f}\")\n",
    "print(f\"   MAE: {cnn_mae:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ¤ HYBRID CNN-RNN MODEL\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Build hybrid CNN-RNN model\n",
    "hybrid_model = models.Sequential([\n",
    "    # CNN part for local feature extraction\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu',\n",
    "                  input_shape=(sequence_length, 1), name='conv1d'),\n",
    "    layers.MaxPooling1D(pool_size=2, name='maxpool1d'),\n",
    "    \n",
    "    # RNN part for sequential dependencies\n",
    "    layers.LSTM(50, return_sequences=False, name='lstm'),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(25, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.3, name='dropout'),\n",
    "    layers.Dense(1, name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… Hybrid CNN-RNN Architecture:\")\n",
    "hybrid_model.summary()\n",
    "\n",
    "# Compile and train\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Training Hybrid CNN-RNN...\")\n",
    "hybrid_history = hybrid_model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test_cnn, y_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate hybrid model\n",
    "hybrid_loss, hybrid_mae = hybrid_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "hybrid_pred = hybrid_model.predict(X_test_cnn, verbose=0)\n",
    "hybrid_rmse = np.sqrt(mean_squared_error(y_test_cnn, hybrid_pred))\n",
    "\n",
    "print(f\"\\nğŸ¤ PARALLEL CNN-RNN MODEL\")\n",
    "print(\"-\" * 29)\n",
    "\n",
    "# Build parallel CNN-RNN model using Functional API\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(sequence_length, 1))\n",
    "\n",
    "# CNN branch\n",
    "cnn_branch = layers.Conv1D(32, 3, activation='relu')(input_layer)\n",
    "cnn_branch = layers.MaxPooling1D(2)(cnn_branch)\n",
    "cnn_branch = layers.Conv1D(64, 3, activation='relu')(cnn_branch)\n",
    "cnn_branch = layers.GlobalMaxPooling1D()(cnn_branch)\n",
    "\n",
    "# RNN branch  \n",
    "rnn_branch = layers.LSTM(50, return_sequences=False)(input_layer)\n",
    "\n",
    "# Combine branches\n",
    "combined = Concatenate()([cnn_branch, rnn_branch])\n",
    "combined = layers.Dense(50, activation='relu')(combined)\n",
    "combined = layers.Dropout(0.3)(combined)\n",
    "output = layers.Dense(1)(combined)\n",
    "\n",
    "# Create model\n",
    "parallel_model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "print(\"âœ… Parallel CNN-RNN Architecture:\")\n",
    "parallel_model.summary()\n",
    "\n",
    "# Compile and train\n",
    "parallel_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Training Parallel CNN-RNN...\")\n",
    "parallel_history = parallel_model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(X_test_cnn, y_test_cnn),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate parallel model\n",
    "parallel_loss, parallel_mae = parallel_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "parallel_pred = parallel_model.predict(X_test_cnn, verbose=0)\n",
    "parallel_rmse = np.sqrt(mean_squared_error(y_test_cnn, parallel_pred))\n",
    "\n",
    "print(f\"\\nğŸ“Š COMPREHENSIVE CNN COMPARISON\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Add previous best RNN model for comparison (use GRU as it was efficient)\n",
    "gru_pred_complex = gru_model.predict(X_test_cnn, verbose=0)\n",
    "gru_rmse_complex = np.sqrt(mean_squared_error(y_test_cnn, gru_pred_complex))\n",
    "\n",
    "# Compare all models including CNN approaches\n",
    "cnn_comparison_data = {\n",
    "    'Model': ['GRU (Best RNN)', 'CNN', 'Hybrid CNN-RNN', 'Parallel CNN-RNN'],\n",
    "    'RMSE': [gru_rmse_complex, cnn_rmse, hybrid_rmse, parallel_rmse],\n",
    "    'Parameters': [\n",
    "        gru_model.count_params(),\n",
    "        cnn_model.count_params(),\n",
    "        hybrid_model.count_params(),\n",
    "        parallel_model.count_params()\n",
    "    ]\n",
    "}\n",
    "\n",
    "cnn_comparison_df = pd.DataFrame(cnn_comparison_data)\n",
    "print(cnn_comparison_df.round(6))\n",
    "\n",
    "# Find best CNN approach\n",
    "best_cnn_idx = cnn_comparison_df['RMSE'].idxmin()\n",
    "best_cnn = cnn_comparison_df.loc[best_cnn_idx, 'Model']\n",
    "print(f\"\\nğŸ† Best Model: {best_cnn}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ CNN VISUALIZATION\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Create comprehensive CNN visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Training progress\n",
    "axes[0, 0].plot(cnn_history.history['val_loss'], label='CNN', marker='o', alpha=0.8)\n",
    "axes[0, 0].plot(hybrid_history.history['val_loss'], label='Hybrid CNN-RNN', marker='s', alpha=0.8)\n",
    "axes[0, 0].plot(parallel_history.history['val_loss'], label='Parallel CNN-RNN', marker='^', alpha=0.8)\n",
    "axes[0, 0].set_title('ğŸš€ CNN Models Training Progress', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Performance comparison\n",
    "model_names_cnn = cnn_comparison_df['Model']\n",
    "rmse_values_cnn = cnn_comparison_df['RMSE']\n",
    "bars = axes[0, 1].bar(range(len(model_names_cnn)), rmse_values_cnn,\n",
    "                      color=['lightblue', 'lightgreen', 'orange', 'pink'])\n",
    "axes[0, 1].set_title('ğŸ“Š Model Performance Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(range(len(model_names_cnn)))\n",
    "axes[0, 1].set_xticklabels(model_names_cnn, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, rmse_values_cnn):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "                    f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Predictions comparison\n",
    "test_range_cnn = range(40)  # First 40 points\n",
    "axes[1, 0].plot(test_range_cnn, y_test_cnn[:40], label='Actual', linewidth=3, alpha=0.9)\n",
    "axes[1, 0].plot(test_range_cnn, cnn_pred[:40], label='CNN', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].plot(test_range_cnn, hybrid_pred[:40], label='Hybrid CNN-RNN', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].plot(test_range_cnn, parallel_pred[:40], label='Parallel CNN-RNN', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].set_title('ğŸ”® Predictions Comparison (First 40 points)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Test Sample')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model complexity vs performance\n",
    "param_counts_cnn = cnn_comparison_df['Parameters']\n",
    "axes[1, 1].scatter(param_counts_cnn, rmse_values_cnn, s=150, alpha=0.7,\n",
    "                   c=['lightblue', 'lightgreen', 'orange', 'pink'])\n",
    "for i, model in enumerate(model_names_cnn):\n",
    "    axes[1, 1].annotate(model, (param_counts_cnn[i], rmse_values_cnn[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 1].set_title('âš–ï¸ Complexity vs Performance', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Number of Parameters')\n",
    "axes[1, 1].set_ylabel('RMSE')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ CNN FOR SEQUENCES INSIGHTS:\")\n",
    "print(\"-\" * 32)\n",
    "print(\"ğŸ”¸ CNN can be very effective for sequence data with local patterns\")\n",
    "print(\"ğŸ”¸ Much faster training than RNNs due to parallel processing\")\n",
    "print(\"ğŸ”¸ Hybrid approaches can combine best of both worlds\")\n",
    "print(\"ğŸ”¸ CNN excels when patterns are more spatial than temporal\")\n",
    "print(\"ğŸ”¸ Global pooling helps handle variable sequence lengths\")\n",
    "\n",
    "print(f\"\\nğŸ¯ WHEN TO USE EACH APPROACH:\")\n",
    "print(\"-\" * 33)\n",
    "print(\"ğŸ”¸ Pure CNN: Fast processing, local patterns dominant\")\n",
    "print(\"ğŸ”¸ Hybrid CNN-RNN: CNN features + RNN sequential modeling\")\n",
    "print(\"ğŸ”¸ Parallel CNN-RNN: Combine different perspectives\")\n",
    "print(\"ğŸ”¸ Pure RNN: Long-term dependencies are crucial\")\n",
    "\n",
    "print(f\"\\nâš¡ PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 24)\n",
    "for i, model_name in enumerate(model_names_cnn):\n",
    "    print(f\"   {model_name:18}: RMSE={rmse_values_cnn[i]:.6f}, Params={param_counts_cnn[i]:,}\")\n",
    "\n",
    "print(f\"\\nâœ… CNN for sequences demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c1c5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ˆ 5. Time Series Forecasting\n",
    "\n",
    "## ğŸ¯ Apa itu Time Series Forecasting?\n",
    "\n",
    "**Time Series Forecasting** adalah prediksi nilai future berdasarkan historical data yang **time-dependent**.\n",
    "\n",
    "### ğŸ¢ **Real-World Applications**:\n",
    "- **ğŸ“Š Financial** - Stock prices, cryptocurrency, forex\n",
    "- **ğŸŒ¡ï¸ Weather** - Temperature, rainfall, atmospheric pressure\n",
    "- **âš¡ Energy** - Power consumption, renewable energy generation\n",
    "- **ğŸ›’ Retail** - Sales forecasting, inventory management\n",
    "- **ğŸš— Transportation** - Traffic flow, demand prediction\n",
    "- **ğŸ¥ Healthcare** - Patient admissions, disease spread\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Types of Time Series Problems\n",
    "\n",
    "### ğŸ“Š **1. Univariate Time Series**\n",
    "- **Single variable** over time\n",
    "- **Example**: Daily stock price of single company\n",
    "- **Input**: [price(t-n), ..., price(t-1)] â†’ **Output**: price(t)\n",
    "\n",
    "### ğŸ“Š **2. Multivariate Time Series**  \n",
    "- **Multiple variables** over time\n",
    "- **Example**: Weather data (temperature, humidity, pressure)\n",
    "- **Input**: [temp(t-n), humid(t-n), press(t-n), ..., temp(t-1), humid(t-1), press(t-1)]\n",
    "- **Output**: temp(t), humid(t), press(t)\n",
    "\n",
    "### ğŸ“Š **3. Multi-step Forecasting**\n",
    "- **Predict multiple future values**\n",
    "- **Example**: Next 7 days of stock prices\n",
    "- **Input**: [price(t-n), ..., price(t-1)] â†’ **Output**: [price(t), price(t+1), ..., price(t+6)]\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Deep Learning Approaches\n",
    "\n",
    "### ğŸ”„ **RNN-based Approaches**:\n",
    "- **Simple RNN** - Basic temporal patterns\n",
    "- **LSTM** - Long-term dependencies in time series\n",
    "- **GRU** - Efficient alternative to LSTM\n",
    "- **Bidirectional** - When future context available (not for real-time)\n",
    "\n",
    "### ğŸ” **CNN-based Approaches**:\n",
    "- **1D CNN** - Local temporal patterns\n",
    "- **Dilated CNN** - Wider receptive fields\n",
    "- **CNN-RNN Hybrid** - Local + global patterns\n",
    "\n",
    "### ğŸš€ **Advanced Approaches**:\n",
    "- **Attention Mechanisms** - Focus on relevant time steps\n",
    "- **Transformer** - Self-attention for sequences\n",
    "- **Prophet** - Facebook's time series forecasting tool\n",
    "- **Neural ODE** - Continuous-time neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Key Challenges in Time Series\n",
    "\n",
    "### âš ï¸ **Data Challenges**:\n",
    "- **Seasonality** - Recurring patterns (daily, weekly, yearly)\n",
    "- **Trend** - Long-term direction of data\n",
    "- **Noise** - Random fluctuations\n",
    "- **Missing Values** - Gaps in temporal data\n",
    "- **Outliers** - Extreme values that can skew predictions\n",
    "\n",
    "### âš ï¸ **Modeling Challenges**:\n",
    "- **Non-stationarity** - Statistical properties change over time\n",
    "- **Multiple Seasonalities** - Different recurring patterns\n",
    "- **Concept Drift** - Underlying patterns change\n",
    "- **Cold Start** - Limited historical data\n",
    "- **Real-time Constraints** - Fast prediction requirements\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Preprocessing Techniques\n",
    "\n",
    "### ğŸ“ **Normalization**:\n",
    "```python\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler() \n",
    "scaled_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Differencing** (Remove Trend):\n",
    "```python\n",
    "# First difference\n",
    "diff_data = data.diff().dropna()\n",
    "\n",
    "# Seasonal difference\n",
    "seasonal_diff = data.diff(12).dropna()  # 12 for monthly data\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Feature Engineering**:\n",
    "- **Lag Features** - Previous values as features\n",
    "- **Rolling Statistics** - Moving averages, rolling std\n",
    "- **Time Features** - Hour, day, month, year\n",
    "- **Fourier Features** - Capture cyclical patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ 5.1 Time Series Forecasting - Practical Implementation  \n",
    "print(\"ğŸ“ˆ TIME SERIES FORECASTING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“Š CREATING REALISTIC TIME SERIES DATA\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "def create_realistic_timeseries(length=1000):\n",
    "    \"\"\"Create realistic time series with trend, seasonality, and noise\"\"\"\n",
    "    t = np.arange(length)\n",
    "    \n",
    "    # Components\n",
    "    trend = 0.02 * t  # Linear upward trend\n",
    "    seasonal_yearly = 10 * np.sin(2 * np.pi * t / 365)  # Yearly seasonality\n",
    "    seasonal_weekly = 5 * np.sin(2 * np.pi * t / 7)     # Weekly seasonality  \n",
    "    noise = 2 * np.random.randn(length)\n",
    "    \n",
    "    # Combine all components\n",
    "    ts = 100 + trend + seasonal_yearly + seasonal_weekly + noise\n",
    "    \n",
    "    # Add some non-linear components\n",
    "    ts += 3 * np.sin(2 * np.pi * t / 50) * np.exp(-t/500)  # Decaying oscillation\n",
    "    \n",
    "    return ts\n",
    "\n",
    "# Generate realistic time series\n",
    "print(\"Generating realistic time series data...\")\n",
    "ts_data = create_realistic_timeseries(1000)\n",
    "dates = pd.date_range(start='2021-01-01', periods=len(ts_data), freq='D')\n",
    "ts_df = pd.DataFrame({'date': dates, 'value': ts_data})\n",
    "\n",
    "print(f\"âœ… Generated time series with {len(ts_data)} days of data\")\n",
    "\n",
    "# Visualize the time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Full time series\n",
    "axes[0, 0].plot(ts_df['date'], ts_df['value'], linewidth=1.5)\n",
    "axes[0, 0].set_title('ğŸ“ˆ Complete Time Series (1000 days)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# First 100 days (detailed view)\n",
    "axes[0, 1].plot(ts_df['date'][:100], ts_df['value'][:100], linewidth=2)\n",
    "axes[0, 1].set_title('ğŸ” Detailed View (First 100 days)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[1, 0].hist(ts_df['value'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('ğŸ“Š Value Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly pattern (sample weeks)\n",
    "sample_weeks = ts_df.iloc[:70]  # First 10 weeks\n",
    "sample_weeks['day_of_week'] = sample_weeks['date'].dt.day_name()\n",
    "weekly_avg = sample_weeks.groupby('day_of_week')['value'].mean()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_avg = weekly_avg.reindex(day_order)\n",
    "\n",
    "axes[1, 1].bar(range(7), weekly_avg.values, color='skyblue', alpha=0.8)\n",
    "axes[1, 1].set_title('ğŸ“… Weekly Pattern (Average)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Day of Week')\n",
    "axes[1, 1].set_ylabel('Average Value')\n",
    "axes[1, 1].set_xticks(range(7))\n",
    "axes[1, 1].set_xticklabels([day[:3] for day in day_order])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ”§ PREPROCESSING FOR FORECASTING\")\n",
    "print(\"-\" * 34)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(ts_df['value'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"âœ… Data normalized to range [{scaled_values.min():.3f}, {scaled_values.max():.3f}]\")\n",
    "\n",
    "# Create sequences for forecasting\n",
    "def create_forecast_sequences(data, seq_length, forecast_horizon=1):\n",
    "    \"\"\"Create sequences for time series forecasting\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        if forecast_horizon == 1:\n",
    "            y.append(data[i + seq_length])\n",
    "        else:\n",
    "            y.append(data[i + seq_length:i + seq_length + forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters for forecasting\n",
    "seq_length = 30  # Use 30 days to predict next day(s)\n",
    "forecast_horizon = 1  # Predict 1 day ahead\n",
    "\n",
    "# Create sequences\n",
    "X_ts, y_ts = create_forecast_sequences(scaled_values, seq_length, forecast_horizon)\n",
    "\n",
    "print(f\"âœ… Sequence creation:\")\n",
    "print(f\"   Sequence length: {seq_length} days\")\n",
    "print(f\"   Forecast horizon: {forecast_horizon} day(s)\")\n",
    "print(f\"   Input shape: {X_ts.shape}\")\n",
    "print(f\"   Target shape: {y_ts.shape}\")\n",
    "\n",
    "# Train/validation/test split (60/20/20)\n",
    "n_samples = len(X_ts)\n",
    "train_size = int(0.6 * n_samples)\n",
    "val_size = int(0.2 * n_samples)\n",
    "\n",
    "X_train_ts = X_ts[:train_size]\n",
    "y_train_ts = y_ts[:train_size]\n",
    "X_val_ts = X_ts[train_size:train_size + val_size]\n",
    "y_val_ts = y_ts[train_size:train_size + val_size]\n",
    "X_test_ts = X_ts[train_size + val_size:]\n",
    "y_test_ts = y_ts[train_size + val_size:]\n",
    "\n",
    "# Reshape for RNN\n",
    "X_train_ts = X_train_ts.reshape((X_train_ts.shape[0], X_train_ts.shape[1], 1))\n",
    "X_val_ts = X_val_ts.reshape((X_val_ts.shape[0], X_val_ts.shape[1], 1))\n",
    "X_test_ts = X_test_ts.reshape((X_test_ts.shape[0], X_test_ts.shape[1], 1))\n",
    "\n",
    "print(f\"   Train: {X_train_ts.shape}, Val: {X_val_ts.shape}, Test: {X_test_ts.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING FORECASTING MODELS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# 1. Simple LSTM Forecaster\n",
    "lstm_forecaster = models.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "], name='LSTM_Forecaster')\n",
    "\n",
    "# 2. GRU Forecaster\n",
    "gru_forecaster = models.Sequential([\n",
    "    layers.GRU(64, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GRU(32, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "], name='GRU_Forecaster')\n",
    "\n",
    "# 3. CNN-LSTM Hybrid Forecaster\n",
    "cnn_lstm_forecaster = models.Sequential([\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(seq_length, 1)),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(50, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "], name='CNN_LSTM_Forecaster')\n",
    "\n",
    "# Compile all models\n",
    "models_ts = [lstm_forecaster, gru_forecaster, cnn_lstm_forecaster]\n",
    "model_names_ts = ['LSTM Forecaster', 'GRU Forecaster', 'CNN-LSTM Forecaster']\n",
    "\n",
    "for model in models_ts:\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"âœ… Built 3 forecasting models:\")\n",
    "for name in model_names_ts:\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING FORECASTING MODELS\")\n",
    "print(\"-\" * 33)\n",
    "\n",
    "# Train all models\n",
    "histories_ts = []\n",
    "predictions_ts = []\n",
    "metrics_ts = []\n",
    "\n",
    "for i, (model, name) in enumerate(zip(models_ts, model_names_ts)):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_ts, y_train_ts,\n",
    "        batch_size=32,\n",
    "        epochs=15,\n",
    "        validation_data=(X_val_ts, y_val_ts),\n",
    "        verbose=1\n",
    "    )\n",
    "    histories_ts.append(history)\n",
    "    \n",
    "    # Evaluate and predict\n",
    "    test_loss, test_mae = model.evaluate(X_test_ts, y_test_ts, verbose=0)\n",
    "    predictions = model.predict(X_test_ts, verbose=0)\n",
    "    predictions_ts.append(predictions.flatten())\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_ts, predictions))\n",
    "    mae = mean_absolute_error(y_test_ts, predictions)\n",
    "    \n",
    "    metrics_ts.append({\n",
    "        'Model': name,\n",
    "        'MSE': test_loss,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Parameters': model.count_params()\n",
    "    })\n",
    "    \n",
    "    print(f\"   âœ… {name} - RMSE: {rmse:.6f}, MAE: {mae:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FORECASTING RESULTS COMPARISON\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "ts_results_df = pd.DataFrame(metrics_ts)\n",
    "print(ts_results_df.round(6))\n",
    "\n",
    "# Find best model\n",
    "best_forecaster_idx = ts_results_df['RMSE'].idxmin()\n",
    "best_forecaster = ts_results_df.loc[best_forecaster_idx, 'Model']\n",
    "print(f\"\\nğŸ† Best Forecaster: {best_forecaster}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ MULTI-STEP FORECASTING\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Build multi-step forecaster (predict next 7 days)\n",
    "multi_step_horizon = 7\n",
    "X_multi, y_multi = create_forecast_sequences(scaled_values, seq_length, multi_step_horizon)\n",
    "\n",
    "# Use only recent data for multi-step (to speed up training)\n",
    "n_multi = min(500, len(X_multi))\n",
    "X_multi = X_multi[-n_multi:]\n",
    "y_multi = y_multi[-n_multi:]\n",
    "\n",
    "train_size_multi = int(0.7 * len(X_multi))\n",
    "X_train_multi = X_multi[:train_size_multi].reshape((train_size_multi, seq_length, 1))\n",
    "y_train_multi = y_multi[:train_size_multi]\n",
    "X_test_multi = X_multi[train_size_multi:].reshape((len(X_multi) - train_size_multi, seq_length, 1))\n",
    "y_test_multi = y_multi[train_size_multi:]\n",
    "\n",
    "# Multi-step forecaster model\n",
    "multi_step_model = models.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(multi_step_horizon)  # Output 7 values\n",
    "], name='Multi_Step_Forecaster')\n",
    "\n",
    "multi_step_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"Training Multi-step Forecaster (predicting {multi_step_horizon} days ahead)...\")\n",
    "multi_history = multi_step_model.fit(\n",
    "    X_train_multi, y_train_multi,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate multi-step model\n",
    "multi_pred = multi_step_model.predict(X_test_multi, verbose=0)\n",
    "multi_rmse = np.sqrt(mean_squared_error(y_test_multi, multi_pred))\n",
    "print(f\"âœ… Multi-step RMSE: {multi_rmse:.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FORECASTING VISUALIZATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create comprehensive forecasting visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Training progress\n",
    "for i, (history, name) in enumerate(zip(histories_ts, model_names_ts)):\n",
    "    axes[0, 0].plot(history.history['val_loss'], label=name, marker='o', alpha=0.8)\n",
    "axes[0, 0].set_title('ğŸš€ Forecasting Models Training Progress', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Performance comparison\n",
    "bars = axes[0, 1].bar(range(len(model_names_ts)), ts_results_df['RMSE'], color=['lightblue', 'lightgreen', 'orange'])\n",
    "axes[0, 1].set_title('ğŸ“Š Forecasting Performance (RMSE)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(range(len(model_names_ts)))\n",
    "axes[0, 1].set_xticklabels(model_names_ts, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, ts_results_df['RMSE']):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Single-step predictions comparison (first 50 test points)\n",
    "test_range = range(50)\n",
    "axes[1, 0].plot(test_range, y_test_ts[:50], label='Actual', linewidth=3, alpha=0.9, color='black')\n",
    "for i, (pred, name) in enumerate(zip(predictions_ts, model_names_ts)):\n",
    "    axes[1, 0].plot(test_range, pred[:50], label=name, linewidth=2, alpha=0.7)\n",
    "axes[1, 0].set_title('ğŸ”® Single-step Forecasting (First 50 test points)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Test Sample')\n",
    "axes[1, 0].set_ylabel('Scaled Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Multi-step forecasting example\n",
    "if len(multi_pred) > 0:\n",
    "    sample_idx = 5  # Sample multi-step prediction\n",
    "    days_range = range(multi_step_horizon)\n",
    "    axes[1, 1].plot(days_range, y_test_multi[sample_idx], 'o-', label='Actual', linewidth=3, markersize=6)\n",
    "    axes[1, 1].plot(days_range, multi_pred[sample_idx], 's-', label='Predicted', linewidth=3, markersize=6)\n",
    "    axes[1, 1].set_title(f'ğŸ“… Multi-step Forecast Example ({multi_step_horizon} days)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Days Ahead')\n",
    "    axes[1, 1].set_ylabel('Scaled Value')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Forecast error analysis\n",
    "best_predictions = predictions_ts[best_forecaster_idx]\n",
    "forecast_errors = y_test_ts - best_predictions\n",
    "axes[2, 0].scatter(y_test_ts, best_predictions, alpha=0.6, color='blue')\n",
    "axes[2, 0].plot([y_test_ts.min(), y_test_ts.max()], [y_test_ts.min(), y_test_ts.max()], 'r--', linewidth=2)\n",
    "axes[2, 0].set_title(f'ğŸ¯ {best_forecaster} - Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Actual Values')\n",
    "axes[2, 0].set_ylabel('Predicted Values')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Residual analysis\n",
    "axes[2, 1].hist(forecast_errors, bins=30, alpha=0.7, edgecolor='black', color='lightcoral')\n",
    "axes[2, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[2, 1].set_title(f'ğŸ“Š {best_forecaster} - Residual Distribution', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Forecast Error')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ TIME SERIES FORECASTING INSIGHTS:\")\n",
    "print(\"-\" * 38)\n",
    "print(\"ğŸ”¸ LSTM and GRU are effective for time series with temporal dependencies\")\n",
    "print(\"ğŸ”¸ CNN-LSTM hybrid can capture both local and temporal patterns\")\n",
    "print(\"ğŸ”¸ Multi-step forecasting becomes more challenging with longer horizons\")\n",
    "print(\"ğŸ”¸ Data preprocessing (normalization) is crucial for good performance\")\n",
    "print(\"ğŸ”¸ Validation strategy should respect temporal order (no future leakage)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FORECASTING PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 36)\n",
    "for _, row in ts_results_df.iterrows():\n",
    "    print(f\"   {row['Model']:20}: RMSE={row['RMSE']:.6f}, Params={row['Parameters']:,}\")\n",
    "\n",
    "print(f\"\\nâœ… Time series forecasting demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689c2f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ 6. Text Processing dengan RNN\n",
    "\n",
    "## ğŸ¯ Mengapa RNN untuk Text?\n",
    "\n",
    "**Text** adalah sequential data alami yang sangat cocok untuk RNN processing:\n",
    "\n",
    "### ğŸ“ **Karakteristik Text sebagai Sequence**:\n",
    "- **Word Order Matters** - \"Dog bites man\" â‰  \"Man bites dog\"\n",
    "- **Variable Length** - Sentences have different lengths\n",
    "- **Context Dependencies** - Meaning depends on surrounding words\n",
    "- **Long-range Dependencies** - Pronouns refer to nouns mentioned earlier\n",
    "\n",
    "### ğŸ¢ **Text Processing Applications**:\n",
    "- **ğŸ“Š Sentiment Analysis** - Positive/negative/neutral classification\n",
    "- **ğŸ·ï¸ Text Classification** - Topic categorization, spam detection\n",
    "- **ğŸ”¤ Named Entity Recognition** - Find persons, organizations, locations\n",
    "- **ğŸŒ Machine Translation** - Translate between languages\n",
    "- **ğŸ’¬ Chatbots** - Conversational AI systems\n",
    "- **ğŸ“„ Text Summarization** - Automatic summary generation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Text Preprocessing Pipeline\n",
    "\n",
    "### 1ï¸âƒ£ **Tokenization**\n",
    "```python\n",
    "\"Hello world!\" â†’ [\"Hello\", \"world\", \"!\"]\n",
    "```\n",
    "\n",
    "### 2ï¸âƒ£ **Lowercasing**\n",
    "```python\n",
    "[\"Hello\", \"world\", \"!\"] â†’ [\"hello\", \"world\", \"!\"]\n",
    "```\n",
    "\n",
    "### 3ï¸âƒ£ **Remove Punctuation/Stop Words**\n",
    "```python\n",
    "[\"hello\", \"world\", \"!\"] â†’ [\"hello\", \"world\"]\n",
    "```\n",
    "\n",
    "### 4ï¸âƒ£ **Vocabulary Building**\n",
    "```python\n",
    "# Create word-to-index mapping\n",
    "vocab = {\"hello\": 1, \"world\": 2, \"<UNK>\": 3, \"<PAD>\": 0}\n",
    "```\n",
    "\n",
    "### 5ï¸âƒ£ **Sequence Conversion**\n",
    "```python\n",
    "[\"hello\", \"world\"] â†’ [1, 2]\n",
    "```\n",
    "\n",
    "### 6ï¸âƒ£ **Padding**\n",
    "```python\n",
    "[1, 2] â†’ [1, 2, 0, 0, 0]  # Pad to max_length\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Text Classification Architecture\n",
    "\n",
    "### ğŸ—ï¸ **Basic RNN Text Classifier**:\n",
    "```\n",
    "Input: \"This movie is great!\"\n",
    "    â†“\n",
    "Tokenize: [\"this\", \"movie\", \"is\", \"great\"]\n",
    "    â†“  \n",
    "Embed: [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]]\n",
    "    â†“\n",
    "RNN: Process sequence â†’ final hidden state\n",
    "    â†“\n",
    "Dense: hidden_state â†’ class probabilities\n",
    "    â†“\n",
    "Output: [0.1, 0.9] â†’ \"Positive\"\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Advanced Architectures**:\n",
    "- **Bidirectional RNN** - Process text forwards and backwards\n",
    "- **Attention Mechanism** - Focus on important words\n",
    "- **Hierarchical RNN** - Word-level + sentence-level processing\n",
    "- **CNN + RNN** - Local features + sequential processing\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ­ Sentiment Analysis Example\n",
    "\n",
    "### ğŸ“Š **Problem Setup**:\n",
    "- **Input**: Text review/comment\n",
    "- **Output**: Sentiment (Positive/Negative/Neutral)\n",
    "- **Challenge**: Sarcasm, negation, context\n",
    "\n",
    "### ğŸ”§ **Implementation Steps**:\n",
    "1. **Data Collection** - Movie reviews, social media posts\n",
    "2. **Preprocessing** - Clean, tokenize, pad sequences\n",
    "3. **Embedding** - Convert words to dense vectors\n",
    "4. **RNN Processing** - LSTM/GRU for sequence modeling\n",
    "5. **Classification** - Dense layer with softmax\n",
    "6. **Training** - Minimize cross-entropy loss\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Advanced Techniques\n",
    "\n",
    "### ğŸ¯ **Word Embeddings**:\n",
    "- **Word2Vec** - Skip-gram and CBOW\n",
    "- **GloVe** - Global vectors for word representation\n",
    "- **FastText** - Subword embeddings\n",
    "- **Pre-trained** - Use embeddings trained on large corpora\n",
    "\n",
    "### ğŸ” **Attention Mechanisms**:\n",
    "- **Self-Attention** - Words attend to other words in same sentence\n",
    "- **Cross-Attention** - Words attend to words in different sequences\n",
    "- **Multi-Head Attention** - Multiple attention mechanisms in parallel\n",
    "\n",
    "### ğŸš€ **Transfer Learning**:\n",
    "- **Pre-trained Models** - BERT, GPT, RoBERTa\n",
    "- **Fine-tuning** - Adapt pre-trained models to specific tasks\n",
    "- **Feature Extraction** - Use pre-trained embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ 6.1 Text Processing - Sentiment Analysis with RNN\n",
    "print(\"ğŸ“ TEXT PROCESSING WITH RNN DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“Š CREATING SYNTHETIC TEXT DATASET\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create synthetic movie reviews dataset for demonstration\n",
    "positive_reviews = [\n",
    "    \"This movie is absolutely fantastic and amazing\",\n",
    "    \"I loved every moment of this incredible film\",\n",
    "    \"Outstanding performance by the actors\",\n",
    "    \"A masterpiece of cinema that everyone should watch\",\n",
    "    \"Brilliant storytelling and excellent direction\",\n",
    "    \"One of the best movies I have ever seen\",\n",
    "    \"Exceptional cinematography and wonderful music\",\n",
    "    \"This film exceeded all my expectations completely\",\n",
    "    \"A truly remarkable and inspiring story\",\n",
    "    \"Perfect blend of drama and entertainment\",\n",
    "    \"Absolutely loved the characters and plot\",\n",
    "    \"This movie made me laugh and cry with joy\",\n",
    "    \"An unforgettable cinematic experience for sure\",\n",
    "    \"The acting was superb and very convincing\",\n",
    "    \"I would definitely recommend this to everyone\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This movie was terrible and completely boring\",\n",
    "    \"I hated every single minute of this film\",\n",
    "    \"Poor acting and terrible direction throughout\",\n",
    "    \"One of the worst movies ever made\",\n",
    "    \"Awful storyline and bad character development\",\n",
    "    \"This film was a complete waste of time\",\n",
    "    \"Horrible cinematography and annoying music\",\n",
    "    \"This movie disappointed me in every way\",\n",
    "    \"A truly awful and depressing story\",\n",
    "    \"Terrible blend of bad acting and poor writing\",\n",
    "    \"I absolutely hated the characters and plot\",\n",
    "    \"This movie made me want to leave early\",\n",
    "    \"A forgettable and painful experience overall\",\n",
    "    \"The acting was horrible and unconvincing\",\n",
    "    \"I would never recommend this to anyone\"\n",
    "]\n",
    "\n",
    "# Combine datasets\n",
    "reviews = positive_reviews + negative_reviews\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)  # 1=positive, 0=negative\n",
    "\n",
    "print(f\"âœ… Created synthetic dataset:\")\n",
    "print(f\"   Positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"   Negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"   Total reviews: {len(reviews)}\")\n",
    "\n",
    "# Show sample reviews\n",
    "print(f\"\\nğŸ“ Sample Reviews:\")\n",
    "print(f\"   Positive: '{positive_reviews[0]}'\")\n",
    "print(f\"   Negative: '{negative_reviews[0]}'\")\n",
    "\n",
    "print(f\"\\nğŸ”§ TEXT PREPROCESSING\")\n",
    "print(\"-\" * 21)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and keep only alphanumeric and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Split into tokens\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Preprocess all reviews\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]\n",
    "\n",
    "print(\"âœ… Preprocessing steps applied:\")\n",
    "print(\"   - Convert to lowercase\")\n",
    "print(\"   - Remove punctuation\")\n",
    "print(\"   - Tokenize into words\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = []\n",
    "for review in processed_reviews:\n",
    "    all_words.extend(review)\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "vocab_size = min(1000, len(word_counts))  # Limit vocab size\n",
    "most_common_words = word_counts.most_common(vocab_size - 2)  # Reserve 2 for special tokens\n",
    "\n",
    "# Create word-to-index mapping\n",
    "word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, _ in most_common_words:\n",
    "    word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"âœ… Vocabulary built:\")\n",
    "print(f\"   Vocabulary size: {len(word_to_idx)}\")\n",
    "print(f\"   Most common words: {list(word_to_idx.keys())[2:12]}\")\n",
    "\n",
    "def texts_to_sequences(texts, word_to_idx, max_length=20):\n",
    "    \"\"\"Convert texts to sequences of indices\"\"\"\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = []\n",
    "        for word in text:\n",
    "            if word in word_to_idx:\n",
    "                sequence.append(word_to_idx[word])\n",
    "            else:\n",
    "                sequence.append(word_to_idx['<UNK>'])\n",
    "        # Pad or truncate to max_length\n",
    "        if len(sequence) < max_length:\n",
    "            sequence.extend([word_to_idx['<PAD>']] * (max_length - len(sequence)))\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Convert to sequences\n",
    "max_length = 15  # Maximum sequence length\n",
    "X_text = texts_to_sequences(processed_reviews, word_to_idx, max_length)\n",
    "y_text = np.array(labels)\n",
    "\n",
    "print(f\"âœ… Sequence conversion:\")\n",
    "print(f\"   Max sequence length: {max_length}\")\n",
    "print(f\"   Input shape: {X_text.shape}\")\n",
    "print(f\"   Label shape: {y_text.shape}\")\n",
    "\n",
    "# Show example conversion\n",
    "sample_idx = 0\n",
    "print(f\"\\nğŸ“ Example conversion:\")\n",
    "print(f\"   Original: '{reviews[sample_idx]}'\")\n",
    "print(f\"   Processed: {processed_reviews[sample_idx]}\")\n",
    "print(f\"   Sequence: {X_text[sample_idx]}\")\n",
    "print(f\"   Label: {y_text[sample_idx]} ({'Positive' if y_text[sample_idx] == 1 else 'Negative'})\")\n",
    "\n",
    "print(f\"\\nğŸ”€ TRAIN/TEST SPLIT\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Shuffle and split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    X_text, y_text, test_size=0.3, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data split:\")\n",
    "print(f\"   Training samples: {len(X_train_text)}\")\n",
    "print(f\"   Test samples: {len(X_test_text)}\")\n",
    "print(f\"   Train positive ratio: {y_train_text.mean():.2f}\")\n",
    "print(f\"   Test positive ratio: {y_test_text.mean():.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING TEXT CLASSIFICATION MODELS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Simple RNN Text Classifier\n",
    "simple_rnn_text = models.Sequential([\n",
    "    layers.Embedding(len(word_to_idx), 64, input_length=max_length, name='embedding'),\n",
    "    layers.SimpleRNN(32, return_sequences=False, name='simple_rnn'),\n",
    "    layers.Dense(16, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')  # Binary classification\n",
    "], name='Simple_RNN_Text')\n",
    "\n",
    "# 2. LSTM Text Classifier\n",
    "lstm_text = models.Sequential([\n",
    "    layers.Embedding(len(word_to_idx), 64, input_length=max_length, name='embedding'),\n",
    "    layers.LSTM(32, return_sequences=False, name='lstm'),\n",
    "    layers.Dense(16, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')\n",
    "], name='LSTM_Text')\n",
    "\n",
    "# 3. Bidirectional LSTM Text Classifier\n",
    "bi_lstm_text = models.Sequential([\n",
    "    layers.Embedding(len(word_to_idx), 64, input_length=max_length, name='embedding'),\n",
    "    layers.Bidirectional(layers.LSTM(16, return_sequences=False), name='bidirectional_lstm'),\n",
    "    layers.Dense(16, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')\n",
    "], name='Bidirectional_LSTM_Text')\n",
    "\n",
    "# 4. CNN-RNN Hybrid for Text\n",
    "cnn_rnn_text = models.Sequential([\n",
    "    layers.Embedding(len(word_to_idx), 64, input_length=max_length, name='embedding'),\n",
    "    layers.Conv1D(32, 3, activation='relu', name='conv1d'),\n",
    "    layers.MaxPooling1D(2, name='maxpool1d'),\n",
    "    layers.LSTM(16, return_sequences=False, name='lstm'),\n",
    "    layers.Dense(16, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')\n",
    "], name='CNN_RNN_Text')\n",
    "\n",
    "# Compile all text models\n",
    "text_models = [simple_rnn_text, lstm_text, bi_lstm_text, cnn_rnn_text]\n",
    "text_model_names = ['Simple RNN', 'LSTM', 'Bidirectional LSTM', 'CNN-RNN Hybrid']\n",
    "\n",
    "for model in text_models:\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "\n",
    "print(\"âœ… Built 4 text classification models:\")\n",
    "for name in text_model_names:\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING TEXT CLASSIFICATION MODELS\")\n",
    "print(\"-\" * 39)\n",
    "\n",
    "# Train all text models\n",
    "text_histories = []\n",
    "text_predictions = []\n",
    "text_metrics = []\n",
    "\n",
    "for i, (model, name) in enumerate(zip(text_models, text_model_names)):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_text, y_train_text,\n",
    "        batch_size=8,  # Small batch size for small dataset\n",
    "        epochs=20,\n",
    "        validation_data=(X_test_text, y_test_text),\n",
    "        verbose=1\n",
    "    )\n",
    "    text_histories.append(history)\n",
    "    \n",
    "    # Evaluate and predict\n",
    "    test_loss, test_acc, test_prec, test_rec = model.evaluate(X_test_text, y_test_text, verbose=0)\n",
    "    predictions = model.predict(X_test_text, verbose=0)\n",
    "    binary_predictions = (predictions > 0.5).astype(int).flatten()\n",
    "    text_predictions.append(binary_predictions)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    from sklearn.metrics import f1_score, classification_report\n",
    "    f1 = f1_score(y_test_text, binary_predictions)\n",
    "    \n",
    "    text_metrics.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': test_acc,\n",
    "        'Precision': test_prec,\n",
    "        'Recall': test_rec,\n",
    "        'F1-Score': f1,\n",
    "        'Parameters': model.count_params()\n",
    "    })\n",
    "    \n",
    "    print(f\"   âœ… {name} - Accuracy: {test_acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š TEXT CLASSIFICATION RESULTS\")\n",
    "print(\"-\" * 33)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "text_results_df = pd.DataFrame(text_metrics)\n",
    "print(text_results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_text_idx = text_results_df['F1-Score'].idxmax()\n",
    "best_text_model = text_results_df.loc[best_text_idx, 'Model']\n",
    "print(f\"\\nğŸ† Best Text Classifier: {best_text_model}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ PREDICTION EXAMPLES\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Test with some examples\n",
    "test_examples = [\n",
    "    \"This movie is absolutely amazing and wonderful\",\n",
    "    \"I hated this terrible and boring film\",\n",
    "    \"The acting was okay but not great\",\n",
    "    \"One of the best movies ever made\"\n",
    "]\n",
    "\n",
    "print(\"Testing on new examples:\")\n",
    "best_model = text_models[best_text_idx]\n",
    "\n",
    "for example in test_examples:\n",
    "    # Preprocess the example\n",
    "    processed = preprocess_text(example)\n",
    "    sequence = texts_to_sequences([processed], word_to_idx, max_length)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_prob = best_model.predict(sequence, verbose=0)[0][0]\n",
    "    sentiment = \"Positive\" if pred_prob > 0.5 else \"Negative\"\n",
    "    confidence = pred_prob if pred_prob > 0.5 else 1 - pred_prob\n",
    "    \n",
    "    print(f\"   Text: '{example}'\")\n",
    "    print(f\"   Prediction: {sentiment} (confidence: {confidence:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ TEXT PROCESSING VISUALIZATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create text processing visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Training accuracy\n",
    "for i, (history, name) in enumerate(zip(text_histories, text_model_names)):\n",
    "    axes[0, 0].plot(history.history['accuracy'], label=f'{name} (train)', alpha=0.7)\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label=f'{name} (val)', alpha=0.7, linestyle='--')\n",
    "axes[0, 0].set_title('ğŸš€ Training Progress - Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Model performance comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x_pos = np.arange(len(text_model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = text_results_df[metric].values\n",
    "    axes[0, 1].bar(x_pos + i * width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title('ğŸ“Š Model Performance Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_xticks(x_pos + width * 1.5)\n",
    "axes[0, 1].set_xticklabels(text_model_names, rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Confusion matrix for best model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_predictions = text_predictions[best_text_idx]\n",
    "cm = confusion_matrix(y_test_text, best_predictions)\n",
    "\n",
    "im = axes[0, 2].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0, 2].set_title(f'ğŸ¯ {best_text_model} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "tick_marks = np.arange(2)\n",
    "axes[0, 2].set_xticks(tick_marks)\n",
    "axes[0, 2].set_yticks(tick_marks)\n",
    "axes[0, 2].set_xticklabels(['Negative', 'Positive'])\n",
    "axes[0, 2].set_yticklabels(['Negative', 'Positive'])\n",
    "axes[0, 2].set_xlabel('Predicted')\n",
    "axes[0, 2].set_ylabel('Actual')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 2].text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Word frequency\n",
    "word_freq = Counter(word for review in processed_reviews for word in review)\n",
    "top_words = word_freq.most_common(10)\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "axes[1, 0].barh(range(len(words)), counts, color='skyblue', alpha=0.8)\n",
    "axes[1, 0].set_title('ğŸ“ Top 10 Most Frequent Words', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Frequency')\n",
    "axes[1, 0].set_yticks(range(len(words)))\n",
    "axes[1, 0].set_yticklabels(words)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 5. Sequence length distribution\n",
    "seq_lengths = [len([w for w in review if w != '<PAD>']) for review in processed_reviews]\n",
    "axes[1, 1].hist(seq_lengths, bins=15, alpha=0.7, edgecolor='black', color='lightgreen')\n",
    "axes[1, 1].axvline(x=max_length, color='red', linestyle='--', linewidth=2, label=f'Max Length ({max_length})')\n",
    "axes[1, 1].set_title('ğŸ“ Sequence Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Sequence Length')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Model complexity vs performance\n",
    "param_counts_text = text_results_df['Parameters']\n",
    "f1_scores = text_results_df['F1-Score']\n",
    "axes[1, 2].scatter(param_counts_text, f1_scores, s=150, alpha=0.7, \n",
    "                   c=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "for i, model in enumerate(text_model_names):\n",
    "    axes[1, 2].annotate(model, (param_counts_text[i], f1_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 2].set_title('âš–ï¸ Model Complexity vs F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Number of Parameters')\n",
    "axes[1, 2].set_ylabel('F1-Score')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ TEXT PROCESSING INSIGHTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"ğŸ”¸ Embedding layer converts words to dense vector representations\")\n",
    "print(\"ğŸ”¸ RNNs can capture sequential dependencies in text effectively\")\n",
    "print(\"ğŸ”¸ Bidirectional RNNs provide richer context by processing both directions\")\n",
    "print(\"ğŸ”¸ CNN-RNN hybrids can capture local patterns and sequential dependencies\")\n",
    "print(\"ğŸ”¸ Proper preprocessing is crucial for good text classification performance\")\n",
    "\n",
    "print(f\"\\nğŸ¯ TEXT CLASSIFICATION BEST PRACTICES:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ğŸ”¸ Use pre-trained embeddings (Word2Vec, GloVe) for better performance\")\n",
    "print(\"ğŸ”¸ Handle class imbalance with proper sampling or weighted loss\")\n",
    "print(\"ğŸ”¸ Apply regularization (dropout) to prevent overfitting\")\n",
    "print(\"ğŸ”¸ Consider attention mechanisms for longer texts\")\n",
    "print(\"ğŸ”¸ Experiment with different sequence lengths and vocabulary sizes\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FINAL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 31)\n",
    "for _, row in text_results_df.iterrows():\n",
    "    print(f\"   {row['Model']:20}: F1={row['F1-Score']:.4f}, Acc={row['Accuracy']:.4f}, Params={row['Parameters']:,}\")\n",
    "\n",
    "print(f\"\\nâœ… Text processing demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d890ff2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ’¡ 7. Best Practices & Advanced Tips\n",
    "\n",
    "## ğŸ¯ Model Selection Guidelines\n",
    "\n",
    "### ğŸ”„ **When to Use Each Architecture**:\n",
    "\n",
    "#### **Simple RNN** ğŸ”„\n",
    "- âœ… **Use When**: Basic temporal patterns, fast prototyping, limited computational resources\n",
    "- âŒ **Avoid When**: Long sequences (>20-30 steps), complex dependencies\n",
    "- ğŸ¯ **Best For**: Simple time series, basic text classification, educational purposes\n",
    "\n",
    "#### **LSTM** ğŸ§ ğŸ’¾  \n",
    "- âœ… **Use When**: Long-term dependencies, complex patterns, large datasets\n",
    "- âŒ **Avoid When**: Computational constraints, simple patterns\n",
    "- ğŸ¯ **Best For**: Language modeling, machine translation, complex time series\n",
    "\n",
    "#### **GRU** ğŸšª\n",
    "- âœ… **Use When**: Good balance of performance and efficiency needed\n",
    "- âŒ **Avoid When**: Maximum performance is critical regardless of cost\n",
    "- ğŸ¯ **Best For**: Most sequence tasks, resource-constrained environments\n",
    "\n",
    "#### **CNN for Sequences** ğŸ”\n",
    "- âœ… **Use When**: Local patterns are important, parallel processing needed\n",
    "- âŒ **Avoid When**: Long-term dependencies are crucial\n",
    "- ğŸ¯ **Best For**: Text classification, audio processing, fast inference\n",
    "\n",
    "#### **Hybrid CNN-RNN** ğŸ¤\n",
    "- âœ… **Use When**: Both local and global patterns are important\n",
    "- âŒ **Avoid When**: Simplicity is preferred, limited training time\n",
    "- ğŸ¯ **Best For**: Complex sequence tasks, maximum performance\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Performance Optimization\n",
    "\n",
    "### ğŸš€ **Training Speed Optimization**:\n",
    "\n",
    "#### **Data Loading**:\n",
    "```python\n",
    "# Use tf.data for efficient data pipeline\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "#### **Mixed Precision Training**:\n",
    "```python\n",
    "# Enable mixed precision for faster training\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "```\n",
    "\n",
    "#### **GPU Optimization**:\n",
    "```python\n",
    "# Use CuDNN optimized layers\n",
    "layers.LSTM(units, implementation=2)  # CuDNN optimized\n",
    "layers.GRU(units, implementation=2)   # CuDNN optimized\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Memory Optimization**:\n",
    "\n",
    "#### **Gradient Checkpointing**:\n",
    "```python\n",
    "# Trade computation for memory\n",
    "@tf.recompute_grad\n",
    "def lstm_layer(inputs):\n",
    "    return layers.LSTM(units)(inputs)\n",
    "```\n",
    "\n",
    "#### **Sequence Bucketing**:\n",
    "```python\n",
    "# Group sequences by similar length\n",
    "def bucket_sequences(sequences, bucket_boundaries):\n",
    "    # Group sequences into buckets to minimize padding\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Hyperparameter Tuning\n",
    "\n",
    "### ğŸ“Š **Key Hyperparameters**:\n",
    "\n",
    "#### **Architecture Parameters**:\n",
    "- **Hidden Units**: Start with 32-128, increase if underfitting\n",
    "- **Number of Layers**: 1-3 layers, more layers for complex tasks\n",
    "- **Dropout Rate**: 0.1-0.5, higher for overfitting prevention\n",
    "\n",
    "#### **Training Parameters**:\n",
    "- **Learning Rate**: 0.001 (Adam default), adjust based on convergence\n",
    "- **Batch Size**: 32-128, larger for stable gradients\n",
    "- **Sequence Length**: Balance between context and computational cost\n",
    "\n",
    "#### **Regularization**:\n",
    "- **Dropout**: Apply after RNN layers and before final dense layer\n",
    "- **Recurrent Dropout**: Dropout on recurrent connections\n",
    "- **L1/L2 Regularization**: For weight penalty\n",
    "\n",
    "### ğŸ¯ **Hyperparameter Search Strategies**:\n",
    "\n",
    "#### **Grid Search**:\n",
    "```python\n",
    "param_grid = {\n",
    "    'units': [32, 64, 128],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Random Search**:\n",
    "```python\n",
    "# More efficient than grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "```\n",
    "\n",
    "#### **Bayesian Optimization**:\n",
    "```python\n",
    "# Use libraries like Optuna or Hyperopt\n",
    "import optuna\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›¡ï¸ Common Problems & Solutions\n",
    "\n",
    "### âš ï¸ **Vanishing Gradients**:\n",
    "- **Problem**: Gradients become very small in long sequences\n",
    "- **Solutions**: \n",
    "  - Use LSTM/GRU instead of Simple RNN\n",
    "  - Gradient clipping: `clipnorm=1.0`\n",
    "  - Residual connections\n",
    "  - Better initialization\n",
    "\n",
    "### âš ï¸ **Exploding Gradients**:\n",
    "- **Problem**: Gradients become extremely large\n",
    "- **Solutions**:\n",
    "  - Gradient clipping: `clipnorm=1.0` or `clipvalue=0.5`\n",
    "  - Lower learning rate\n",
    "  - Better weight initialization\n",
    "\n",
    "### âš ï¸ **Overfitting**:\n",
    "- **Problem**: Model memorizes training data\n",
    "- **Solutions**:\n",
    "  - Dropout regularization\n",
    "  - Early stopping\n",
    "  - Data augmentation\n",
    "  - Reduce model complexity\n",
    "\n",
    "### âš ï¸ **Slow Convergence**:\n",
    "- **Problem**: Training takes too long or doesn't converge\n",
    "- **Solutions**:\n",
    "  - Learning rate scheduling\n",
    "  - Better optimization (Adam, RMSprop)\n",
    "  - Batch normalization\n",
    "  - Skip connections\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Data Preprocessing Best Practices\n",
    "\n",
    "### ğŸ”§ **Sequence Data**:\n",
    "\n",
    "#### **Normalization**:\n",
    "```python\n",
    "# For time series\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# For text embeddings\n",
    "embeddings = tf.nn.l2_normalize(embeddings, axis=-1)\n",
    "```\n",
    "\n",
    "#### **Handling Variable Lengths**:\n",
    "```python\n",
    "# Padding strategy\n",
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_length, padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "# Masking for padded sequences\n",
    "model.add(layers.Masking(mask_value=0.0))\n",
    "```\n",
    "\n",
    "#### **Data Augmentation**:\n",
    "```python\n",
    "# For text: synonym replacement, back-translation\n",
    "# For sequences: noise injection, time warping\n",
    "def add_noise(sequence, noise_factor=0.01):\n",
    "    return sequence + noise_factor * np.random.randn(*sequence.shape)\n",
    "```\n",
    "\n",
    "### ğŸ“ˆ **Feature Engineering**:\n",
    "\n",
    "#### **Time Series Features**:\n",
    "```python\n",
    "# Lag features\n",
    "df['lag_1'] = df['value'].shift(1)\n",
    "df['lag_7'] = df['value'].shift(7)\n",
    "\n",
    "# Rolling statistics\n",
    "df['rolling_mean'] = df['value'].rolling(window=7).mean()\n",
    "df['rolling_std'] = df['value'].rolling(window=7).std()\n",
    "\n",
    "# Time features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "```\n",
    "\n",
    "#### **Text Features**:\n",
    "```python\n",
    "# N-gram features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Evaluation & Monitoring\n",
    "\n",
    "### ğŸ“Š **Evaluation Metrics**:\n",
    "\n",
    "#### **Regression Tasks**:\n",
    "- **MAE**: Mean Absolute Error\n",
    "- **RMSE**: Root Mean Square Error\n",
    "- **MAPE**: Mean Absolute Percentage Error\n",
    "\n",
    "#### **Classification Tasks**:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision/Recall**: For imbalanced datasets\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Area under ROC curve\n",
    "\n",
    "#### **Sequence Tasks**:\n",
    "- **BLEU**: For translation tasks\n",
    "- **Perplexity**: For language modeling\n",
    "- **Edit Distance**: For sequence alignment\n",
    "\n",
    "### ğŸ“ˆ **Monitoring During Training**:\n",
    "\n",
    "#### **Learning Curves**:\n",
    "```python\n",
    "# Plot training vs validation metrics\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "```\n",
    "\n",
    "#### **Early Stopping**:\n",
    "```python\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Learning Rate Scheduling**:\n",
    "```python\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Advanced Techniques\n",
    "\n",
    "### ğŸ¯ **Attention Mechanisms**:\n",
    "```python\n",
    "# Self-attention layer\n",
    "attention = layers.MultiHeadAttention(\n",
    "    num_heads=8, key_dim=64\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ”„ **Transfer Learning**:\n",
    "```python\n",
    "# Use pre-trained embeddings\n",
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size, embed_dim,\n",
    "    weights=[pretrained_embeddings],\n",
    "    trainable=False\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ¤ **Ensemble Methods**:\n",
    "```python\n",
    "# Combine multiple models\n",
    "def ensemble_predict(models, X):\n",
    "    predictions = [model.predict(X) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Multi-task Learning**:\n",
    "```python\n",
    "# Shared encoder, multiple task-specific heads\n",
    "shared_encoder = layers.LSTM(64, return_sequences=True)\n",
    "task1_head = layers.Dense(num_classes1, activation='softmax')\n",
    "task2_head = layers.Dense(num_classes2, activation='softmax')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f419e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ 8. Chapter Summary & Conclusion\n",
    "\n",
    "## ğŸ“‹ Apa yang Telah Kita Pelajari\n",
    "\n",
    "### ğŸ§  **Konsep Fundamental**:\n",
    "- âœ… **Sequential Data Processing** - Memahami pentingnya urutan dalam data\n",
    "- âœ… **RNN Architecture** - Simple RNN, LSTM, GRU dan karakteristiknya\n",
    "- âœ… **CNN for Sequences** - Penggunaan CNN untuk data sekuensial\n",
    "- âœ… **Hybrid Approaches** - Menggabungkan CNN dan RNN untuk performa optimal\n",
    "\n",
    "### ğŸ”„ **RNN Family Comparison**:\n",
    "\n",
    "| Model | Complexity | Speed | Memory | Long-term Deps | Use Case |\n",
    "|-------|------------|-------|--------|----------------|----------|\n",
    "| **Simple RNN** | Low | Fast | Low | Poor | Basic patterns |\n",
    "| **LSTM** | High | Slow | High | Excellent | Complex sequences |\n",
    "| **GRU** | Medium | Medium | Medium | Good | Balanced approach |\n",
    "| **Bidirectional** | 2x base | Slower | 2x base | Better | Full context |\n",
    "| **Stacked** | High | Slow | High | Best | Very complex |\n",
    "\n",
    "### ğŸ¯ **Aplikasi Praktis yang Dipelajari**:\n",
    "- ğŸ“ˆ **Time Series Forecasting** - Prediksi data temporal\n",
    "- ğŸ“ **Sentiment Analysis** - Klasifikasi teks dengan RNN\n",
    "- ğŸ” **Pattern Recognition** - Deteksi pola dalam sequences\n",
    "- ğŸ¤ **Hybrid Modeling** - Kombinasi CNN-RNN untuk performa optimal\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Key Takeaways\n",
    "\n",
    "### ğŸ’¡ **Architecture Selection**:\n",
    "1. **Simple RNN** ğŸ”„ - Good for learning concepts, poor for real applications\n",
    "2. **LSTM** ğŸ§ ğŸ’¾ - Gold standard for complex sequential modeling\n",
    "3. **GRU** ğŸšª - Best balance of performance and efficiency\n",
    "4. **CNN** ğŸ” - Excellent for local patterns and parallel processing\n",
    "5. **Hybrid** ğŸ¤ - Combines strengths of different architectures\n",
    "\n",
    "### ğŸ¯ **Performance Insights**:\n",
    "- **LSTM** generally outperforms Simple RNN on complex tasks\n",
    "- **GRU** provides similar performance to LSTM with fewer parameters\n",
    "- **CNN** can be surprisingly effective for sequence data\n",
    "- **Hybrid models** often achieve best performance but with added complexity\n",
    "- **Bidirectional** processing improves context understanding\n",
    "\n",
    "### ğŸ”§ **Implementation Best Practices**:\n",
    "1. **Data Preprocessing** - Proper normalization and sequence preparation\n",
    "2. **Regularization** - Dropout and early stopping to prevent overfitting\n",
    "3. **Hyperparameter Tuning** - Systematic approach to finding optimal settings\n",
    "4. **Evaluation Strategy** - Proper train/validation/test splits for sequences\n",
    "5. **Computational Efficiency** - Balance between model complexity and resources\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Advanced Topics untuk Eksplorasi Lebih Lanjut\n",
    "\n",
    "### ğŸŒŸ **Attention Mechanisms**:\n",
    "- **Self-Attention** - Transformer architecture foundation\n",
    "- **Multi-Head Attention** - Parallel attention computation\n",
    "- **Cross-Attention** - Attention between different sequences\n",
    "\n",
    "### ğŸ§  **Modern Architectures**:\n",
    "- **Transformer** - State-of-the-art untuk banyak sequence tasks\n",
    "- **BERT/GPT** - Pre-trained language models\n",
    "- **Vision Transformer** - Transformer untuk computer vision\n",
    "\n",
    "### ğŸ“Š **Specialized Applications**:\n",
    "- **Neural Machine Translation** - Sequence-to-sequence models\n",
    "- **Speech Recognition** - Audio sequence processing\n",
    "- **Video Analysis** - Spatio-temporal sequence modeling\n",
    "- **Graph Neural Networks** - Sequences on graph structures\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Useful Resources & References\n",
    "\n",
    "### ğŸ“š **Essential Papers**:\n",
    "- **LSTM**: Hochreiter & Schmidhuber (1997) - \"Long Short-Term Memory\"\n",
    "- **GRU**: Cho et al. (2014) - \"Learning Phrase Representations using RNN\"\n",
    "- **Attention**: Bahdanau et al. (2014) - \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n",
    "- **Transformer**: Vaswani et al. (2017) - \"Attention Is All You Need\"\n",
    "\n",
    "### ğŸ› ï¸ **Tools & Libraries**:\n",
    "- **TensorFlow/Keras** - Deep learning framework yang kita gunakan\n",
    "- **PyTorch** - Alternative framework dengan dynamic graphs\n",
    "- **Hugging Face** - Pre-trained models dan tokenizers\n",
    "- **spaCy** - Advanced NLP preprocessing\n",
    "\n",
    "### ğŸŒ **Online Resources**:\n",
    "- **TensorFlow Tutorials** - Official sequence modeling guides\n",
    "- **Papers with Code** - Latest research dengan implementasi\n",
    "- **Towards Data Science** - Practical tutorials dan case studies\n",
    "- **fast.ai** - Practical deep learning courses\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Practical Next Steps\n",
    "\n",
    "### ğŸ“ˆ **Project Ideas untuk Practice**:\n",
    "\n",
    "#### **Beginner Level**:\n",
    "1. **Stock Price Prediction** - Simple time series forecasting\n",
    "2. **Movie Review Classification** - Basic sentiment analysis\n",
    "3. **Temperature Forecasting** - Weather data prediction\n",
    "\n",
    "#### **Intermediate Level**:\n",
    "1. **Multi-variate Time Series** - Multiple features forecasting\n",
    "2. **Named Entity Recognition** - Sequence labeling task\n",
    "3. **Anomaly Detection** - Unusual pattern detection in sequences\n",
    "\n",
    "#### **Advanced Level**:\n",
    "1. **Neural Machine Translation** - Sequence-to-sequence modeling\n",
    "2. **Chatbot Development** - Conversational AI\n",
    "3. **Music Generation** - Creative sequence generation\n",
    "\n",
    "### ğŸ”§ **Skills untuk Dikembangkan**:\n",
    "1. **Data Engineering** - Efficient data pipelines untuk sequences\n",
    "2. **Model Deployment** - Real-time sequence processing systems\n",
    "3. **Distributed Training** - Training large models pada multiple GPUs\n",
    "4. **MLOps** - Production ML systems untuk sequence models\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Final Thoughts\n",
    "\n",
    "### ğŸ’­ **Key Messages**:\n",
    "\n",
    "1. **Sequences are Everywhere** ğŸ“Š\n",
    "   - Dari text sampai time series, dari audio sampai video\n",
    "   - Understanding sequence processing opens many opportunities\n",
    "\n",
    "2. **No One-Size-Fits-All** ğŸ¯\n",
    "   - Different problems require different approaches\n",
    "   - Always consider trade-offs: performance vs complexity vs resources\n",
    "\n",
    "3. **Practice Makes Perfect** ğŸš€\n",
    "   - Theoretical knowledge needs practical implementation\n",
    "   - Start dengan simple projects, gradually increase complexity\n",
    "\n",
    "4. **Stay Updated** ğŸ“š\n",
    "   - Field berkembang sangat cepat\n",
    "   - Follow latest research dan best practices\n",
    "\n",
    "5. **Think Beyond Accuracy** âš–ï¸\n",
    "   - Consider interpretability, fairness, dan robustness\n",
    "   - Real-world deployment has many considerations\n",
    "\n",
    "### ğŸ‰ **Congratulations!**\n",
    "\n",
    "Anda telah menyelesaikan comprehensive journey melalui **RNN dan CNN untuk sequence processing**! \n",
    "\n",
    "Dari **Simple RNN** yang basic hingga **hybrid architectures** yang sophisticated, dari **time series forecasting** hingga **sentiment analysis**, Anda sekarang memiliki solid foundation untuk tackling berbagai sequence modeling problems.\n",
    "\n",
    "**Remember**: The best way to master these techniques adalah dengan **practice, experiment, dan iterate**. Start building your own projects, explore different datasets, dan jangan takut untuk try novel approaches!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Chapter 15 Complete!\n",
    "\n",
    "**Selamat!** Anda telah berhasil menguasai:\n",
    "- ğŸ”„ **RNN Fundamentals** - Simple RNN, LSTM, GRU\n",
    "- ğŸ” **CNN for Sequences** - Alternative approach untuk sequence data  \n",
    "- ğŸ“ˆ **Time Series Forecasting** - Practical prediction techniques\n",
    "- ğŸ“ **Text Processing** - NLP dengan RNN\n",
    "- ğŸ’¡ **Best Practices** - Production-ready implementations\n",
    "- ğŸ¤ **Hybrid Approaches** - Combining different architectures\n",
    "\n",
    "**Next Steps**: Apply this knowledge to real-world projects dan explore advanced topics seperti Transformers dan Attention mechanisms!\n",
    "\n",
    "ğŸš€ **Happy Deep Learning!** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
