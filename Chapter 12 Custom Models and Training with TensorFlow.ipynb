{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45c31ea",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with TensorFlow\n",
    "\n",
    "## ğŸ¯ **Pengantar Chapter 12**\n",
    "\n",
    "Chapter ini membahas penggunaan TensorFlow di level yang lebih rendah untuk membuat model dan algoritma training yang disesuaikan. Meskipun tf.keras sudah mencukupi untuk 95% kasus penggunaan, terkadang kita memerlukan kontrol ekstra untuk membuat komponen kustom.\n",
    "\n",
    "### **ğŸ“š Konsep Utama yang Dipelajari:**\n",
    "1. **TensorFlow seperti NumPy** - Operasi tensor dasar\n",
    "2. **Custom Components** - Loss, Metrics, Layers, Models\n",
    "3. **Automatic Differentiation** - GradientTape dan autodiff\n",
    "4. **Custom Training Loops** - Kontrol penuh training process\n",
    "5. **TensorFlow Functions** - Graph optimization dan performance\n",
    "\n",
    "### **ğŸ” Mengapa Chapter ini Penting:**\n",
    "- Memberikan **kontrol penuh** atas training process\n",
    "- Memungkinkan implementasi **algoritma research** terbaru\n",
    "- **Optimisasi performa** untuk kasus spesifik\n",
    "- Memahami **inner workings** TensorFlow dan Keras\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– **Outline Chapter:**\n",
    "- **Bagian 1:** TensorFlow seperti NumPy\n",
    "- **Bagian 2:** Custom Components (Loss, Metrics, Layers, Models)\n",
    "- **Bagian 3:** Automatic Differentiation\n",
    "- **Bagian 4:** Custom Training Loops\n",
    "- **Bagian 5:** TensorFlow Functions dan Graph Optimization\n",
    "- **Bagian 6:** Best Practices dan Ringkasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deaf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ SETUP DAN IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ CHAPTER 12: CUSTOM MODELS AND TRAINING WITH TENSORFLOW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“¦ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")\n",
    "print(f\"ğŸ Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Set random seeds untuk reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU configuration (jika tersedia)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"ğŸ® GPU detected: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"ğŸ’» No GPU detected, using CPU\")\n",
    "\n",
    "print(\"âœ… Setup completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ed90a",
   "metadata": {},
   "source": [
    "# ğŸ”§ Bagian 1: Using TensorFlow like NumPy\n",
    "\n",
    "## ğŸ“– **Penjelasan Teoritis - Tensors dan Operations**\n",
    "\n",
    "**Tensor** adalah struktur data fundamental dalam TensorFlow, mirip dengan ndarray NumPy.\n",
    "\n",
    "### **Tensor memiliki:**\n",
    "- **Shape**: dimensi dari tensor\n",
    "- **Data type (dtype)**: tipe data elemen-elemen tensor\n",
    "- **Values**: nilai-nilai aktual dalam tensor\n",
    "\n",
    "### **ğŸ” Perbedaan Key TensorFlow vs NumPy:**\n",
    "1. **tf.transpose(t)** membuat tensor baru dengan copy data, sedangkan NumPy **t.T** hanya view\n",
    "2. **tf.reduce_sum()** tidak menjamin urutan operasi pada GPU\n",
    "3. TensorFlow dioptimalkan untuk **GPU dan distributed computing**\n",
    "4. NumPy: **64-bit precision** default, TensorFlow: **32-bit** untuk efisiensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¯ 1.1 TENSORS AND OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”¸ 1.1 TENSORS AND OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Membuat tensor\n",
    "print(\"ğŸ“Š Creating tensors:\")\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(f\"Tensor t:\\n{t}\")\n",
    "print(f\"Shape: {t.shape}\")\n",
    "print(f\"Data type: {t.dtype}\")\n",
    "print(f\"Number of dimensions: {t.ndim}\")\n",
    "\n",
    "# Operasi indexing (seperti NumPy)\n",
    "print(\"\\nğŸ” Indexing operations:\")\n",
    "print(f\"t[:, 1:] (columns 1 onwards):\\n{t[:, 1:]}\")\n",
    "print(f\"t[..., 1, tf.newaxis] (column 1 as column vector):\\n{t[..., 1, tf.newaxis]}\")\n",
    "\n",
    "# Operasi matematika\n",
    "print(\"\\nğŸ§® Mathematical operations:\")\n",
    "print(f\"t + 10:\\n{t + 10}\")\n",
    "print(f\"tf.square(t):\\n{tf.square(t)}\")\n",
    "print(f\"t @ tf.transpose(t) (matrix multiplication):\\n{t @ tf.transpose(t)}\")\n",
    "\n",
    "print(\"âœ… Tensor operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6925b2e",
   "metadata": {},
   "source": [
    "## ğŸ”„ **1.2 Tensors and NumPy Interoperability**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Precision Differences**\n",
    "\n",
    "- **NumPy**: 64-bit precision secara default\n",
    "- **TensorFlow**: 32-bit precision untuk efisiensi neural networks\n",
    "- **Alasan**: 32-bit cukup untuk neural networks, lebih cepat, dan hemat RAM\n",
    "\n",
    "### **ğŸ”„ Konversi TensorFlow â†” NumPy:**\n",
    "- **NumPy â†’ TensorFlow**: `tf.constant(numpy_array)`\n",
    "- **TensorFlow â†’ NumPy**: `tensor.numpy()`\n",
    "- **Cross-compatibility**: Operasi TF pada NumPy array dan sebaliknya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c424478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”„ 1.2 TENSORS AND NUMPY INTEROPERABILITY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”¸ 1.2 TENSORS AND NUMPY INTEROPERABILITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Konversi antara TensorFlow tensors dan NumPy arrays\n",
    "print(\"ğŸ”„ Converting between TensorFlow and NumPy:\")\n",
    "a = np.array([2., 4., 5.])\n",
    "print(f\"Original NumPy array: {a} (dtype: {a.dtype})\")\n",
    "\n",
    "# NumPy ke TensorFlow\n",
    "tf_tensor = tf.constant(a)\n",
    "print(f\"Converted to TensorFlow: {tf_tensor} (dtype: {tf_tensor.dtype})\")\n",
    "\n",
    "# TensorFlow ke NumPy\n",
    "numpy_array = t.numpy()\n",
    "print(f\"Tensor back to NumPy:\\n{numpy_array} (dtype: {numpy_array.dtype})\")\n",
    "\n",
    "# Cross-platform operations\n",
    "print(\"\\nğŸ”€ Cross-platform operations:\")\n",
    "print(f\"TensorFlow operation on NumPy array: {tf.square(a)}\")\n",
    "print(f\"NumPy operation on TensorFlow tensor:\\n{np.square(t)}\")\n",
    "\n",
    "# Precision comparison\n",
    "print(\"\\nğŸ“ Precision comparison:\")\n",
    "np_default = np.array([1.0])\n",
    "tf_default = tf.constant([1.0])\n",
    "print(f\"NumPy default precision: {np_default.dtype}\")\n",
    "print(f\"TensorFlow default precision: {tf_default.dtype}\")\n",
    "\n",
    "print(\"âœ… Interoperability demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f61679",
   "metadata": {},
   "source": [
    "## âš™ï¸ **1.3 Type Conversions**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Type Conversions**\n",
    "\n",
    "**TensorFlow tidak melakukan konversi tipe secara otomatis** untuk menghindari penurunan performa yang tidak terdeteksi.\n",
    "\n",
    "### **ğŸ”§ Cara Konversi Tipe:**\n",
    "- **Explicit casting**: `tf.cast(tensor, target_dtype)`\n",
    "- **Consistent dtypes**: Pastikan operand memiliki dtype yang sama\n",
    "- **Performance**: Hindari konversi berulang dalam loop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ebf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# âš™ï¸ 1.3 TYPE CONVERSIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”¸ 1.3 TYPE CONVERSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demonstrasi error tanpa konversi tipe\n",
    "print(\"âŒ Attempting operation without type conversion:\")\n",
    "try:\n",
    "    result = tf.constant(2.) + tf.constant(40)  # float32 + int32\n",
    "    print(\"This shouldn't print - different types\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Konversi tipe yang benar\n",
    "print(\"\\nâœ… Correct type conversion:\")\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "result = tf.constant(2.0) + tf.cast(t2, tf.float32)\n",
    "print(f\"Result with proper casting: {result} (dtype: {result.dtype})\")\n",
    "\n",
    "# Berbagai konversi tipe\n",
    "print(\"\\nğŸ”„ Various type conversions:\")\n",
    "int_tensor = tf.constant([1, 2, 3])\n",
    "float_tensor = tf.cast(int_tensor, tf.float32)\n",
    "bool_tensor = tf.cast(int_tensor, tf.bool)\n",
    "\n",
    "print(f\"Original (int32): {int_tensor}\")\n",
    "print(f\"To float32: {float_tensor}\")\n",
    "print(f\"To bool: {bool_tensor}\")\n",
    "\n",
    "# Type checking\n",
    "print(\"\\nğŸ” Type checking:\")\n",
    "print(f\"Is float32? {float_tensor.dtype == tf.float32}\")\n",
    "print(f\"Is int32? {float_tensor.dtype == tf.int32}\")\n",
    "\n",
    "print(\"âœ… Type conversion demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d343a",
   "metadata": {},
   "source": [
    "## ğŸ”§ **1.4 Variables**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Variables**\n",
    "\n",
    "**tf.Variable** digunakan untuk menyimpan state yang dapat diubah, seperti weights dalam neural networks.\n",
    "\n",
    "### **ğŸ”„ Perbedaan tf.Variable vs tf.Tensor:**\n",
    "- **tf.Tensor**: **Immutable** (tidak dapat diubah)\n",
    "- **tf.Variable**: **Mutable** (dapat dimodifikasi)\n",
    "\n",
    "### **ğŸ› ï¸ Methods untuk Modifikasi:**\n",
    "- **assign()**: mengubah nilai variable\n",
    "- **assign_add()**: menambahkan nilai ke variable\n",
    "- **assign_sub()**: mengurangkan nilai dari variable\n",
    "- **scatter_nd_update()**: update nilai tertentu dalam bentuk scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15629b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ 1.4 VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”¸ 1.4 VARIABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Membuat variable\n",
    "print(\"ğŸ”§ Creating and modifying variables:\")\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]], name=\"my_variable\")\n",
    "print(f\"Initial variable v:\\n{v}\")\n",
    "print(f\"Variable name: {v.name}\")\n",
    "print(f\"Trainable: {v.trainable}\")\n",
    "\n",
    "# Memodifikasi variable dengan assign()\n",
    "print(\"\\nğŸ”„ Modifying with assign():\")\n",
    "v.assign(2 * v)\n",
    "print(f\"After v.assign(2 * v):\\n{v}\")\n",
    "\n",
    "# Modifikasi elemen individual\n",
    "print(\"\\nğŸ¯ Individual element modification:\")\n",
    "v[0, 1].assign(42)\n",
    "print(f\"After v[0, 1].assign(42):\\n{v}\")\n",
    "\n",
    "# Modifikasi slice\n",
    "print(\"\\nâœ‚ï¸ Slice modification:\")\n",
    "v[:, 2].assign([0., 1.])\n",
    "print(f\"After v[:, 2].assign([0., 1.]):\\n{v}\")\n",
    "\n",
    "# Scatter update untuk update multiple indices\n",
    "print(\"\\nğŸ² Scatter update:\")\n",
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
    "print(f\"After scatter_nd_update:\\n{v}\")\n",
    "\n",
    "# Variable operations\n",
    "print(\"\\nâ• Variable operations:\")\n",
    "v.assign_add(tf.ones_like(v))  # Add 1 to all elements\n",
    "print(f\"After assign_add(ones):\\n{v}\")\n",
    "\n",
    "v.assign_sub(tf.constant(0.5))  # Subtract 0.5 from all elements\n",
    "print(f\"After assign_sub(0.5):\\n{v}\")\n",
    "\n",
    "print(\"âœ… Variable operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74854246",
   "metadata": {},
   "source": [
    "# ğŸ› ï¸ Bagian 2: Customizing Models and Training Algorithms\n",
    "\n",
    "## ğŸ“– **Overview Custom Components**\n",
    "\n",
    "Chapter ini akan membahas cara membuat komponen kustom untuk TensorFlow:\n",
    "\n",
    "### **ğŸ¯ Komponen yang Akan Dipelajari:**\n",
    "- **ğŸ­ Custom Loss Functions**: untuk kasus khusus yang tidak tersedia di Keras\n",
    "- **ğŸ“Š Custom Metrics**: untuk evaluasi model dengan kriteria khusus\n",
    "- **ğŸ§± Custom Layers**: building blocks custom untuk arsitektur unik\n",
    "- **ğŸ—ï¸ Custom Models**: arsitektur kompleks dengan control flow khusus\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ­ **2.1 Custom Loss Functions**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Custom Loss Functions**\n",
    "\n",
    "**Loss functions** mengukur seberapa jauh prediksi model dari nilai sebenarnya. Terkadang kita perlu loss function khusus yang tidak tersedia di Keras.\n",
    "\n",
    "### **ğŸ¯ Huber Loss Example:**\n",
    "**Huber Loss** adalah loss function yang robust terhadap outliers:\n",
    "- **Error kecil** (`|error| < threshold`): menggunakan **squared loss**  \n",
    "- **Error besar**: menggunakan **linear loss**\n",
    "\n",
    "### **ğŸ“ Rumus Huber Loss:**\n",
    "```\n",
    "L = {\n",
    "    0.5 * errorÂ²,              jika |error| â‰¤ Î´\n",
    "    Î´ * |error| - 0.5 * Î´Â²,    jika |error| > Î´\n",
    "}\n",
    "```\n",
    "\n",
    "### **âœ… Keuntungan Huber Loss:**\n",
    "1. **Robust** terhadap outliers (tidak explode seperti MSE)\n",
    "2. **Differentiable** di semua titik\n",
    "3. **Cocok** untuk regression dengan data noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb284b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ­ 2.1 CUSTOM LOSS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¸ 2.1 CUSTOM LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple Huber Loss function\n",
    "def huber_fn(y_true, y_pred, threshold=1.0):\n",
    "    \"\"\"\n",
    "    ğŸ¯ Simple Huber loss function implementation\n",
    "    \n",
    "    Args:\n",
    "        y_true: label sebenarnya\n",
    "        y_pred: prediksi model\n",
    "        threshold: batas antara squared dan linear loss\n",
    "    \n",
    "    Returns:\n",
    "        tensor berisi loss untuk setiap instance\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < threshold\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Test simple loss function\n",
    "print(\"ğŸ§ª Testing simple Huber loss function:\")\n",
    "y_true = tf.constant([1., 2., 3., 10.])\n",
    "y_pred = tf.constant([1.5, 1.8, 3.2, 8.0])\n",
    "loss = huber_fn(y_true, y_pred)\n",
    "print(f\"True values: {y_true.numpy()}\")\n",
    "print(f\"Predictions: {y_pred.numpy()}\")\n",
    "print(f\"Huber losses: {loss.numpy()}\")\n",
    "print(f\"Mean loss: {tf.reduce_mean(loss):.4f}\")\n",
    "\n",
    "# Configurable Huber loss using factory pattern\n",
    "def create_huber(threshold=1.0):\n",
    "    \"\"\"\n",
    "    ğŸ­ Factory function untuk membuat Huber loss dengan threshold kustom\n",
    "    \"\"\"\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "print(\"\\nğŸ”§ Testing configurable Huber loss (threshold=2.0):\")\n",
    "huber_2 = create_huber(threshold=2.0)\n",
    "loss_2 = huber_2(y_true, y_pred)\n",
    "print(f\"Huber losses (Î´=2.0): {loss_2.numpy()}\")\n",
    "print(f\"Mean loss (Î´=2.0): {tf.reduce_mean(loss_2):.4f}\")\n",
    "\n",
    "# Professional Huber Loss Class\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    ğŸ­ Professional Huber Loss class untuk production use\n",
    "    \n",
    "    ğŸ“š Keuntungan class approach:\n",
    "    1. Menyimpan hyperparameters dalam model\n",
    "    2. Menggunakan get_config() untuk serialization\n",
    "    3. Kompatibel dengan model saving/loading\n",
    "    4. Mendukung reduction strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=1.0, name=\"huber_loss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"threshold\": self.threshold})\n",
    "        return config\n",
    "\n",
    "# Test HuberLoss class\n",
    "print(\"\\nğŸ—ï¸ Testing HuberLoss class:\")\n",
    "huber_loss = HuberLoss(threshold=1.5, name=\"custom_huber\")\n",
    "loss_class = huber_loss(y_true, y_pred)\n",
    "print(f\"Class-based loss: {loss_class.numpy()}\")\n",
    "print(f\"Mean loss: {tf.reduce_mean(loss_class):.4f}\")\n",
    "print(f\"Loss name: {huber_loss.name}\")\n",
    "print(f\"Configuration: {huber_loss.get_config()}\")\n",
    "\n",
    "# Comparison with MSE\n",
    "print(\"\\nğŸ“Š Comparison with MSE:\")\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "mse_result = mse_loss(y_true, y_pred)\n",
    "print(f\"MSE loss: {mse_result:.4f}\")\n",
    "print(f\"Huber loss: {tf.reduce_mean(loss_class):.4f}\")\n",
    "print(f\"Difference: {mse_result - tf.reduce_mean(loss_class):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Custom Loss Functions demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1267",
   "metadata": {},
   "source": [
    "## ğŸ“Š **2.2 Custom Metrics**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Custom Metrics vs Loss Functions**\n",
    "\n",
    "**Perbedaan Metrics dan Loss functions:**\n",
    "\n",
    "| Aspek | Loss Functions | Metrics |\n",
    "|-------|---------------|---------|\n",
    "| **Tujuan** | Training (backpropagation) | Evaluasi |\n",
    "| **Syarat** | Harus differentiable | Boleh non-differentiable |\n",
    "| **Interpretasi** | Untuk optimizer | Untuk humans |\n",
    "| **Contoh** | MSE, Cross-entropy | Accuracy, F1-score |\n",
    "\n",
    "### **ğŸ”„ Streaming Metrics**\n",
    "**Streaming Metrics** menyimpan state antar batches untuk menghitung metric yang akurat. Diperlukan ketika metric tidak bisa di-average secara sederhana antar batches.\n",
    "\n",
    "**Contoh:** precision, recall, F1-score\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± **2.3 Custom Layers**\n",
    "\n",
    "### **ğŸ“– Penjelasan Teoritis - Custom Layers**\n",
    "\n",
    "**Custom layers berguna untuk:**\n",
    "1. **Implementasi operasi** yang tidak tersedia di Keras\n",
    "2. **Building blocks** yang dapat digunakan ulang\n",
    "3. **Menggabungkan layers** menjadi satu unit\n",
    "4. **Research purposes** dengan operasi experimental\n",
    "\n",
    "### **ğŸ“‹ Jenis Custom Layers:**\n",
    "1. **Stateless**: tanpa weights (gunakan `Lambda` layer)\n",
    "2. **Stateful**: dengan weights (subclass `Layer` class)\n",
    "\n",
    "### **ğŸ”§ Key Methods untuk Custom Layers:**\n",
    "- **`__init__()`**: Initialize layer parameters\n",
    "- **`build()`**: Create weights when input shape is known\n",
    "- **`call()`**: Forward pass computation\n",
    "- **`compute_output_shape()`**: Calculate output shape\n",
    "- **`get_config()`**: Serialization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef16ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 2.2 CUSTOM METRICS & ğŸ§± 2.3 CUSTOM LAYERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¸ 2.2 CUSTOM METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple custom metric (function-based)\n",
    "def huber_metric(y_true, y_pred, threshold=1.0):\n",
    "    \"\"\"ğŸ¯ Simple Huber metric function\"\"\"\n",
    "    return huber_fn(y_true, y_pred, threshold)\n",
    "\n",
    "# Professional Streaming Metric Class\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    ğŸ“Š Professional Streaming Huber metric\n",
    "    \n",
    "    ğŸ”„ Maintains state across batches for accurate computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=1.0, name='huber_metric', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Calculate metric for current batch\n",
    "        metric_values = huber_fn(y_true, y_pred, self.threshold)\n",
    "        \n",
    "        # Update running totals\n",
    "        self.total.assign_add(tf.reduce_sum(metric_values))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'threshold': self.threshold})\n",
    "        return config\n",
    "\n",
    "# Test streaming metric\n",
    "print(\"ğŸ§ª Testing streaming Huber metric:\")\n",
    "huber_metric_obj = HuberMetric(threshold=1.5, name=\"streaming_huber\")\n",
    "\n",
    "# Simulate multiple batches\n",
    "batch1_true = tf.constant([1., 2., 3.])\n",
    "batch1_pred = tf.constant([1.1, 2.2, 2.8])\n",
    "batch2_true = tf.constant([4., 5., 6.])\n",
    "batch2_pred = tf.constant([4.2, 4.8, 6.1])\n",
    "\n",
    "huber_metric_obj.update_state(batch1_true, batch1_pred)\n",
    "print(f\"After batch 1: {huber_metric_obj.result():.4f}\")\n",
    "\n",
    "huber_metric_obj.update_state(batch2_true, batch2_pred)\n",
    "print(f\"After batch 2: {huber_metric_obj.result():.4f}\")\n",
    "\n",
    "huber_metric_obj.reset_state()\n",
    "print(f\"After reset: {huber_metric_obj.result():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¸ 2.3 CUSTOM LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Simple stateless layer (Lambda)\n",
    "print(\"ğŸ”§ 1. Stateless Layer (Lambda):\")\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x), name=\"exponential\")\n",
    "test_input = tf.constant([0., 1., 2.])\n",
    "exp_output = exponential_layer(test_input)\n",
    "print(f\"Input: {test_input.numpy()}\")\n",
    "print(f\"exp(input): {exp_output.numpy()}\")\n",
    "\n",
    "# 2. Professional Custom Dense Layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ğŸ§± Professional Custom Dense Layer Implementation\n",
    "    \n",
    "    ğŸ“š Demonstrates:\n",
    "    - Weight creation in build()\n",
    "    - Forward pass in call()\n",
    "    - Configuration serialization\n",
    "    - Proper initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Create weights when input shape is known\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(self.units,),\n",
    "                initializer='zeros',\n",
    "                trainable=True\n",
    "            )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Forward pass computation\n",
    "        output = tf.matmul(inputs, self.kernel)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output = tf.nn.bias_add(output, self.bias)\n",
    "        \n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(input_shape[:-1].as_list() + [self.units])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': tf.keras.activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Test custom dense layer\n",
    "print(\"\\nğŸ§ª 2. Testing Custom Dense Layer:\")\n",
    "custom_dense = MyDense(units=3, activation='relu', name='my_dense')\n",
    "test_input = tf.random.normal((2, 4))  # batch_size=2, features=4\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "output = custom_dense(test_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\\\n{output.numpy()}\")\n",
    "print(f\"Number of parameters: {custom_dense.count_params()}\")\n",
    "\n",
    "# 3. Advanced Layer with Training-dependent Behavior\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ğŸ­ Advanced Custom Layer dengan training-dependent behavior\n",
    "    \n",
    "    ğŸ”§ Features:\n",
    "    - Different behavior during training vs inference\n",
    "    - Proper handling of training argument\n",
    "    - Regularization during training only\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            # Add noise during training for regularization\n",
    "            noise = tf.random.normal(tf.shape(inputs), \n",
    "                                   mean=0.0, \n",
    "                                   stddev=self.stddev)\n",
    "            return inputs + noise\n",
    "        else:\n",
    "            # No noise during inference\n",
    "            return inputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'stddev': self.stddev})\n",
    "        return config\n",
    "\n",
    "# Test training-dependent layer\n",
    "print(\"\\nğŸ­ 3. Testing Training-dependent Layer:\")\n",
    "noise_layer = MyGaussianNoise(stddev=0.1, name='gaussian_noise')\n",
    "test_input = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(f\"Original input:\\\\n{test_input.numpy()}\")\n",
    "training_output = noise_layer(test_input, training=True)\n",
    "print(f\"With noise (training=True):\\\\n{training_output.numpy()}\")\n",
    "inference_output = noise_layer(test_input, training=False)\n",
    "print(f\"Without noise (training=False):\\\\n{inference_output.numpy()}\")\n",
    "\n",
    "print(\"\\nâœ… Custom Metrics and Layers demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2c6ec",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ Bagian 3: Advanced Custom Components\n",
    "\n",
    "## ğŸ—ï¸ **3.1 Custom Models dan Autodiff**\n",
    "\n",
    "### **ğŸ“– Custom Models - Kapan Dibutuhkan:**\n",
    "- **Arsitektur kompleks** dengan skip connections\n",
    "- **Multiple inputs/outputs** \n",
    "- **Dynamic behavior** berdasarkan input\n",
    "- **Research purposes** dengan arsitektur experimental\n",
    "\n",
    "### **ğŸ”¬ Automatic Differentiation (Autodiff):**\n",
    "- **Forward pass**: menghitung output dan menyimpan intermediate values\n",
    "- **Reverse pass**: menghitung gradients dengan chain rule\n",
    "- **GradientTape**: merekam operasi untuk gradient computation\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ Bagian 4: Practical Implementation\n",
    "\n",
    "## ğŸš€ **4.1 Putting It All Together**\n",
    "\n",
    "Bagian ini akan mendemonstrasikan penggunaan semua komponen custom dalam satu workflow lengkap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ—ï¸ 3.1 CUSTOM MODELS & ğŸ”¬ AUTODIFF\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¸ 3.1 CUSTOM MODELS & AUTODIFF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Advanced Custom Model with Residual Connections\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ğŸ”— Residual Block with skip connections\n",
    "    \n",
    "    ğŸ“š Theory: output = input + F(input)\n",
    "    âœ… Helps with vanishing gradient problem\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(\n",
    "                    n_neurons, \n",
    "                    activation='elu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    name=f'hidden_{i}'\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z  # Skip connection\n",
    "\n",
    "class CustomResNet(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ğŸ—ï¸ Custom ResNet-style model\n",
    "    \n",
    "    ğŸ¯ Demonstrates:\n",
    "    - Custom model architecture\n",
    "    - Residual connections\n",
    "    - Multiple custom layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation='elu',\n",
    "                                           kernel_initializer='he_normal')\n",
    "        self.block1 = ResidualBlock(2, 30, name='block1')\n",
    "        self.block2 = ResidualBlock(2, 30, name='block2')\n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, name='output')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.output_layer(Z)\n",
    "\n",
    "# Test custom model\n",
    "print(\"ğŸ—ï¸ Testing Custom ResNet model:\")\n",
    "model = CustomResNet(output_dim=1, name='custom_resnet')\n",
    "test_input = tf.random.normal((5, 8))  # 5 samples, 8 features\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "output = model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model summary:\")\n",
    "model.build(input_shape=(None, 8))\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Autodiff demonstration\n",
    "print(\"\\nğŸ”¬ Autodiff Demonstration:\")\n",
    "\n",
    "def simple_function(w1, w2):\n",
    "    \"\"\"Simple function for gradient demo\"\"\"\n",
    "    return 3 * w1**2 + 2 * w1 * w2\n",
    "\n",
    "# Manual gradient (inefficient)\n",
    "w1, w2 = 5.0, 3.0\n",
    "eps = 1e-6\n",
    "manual_dw1 = (simple_function(w1 + eps, w2) - simple_function(w1, w2)) / eps\n",
    "manual_dw2 = (simple_function(w1, w2 + eps) - simple_function(w1, w2)) / eps\n",
    "\n",
    "print(f\"Manual gradients: dw1={manual_dw1:.6f}, dw2={manual_dw2:.6f}\")\n",
    "\n",
    "# Autodiff with GradientTape\n",
    "w1, w2 = tf.Variable(5.0), tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    result = simple_function(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(result, [w1, w2])\n",
    "print(f\"Autodiff gradients: dw1={gradients[0]:.6f}, dw2={gradients[1]:.6f}\")\n",
    "\n",
    "# Advanced: Custom gradient for numerical stability\n",
    "@tf.custom_gradient\n",
    "def safe_softplus(z):\n",
    "    \"\"\"Numerically stable softplus with custom gradient\"\"\"\n",
    "    exp_z = tf.exp(z)\n",
    "    def grad(dy):\n",
    "        return dy / (1 + 1 / exp_z)\n",
    "    return tf.math.log(exp_z + 1), grad\n",
    "\n",
    "print(\"\\nğŸ›¡ï¸ Testing custom gradient:\")\n",
    "x = tf.Variable([100.0])  # Large value that might cause issues\n",
    "with tf.GradientTape() as tape:\n",
    "    y = safe_softplus(x)\n",
    "grad = tape.gradient(y, [x])\n",
    "print(f\"Safe softplus gradient at x=100: {grad[0]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”¸ 4.1 COMPLETE WORKFLOW DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Complete workflow using all custom components\n",
    "print(\"ğŸš€ Complete Custom TensorFlow Workflow:\")\n",
    "\n",
    "# 1. Prepare data\n",
    "print(\"\\nğŸ“Š 1. Data Preparation:\")\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32 for TensorFlow\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32).reshape(-1, 1)\n",
    "y_test = y_test.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# 2. Build model with custom components\n",
    "print(\"\\nğŸ—ï¸ 2. Building Model with Custom Components:\")\n",
    "model = tf.keras.Sequential([\n",
    "    MyDense(32, activation='relu', name='custom_dense1'),\n",
    "    MyGaussianNoise(0.1, name='custom_noise'),\n",
    "    MyDense(16, activation='relu', name='custom_dense2'),\n",
    "    tf.keras.layers.Dense(1, name='output')\n",
    "], name='custom_model')\n",
    "\n",
    "# 3. Compile with custom loss and metrics\n",
    "print(\"\\nâš™ï¸ 3. Compiling with Custom Components:\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=HuberLoss(threshold=1.0),\n",
    "    metrics=[HuberMetric(threshold=1.0)]\n",
    ")\n",
    "\n",
    "# 4. Train model\n",
    "print(\"\\nğŸ¯ 4. Training Model:\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"\\nğŸ“ˆ 5. Model Evaluation:\")\n",
    "test_loss, test_metric = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss (Huber): {test_loss:.4f}\")\n",
    "print(f\"Test Metric (Huber): {test_metric:.4f}\")\n",
    "\n",
    "# 6. Predictions\n",
    "print(\"\\nğŸ”® 6. Making Predictions:\")\n",
    "predictions = model.predict(X_test[:5])\n",
    "print(f\"Sample predictions: {predictions.flatten()}\")\n",
    "print(f\"Actual values: {y_test[:5].flatten()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ COMPLETE WORKFLOW SUCCESSFULLY EXECUTED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c05f2",
   "metadata": {},
   "source": [
    "# ğŸ“‹ Bagian 5: Summary dan Best Practices\n",
    "\n",
    "## ğŸ¯ **Ringkasan Chapter 12**\n",
    "\n",
    "### **âœ… Apa yang Telah Dipelajari:**\n",
    "\n",
    "| Komponen | Fungsi | Kapan Digunakan |\n",
    "|----------|--------|----------------|\n",
    "| **ğŸ­ Custom Loss** | Fungsi objektif khusus | Loss standar tidak sesuai |\n",
    "| **ğŸ“Š Custom Metrics** | Evaluasi khusus | Metrik standar tidak cukup |\n",
    "| **ğŸ§± Custom Layers** | Operasi layer khusus | Operasi tidak tersedia di Keras |\n",
    "| **ğŸ—ï¸ Custom Models** | Arsitektur kompleks | Arsitektur non-standard |\n",
    "| **ğŸ”¬ Autodiff** | Gradient computation | Custom training loops |\n",
    "\n",
    "### **ğŸ“Š Decision Tree: Kapan Menggunakan Custom Components?**\n",
    "\n",
    "```\n",
    "95% kasus: tf.keras sudah cukup âœ…\n",
    "    â†“\n",
    "5% kasus yang memerlukan custom:\n",
    "    â”œâ”€â”€ Research dengan algoritma baru ğŸ”¬\n",
    "    â”œâ”€â”€ Arsitektur yang sangat khusus ğŸ—ï¸\n",
    "    â”œâ”€â”€ Performance optimization ekstrem âš¡\n",
    "    â””â”€â”€ Integration dengan sistem khusus ğŸ”§\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **Best Practices dan Guidelines**\n",
    "\n",
    "### **1. ğŸ Getting Started:**\n",
    "- âœ… **Mulai dengan tf.keras** standard components\n",
    "- âœ… **Custom hanya jika benar-benar perlu**\n",
    "- âœ… **Test thoroughly** sebelum production\n",
    "\n",
    "### **2. ğŸ”§ Development Practices:**\n",
    "- âœ… **Follow existing patterns** dalam TensorFlow/Keras\n",
    "- âœ… **Implement `get_config()`** untuk serialization\n",
    "- âœ… **Use type hints** dan comprehensive docstrings\n",
    "- âœ… **Add proper error handling**\n",
    "\n",
    "### **3. ğŸ“š Code Quality:**\n",
    "- âœ… **Clear naming conventions**\n",
    "- âœ… **Comprehensive documentation**\n",
    "- âœ… **Unit tests** untuk setiap component\n",
    "- âœ… **Performance profiling** jika diperlukan\n",
    "\n",
    "### **4. ğŸš€ Performance:**\n",
    "- âœ… **Use TF Functions** untuk graph optimization\n",
    "- âœ… **Batch operations** instead of loops\n",
    "- âœ… **Proper data types** (float32 vs float64)\n",
    "- âœ… **GPU-friendly operations**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ **Next Steps**\n",
    "\n",
    "### **ğŸ“ˆ Untuk Further Learning:**\n",
    "1. **TensorFlow Probability** - Advanced probabilistic models\n",
    "2. **TensorFlow Serving** - Model deployment\n",
    "3. **TensorFlow Lite** - Mobile deployment\n",
    "4. **TensorFlow.js** - Web deployment\n",
    "5. **TensorFlow Extended (TFX)** - Production ML pipelines\n",
    "\n",
    "### **ğŸ’¡ Project Ideas:**\n",
    "- Implement paper algorithms with custom components\n",
    "- Build domain-specific layers untuk aplikasi khusus\n",
    "- Create reusable component library\n",
    "- Optimize existing models dengan custom training loops\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ **Kesimpulan**\n",
    "\n",
    "**Chapter 12** memberikan foundation yang solid untuk:\n",
    "- **ğŸ”§ Low-level TensorFlow development**\n",
    "- **ğŸ¯ Custom component creation**\n",
    "- **ğŸš€ Advanced model architectures**\n",
    "- **ğŸ“Š Production-ready implementations**\n",
    "\n",
    "**Dengan knowledge ini, Anda siap untuk:**\n",
    "- **Research dan development** model advanced\n",
    "- **Production deployment** dengan custom requirements\n",
    "- **Performance optimization** untuk kasus spesifik\n",
    "- **Integration** dengan existing systems\n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ‰ Selamat! Anda telah menguasai Custom Models and Training with TensorFlow!**\n",
    "\n",
    "**Next:** Ready untuk tackle real-world custom TensorFlow projects! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
