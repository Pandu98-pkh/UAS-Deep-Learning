{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pandu98-pkh/UAS-Deep-Learning/blob/main/Chapter%2012%20Custom%20Models%20and%20Training%20with%20TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45c31ea",
      "metadata": {
        "id": "c45c31ea"
      },
      "source": [
        "# Chapter 12: Custom Models and Training with TensorFlow\n",
        "\n",
        "## 🎯 **Pengantar Chapter 12**\n",
        "\n",
        "Chapter ini membahas penggunaan TensorFlow di level yang lebih rendah untuk membuat model dan algoritma training yang disesuaikan. Meskipun tf.keras sudah mencukupi untuk 95% kasus penggunaan, terkadang kita memerlukan kontrol ekstra untuk membuat komponen kustom.\n",
        "\n",
        "### **📚 Konsep Utama yang Dipelajari:**\n",
        "1. **TensorFlow seperti NumPy** - Operasi tensor dasar\n",
        "2. **Custom Components** - Loss, Metrics, Layers, Models\n",
        "3. **Automatic Differentiation** - GradientTape dan autodiff\n",
        "4. **Custom Training Loops** - Kontrol penuh training process\n",
        "5. **TensorFlow Functions** - Graph optimization dan performance\n",
        "\n",
        "### **🔍 Mengapa Chapter ini Penting:**\n",
        "- Memberikan **kontrol penuh** atas training process\n",
        "- Memungkinkan implementasi **algoritma research** terbaru\n",
        "- **Optimisasi performa** untuk kasus spesifik\n",
        "- Memahami **inner workings** TensorFlow dan Keras\n",
        "\n",
        "---\n",
        "\n",
        "## 📖 **Outline Chapter:**\n",
        "- **Bagian 1:** TensorFlow seperti NumPy\n",
        "- **Bagian 2:** Custom Components (Loss, Metrics, Layers, Models)\n",
        "- **Bagian 3:** Automatic Differentiation\n",
        "- **Bagian 4:** Custom Training Loops\n",
        "- **Bagian 5:** TensorFlow Functions dan Graph Optimization\n",
        "- **Bagian 6:** Best Practices dan Ringkasan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8deaf4b3",
      "metadata": {
        "id": "8deaf4b3",
        "outputId": "7f0b1fd6-cf23-443b-d322-c5e7a555fc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "🚀 CHAPTER 12: CUSTOM MODELS AND TRAINING WITH TENSORFLOW\n",
            "============================================================\n",
            "📦 TensorFlow version: 2.18.0\n",
            "📦 NumPy version: 2.0.2\n",
            "🐍 Python version: 3.11.13\n",
            "🎮 GPU detected: 1 device(s)\n",
            "✅ Setup completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🔧 SETUP DAN IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"🚀 CHAPTER 12: CUSTOM MODELS AND TRAINING WITH TENSORFLOW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📦 TensorFlow version: {tf.__version__}\")\n",
        "print(f\"📦 NumPy version: {np.__version__}\")\n",
        "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "# Set random seeds untuk reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# GPU configuration (jika tersedia)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"🎮 GPU detected: {len(gpus)} device(s)\")\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"💻 No GPU detected, using CPU\")\n",
        "\n",
        "print(\"✅ Setup completed successfully!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579ed90a",
      "metadata": {
        "id": "579ed90a"
      },
      "source": [
        "# 🔧 Bagian 1: Using TensorFlow like NumPy\n",
        "\n",
        "## 📖 **Penjelasan Teoritis - Tensors dan Operations**\n",
        "\n",
        "**Tensor** adalah struktur data fundamental dalam TensorFlow, mirip dengan ndarray NumPy.\n",
        "\n",
        "### **Tensor memiliki:**\n",
        "- **Shape**: dimensi dari tensor\n",
        "- **Data type (dtype)**: tipe data elemen-elemen tensor\n",
        "- **Values**: nilai-nilai aktual dalam tensor\n",
        "\n",
        "### **🔍 Perbedaan Key TensorFlow vs NumPy:**\n",
        "1. **tf.transpose(t)** membuat tensor baru dengan copy data, sedangkan NumPy **t.T** hanya view\n",
        "2. **tf.reduce_sum()** tidak menjamin urutan operasi pada GPU\n",
        "3. TensorFlow dioptimalkan untuk **GPU dan distributed computing**\n",
        "4. NumPy: **64-bit precision** default, TensorFlow: **32-bit** untuk efisiensi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1ed5a8ca",
      "metadata": {
        "id": "1ed5a8ca",
        "outputId": "663628bf-7363-4f2a-90f9-92ac87eb41d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🔸 1.1 TENSORS AND OPERATIONS\n",
            "==================================================\n",
            "📊 Creating tensors:\n",
            "Tensor t:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Shape: (2, 3)\n",
            "Data type: <dtype: 'float32'>\n",
            "Number of dimensions: 2\n",
            "\n",
            "🔍 Indexing operations:\n",
            "t[:, 1:] (columns 1 onwards):\n",
            "[[2. 3.]\n",
            " [5. 6.]]\n",
            "t[..., 1, tf.newaxis] (column 1 as column vector):\n",
            "[[2.]\n",
            " [5.]]\n",
            "\n",
            "🧮 Mathematical operations:\n",
            "t + 10:\n",
            "[[11. 12. 13.]\n",
            " [14. 15. 16.]]\n",
            "tf.square(t):\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "t @ tf.transpose(t) (matrix multiplication):\n",
            "[[14. 32.]\n",
            " [32. 77.]]\n",
            "✅ Tensor operations completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🎯 1.1 TENSORS AND OPERATIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"🔸 1.1 TENSORS AND OPERATIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Membuat tensor\n",
        "print(\"📊 Creating tensors:\")\n",
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(f\"Tensor t:\\n{t}\")\n",
        "print(f\"Shape: {t.shape}\")\n",
        "print(f\"Data type: {t.dtype}\")\n",
        "print(f\"Number of dimensions: {t.ndim}\")\n",
        "\n",
        "# Operasi indexing (seperti NumPy)\n",
        "print(\"\\n🔍 Indexing operations:\")\n",
        "print(f\"t[:, 1:] (columns 1 onwards):\\n{t[:, 1:]}\")\n",
        "print(f\"t[..., 1, tf.newaxis] (column 1 as column vector):\\n{t[..., 1, tf.newaxis]}\")\n",
        "\n",
        "# Operasi matematika\n",
        "print(\"\\n🧮 Mathematical operations:\")\n",
        "print(f\"t + 10:\\n{t + 10}\")\n",
        "print(f\"tf.square(t):\\n{tf.square(t)}\")\n",
        "print(f\"t @ tf.transpose(t) (matrix multiplication):\\n{t @ tf.transpose(t)}\")\n",
        "\n",
        "print(\"✅ Tensor operations completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6925b2e",
      "metadata": {
        "id": "b6925b2e"
      },
      "source": [
        "## 🔄 **1.2 Tensors and NumPy Interoperability**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Precision Differences**\n",
        "\n",
        "- **NumPy**: 64-bit precision secara default\n",
        "- **TensorFlow**: 32-bit precision untuk efisiensi neural networks\n",
        "- **Alasan**: 32-bit cukup untuk neural networks, lebih cepat, dan hemat RAM\n",
        "\n",
        "### **🔄 Konversi TensorFlow ↔ NumPy:**\n",
        "- **NumPy → TensorFlow**: `tf.constant(numpy_array)`\n",
        "- **TensorFlow → NumPy**: `tensor.numpy()`\n",
        "- **Cross-compatibility**: Operasi TF pada NumPy array dan sebaliknya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7c424478",
      "metadata": {
        "id": "7c424478",
        "outputId": "4fff0408-2f30-4631-b168-ae04d589676f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🔸 1.2 TENSORS AND NUMPY INTEROPERABILITY\n",
            "==================================================\n",
            "🔄 Converting between TensorFlow and NumPy:\n",
            "Original NumPy array: [2. 4. 5.] (dtype: float64)\n",
            "Converted to TensorFlow: [2. 4. 5.] (dtype: <dtype: 'float64'>)\n",
            "Tensor back to NumPy:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]] (dtype: float32)\n",
            "\n",
            "🔀 Cross-platform operations:\n",
            "TensorFlow operation on NumPy array: [ 4. 16. 25.]\n",
            "NumPy operation on TensorFlow tensor:\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "\n",
            "📏 Precision comparison:\n",
            "NumPy default precision: float64\n",
            "TensorFlow default precision: <dtype: 'float32'>\n",
            "✅ Interoperability demonstration completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🔄 1.2 TENSORS AND NUMPY INTEROPERABILITY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"🔸 1.2 TENSORS AND NUMPY INTEROPERABILITY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Konversi antara TensorFlow tensors dan NumPy arrays\n",
        "print(\"🔄 Converting between TensorFlow and NumPy:\")\n",
        "a = np.array([2., 4., 5.])\n",
        "print(f\"Original NumPy array: {a} (dtype: {a.dtype})\")\n",
        "\n",
        "# NumPy ke TensorFlow\n",
        "tf_tensor = tf.constant(a)\n",
        "print(f\"Converted to TensorFlow: {tf_tensor} (dtype: {tf_tensor.dtype})\")\n",
        "\n",
        "# TensorFlow ke NumPy\n",
        "numpy_array = t.numpy()\n",
        "print(f\"Tensor back to NumPy:\\n{numpy_array} (dtype: {numpy_array.dtype})\")\n",
        "\n",
        "# Cross-platform operations\n",
        "print(\"\\n🔀 Cross-platform operations:\")\n",
        "print(f\"TensorFlow operation on NumPy array: {tf.square(a)}\")\n",
        "print(f\"NumPy operation on TensorFlow tensor:\\n{np.square(t)}\")\n",
        "\n",
        "# Precision comparison\n",
        "print(\"\\n📏 Precision comparison:\")\n",
        "np_default = np.array([1.0])\n",
        "tf_default = tf.constant([1.0])\n",
        "print(f\"NumPy default precision: {np_default.dtype}\")\n",
        "print(f\"TensorFlow default precision: {tf_default.dtype}\")\n",
        "\n",
        "print(\"✅ Interoperability demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f61679",
      "metadata": {
        "id": "c0f61679"
      },
      "source": [
        "## ⚙️ **1.3 Type Conversions**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Type Conversions**\n",
        "\n",
        "**TensorFlow tidak melakukan konversi tipe secara otomatis** untuk menghindari penurunan performa yang tidak terdeteksi.\n",
        "\n",
        "### **🔧 Cara Konversi Tipe:**\n",
        "- **Explicit casting**: `tf.cast(tensor, target_dtype)`\n",
        "- **Consistent dtypes**: Pastikan operand memiliki dtype yang sama\n",
        "- **Performance**: Hindari konversi berulang dalam loop training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "329ebf24",
      "metadata": {
        "id": "329ebf24",
        "outputId": "c49965f6-16fd-48e5-e488-31e55f489ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🔸 1.3 TYPE CONVERSIONS\n",
            "==================================================\n",
            "❌ Attempting operation without type conversion:\n",
            "Error: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: \n",
            "\n",
            "✅ Correct type conversion:\n",
            "Result with proper casting: 42.0 (dtype: <dtype: 'float32'>)\n",
            "\n",
            "🔄 Various type conversions:\n",
            "Original (int32): [1 2 3]\n",
            "To float32: [1. 2. 3.]\n",
            "To bool: [ True  True  True]\n",
            "\n",
            "🔍 Type checking:\n",
            "Is float32? True\n",
            "Is int32? False\n",
            "✅ Type conversion demonstration completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ⚙️ 1.3 TYPE CONVERSIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"🔸 1.3 TYPE CONVERSIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Demonstrasi error tanpa konversi tipe\n",
        "print(\"❌ Attempting operation without type conversion:\")\n",
        "try:\n",
        "    result = tf.constant(2.) + tf.constant(40)  # float32 + int32\n",
        "    print(\"This shouldn't print - different types\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Konversi tipe yang benar\n",
        "print(\"\\n✅ Correct type conversion:\")\n",
        "t2 = tf.constant(40., dtype=tf.float64)\n",
        "result = tf.constant(2.0) + tf.cast(t2, tf.float32)\n",
        "print(f\"Result with proper casting: {result} (dtype: {result.dtype})\")\n",
        "\n",
        "# Berbagai konversi tipe\n",
        "print(\"\\n🔄 Various type conversions:\")\n",
        "int_tensor = tf.constant([1, 2, 3])\n",
        "float_tensor = tf.cast(int_tensor, tf.float32)\n",
        "bool_tensor = tf.cast(int_tensor, tf.bool)\n",
        "\n",
        "print(f\"Original (int32): {int_tensor}\")\n",
        "print(f\"To float32: {float_tensor}\")\n",
        "print(f\"To bool: {bool_tensor}\")\n",
        "\n",
        "# Type checking\n",
        "print(\"\\n🔍 Type checking:\")\n",
        "print(f\"Is float32? {float_tensor.dtype == tf.float32}\")\n",
        "print(f\"Is int32? {float_tensor.dtype == tf.int32}\")\n",
        "\n",
        "print(\"✅ Type conversion demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0d343a",
      "metadata": {
        "id": "9c0d343a"
      },
      "source": [
        "## 🔧 **1.4 Variables**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Variables**\n",
        "\n",
        "**tf.Variable** digunakan untuk menyimpan state yang dapat diubah, seperti weights dalam neural networks.\n",
        "\n",
        "### **🔄 Perbedaan tf.Variable vs tf.Tensor:**\n",
        "- **tf.Tensor**: **Immutable** (tidak dapat diubah)\n",
        "- **tf.Variable**: **Mutable** (dapat dimodifikasi)\n",
        "\n",
        "### **🛠️ Methods untuk Modifikasi:**\n",
        "- **assign()**: mengubah nilai variable\n",
        "- **assign_add()**: menambahkan nilai ke variable\n",
        "- **assign_sub()**: mengurangkan nilai dari variable\n",
        "- **scatter_nd_update()**: update nilai tertentu dalam bentuk scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "15629b36",
      "metadata": {
        "id": "15629b36",
        "outputId": "c227322a-2f1c-43a8-a5ae-6528a97b711e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🔸 1.4 VARIABLES\n",
            "==================================================\n",
            "🔧 Creating and modifying variables:\n",
            "Initial variable v:\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[1., 2., 3.],\n",
            "       [4., 5., 6.]], dtype=float32)>\n",
            "Variable name: my_variable:0\n",
            "Trainable: True\n",
            "\n",
            "🔄 Modifying with assign():\n",
            "After v.assign(2 * v):\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2.,  4.,  6.],\n",
            "       [ 8., 10., 12.]], dtype=float32)>\n",
            "\n",
            "🎯 Individual element modification:\n",
            "After v[0, 1].assign(42):\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2., 42.,  6.],\n",
            "       [ 8., 10., 12.]], dtype=float32)>\n",
            "\n",
            "✂️ Slice modification:\n",
            "After v[:, 2].assign([0., 1.]):\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 2., 42.,  0.],\n",
            "       [ 8., 10.,  1.]], dtype=float32)>\n",
            "\n",
            "🎲 Scatter update:\n",
            "After scatter_nd_update:\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[100.,  42.,   0.],\n",
            "       [  8.,  10., 200.]], dtype=float32)>\n",
            "\n",
            "➕ Variable operations:\n",
            "After assign_add(ones):\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[101.,  43.,   1.],\n",
            "       [  9.,  11., 201.]], dtype=float32)>\n",
            "After assign_sub(ones * 0.5):\n",
            "<tf.Variable 'my_variable:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[100.5,  42.5,   0.5],\n",
            "       [  8.5,  10.5, 200.5]], dtype=float32)>\n",
            "✅ Variable operations completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🔧 1.4 VARIABLES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"🔸 1.4 VARIABLES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Membuat variable\n",
        "print(\"🔧 Creating and modifying variables:\")\n",
        "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]], name=\"my_variable\")\n",
        "print(f\"Initial variable v:\\n{v}\")\n",
        "print(f\"Variable name: {v.name}\")\n",
        "print(f\"Trainable: {v.trainable}\")\n",
        "\n",
        "# Memodifikasi variable dengan assign()\n",
        "print(\"\\n🔄 Modifying with assign():\")\n",
        "v.assign(2 * v)\n",
        "print(f\"After v.assign(2 * v):\\n{v}\")\n",
        "\n",
        "# Modifikasi elemen individual\n",
        "print(\"\\n🎯 Individual element modification:\")\n",
        "v[0, 1].assign(42)\n",
        "print(f\"After v[0, 1].assign(42):\\n{v}\")\n",
        "\n",
        "# Modifikasi slice\n",
        "print(\"\\n✂️ Slice modification:\")\n",
        "v[:, 2].assign([0., 1.])\n",
        "print(f\"After v[:, 2].assign([0., 1.]):\\n{v}\")\n",
        "\n",
        "# Scatter update untuk update multiple indices\n",
        "print(\"\\n🎲 Scatter update:\")\n",
        "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
        "print(f\"After scatter_nd_update:\\n{v}\")\n",
        "\n",
        "# Variable operations\n",
        "print(\"\\n➕ Variable operations:\")\n",
        "v.assign_add(tf.ones_like(v))  # Add 1 to all elements\n",
        "print(f\"After assign_add(ones):\\n{v}\")\n",
        "\n",
        "# To subtract 0.5 from all elements:\n",
        "# Option 1: Using tf.ones_like() and multiplying\n",
        "v.assign_sub(tf.ones_like(v) * 0.5)\n",
        "print(f\"After assign_sub(ones * 0.5):\\n{v}\")\n",
        "\n",
        "# Option 2: Using tf.fill() (alternative)\n",
        "# v.assign_sub(tf.fill(v.shape, 0.5))\n",
        "# print(f\"After assign_sub(tf.fill):\\n{v}\")\n",
        "\n",
        "print(\"✅ Variable operations completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74854246",
      "metadata": {
        "id": "74854246"
      },
      "source": [
        "# 🛠️ Bagian 2: Customizing Models and Training Algorithms\n",
        "\n",
        "## 📖 **Overview Custom Components**\n",
        "\n",
        "Chapter ini akan membahas cara membuat komponen kustom untuk TensorFlow:\n",
        "\n",
        "### **🎯 Komponen yang Akan Dipelajari:**\n",
        "- **🎭 Custom Loss Functions**: untuk kasus khusus yang tidak tersedia di Keras\n",
        "- **📊 Custom Metrics**: untuk evaluasi model dengan kriteria khusus\n",
        "- **🧱 Custom Layers**: building blocks custom untuk arsitektur unik\n",
        "- **🏗️ Custom Models**: arsitektur kompleks dengan control flow khusus\n",
        "\n",
        "---\n",
        "\n",
        "## 🎭 **2.1 Custom Loss Functions**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Custom Loss Functions**\n",
        "\n",
        "**Loss functions** mengukur seberapa jauh prediksi model dari nilai sebenarnya. Terkadang kita perlu loss function khusus yang tidak tersedia di Keras.\n",
        "\n",
        "### **🎯 Huber Loss Example:**\n",
        "**Huber Loss** adalah loss function yang robust terhadap outliers:\n",
        "- **Error kecil** (`|error| < threshold`): menggunakan **squared loss**  \n",
        "- **Error besar**: menggunakan **linear loss**\n",
        "\n",
        "### **📐 Rumus Huber Loss:**\n",
        "```\n",
        "L = {\n",
        "    0.5 * error²,              jika |error| ≤ δ\n",
        "    δ * |error| - 0.5 * δ²,    jika |error| > δ\n",
        "}\n",
        "```\n",
        "\n",
        "### **✅ Keuntungan Huber Loss:**\n",
        "1. **Robust** terhadap outliers (tidak explode seperti MSE)\n",
        "2. **Differentiable** di semua titik\n",
        "3. **Cocok** untuk regression dengan data noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "eb284b81",
      "metadata": {
        "id": "eb284b81",
        "outputId": "b78e1882-ab46-4196-a5bc-3e251b0eeec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🔸 2.1 CUSTOM LOSS FUNCTIONS\n",
            "============================================================\n",
            "🧪 Testing simple Huber loss function:\n",
            "True values: [ 1.  2.  3. 10.]\n",
            "Predictions: [1.5 1.8 3.2 8. ]\n",
            "Huber losses: [0.125      0.02000001 0.02000001 1.5       ]\n",
            "Mean loss: 0.4162\n",
            "\n",
            "🔧 Testing configurable Huber loss (threshold=2.0):\n",
            "Huber losses (δ=2.0): [0.125      0.02000001 0.02000001 2.        ]\n",
            "Mean loss (δ=2.0): 0.5412\n",
            "\n",
            "🏗️ Testing HuberLoss class:\n",
            "Class-based loss: 0.5099999904632568\n",
            "Mean loss: 0.5100\n",
            "Loss name: custom_huber\n",
            "Configuration: {'name': 'custom_huber', 'reduction': 'sum_over_batch_size', 'threshold': 1.5}\n",
            "\n",
            "📊 Comparison with MSE:\n",
            "MSE loss: 1.0825\n",
            "Huber loss: 0.5100\n",
            "Difference: 0.5725\n",
            "\n",
            "✅ Custom Loss Functions demonstration completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🎭 2.1 CUSTOM LOSS FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🔸 2.1 CUSTOM LOSS FUNCTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple Huber Loss function\n",
        "def huber_fn(y_true, y_pred, threshold=1.0):\n",
        "    \"\"\"\n",
        "    🎯 Simple Huber loss function implementation\n",
        "\n",
        "    Args:\n",
        "        y_true: label sebenarnya\n",
        "        y_pred: prediksi model\n",
        "        threshold: batas antara squared dan linear loss\n",
        "\n",
        "    Returns:\n",
        "        tensor berisi loss untuk setiap instance\n",
        "    \"\"\"\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < threshold\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Test simple loss function\n",
        "print(\"🧪 Testing simple Huber loss function:\")\n",
        "y_true = tf.constant([1., 2., 3., 10.])\n",
        "y_pred = tf.constant([1.5, 1.8, 3.2, 8.0])\n",
        "loss = huber_fn(y_true, y_pred)\n",
        "print(f\"True values: {y_true.numpy()}\")\n",
        "print(f\"Predictions: {y_pred.numpy()}\")\n",
        "print(f\"Huber losses: {loss.numpy()}\")\n",
        "print(f\"Mean loss: {tf.reduce_mean(loss):.4f}\")\n",
        "\n",
        "# Configurable Huber loss using factory pattern\n",
        "def create_huber(threshold=1.0):\n",
        "    \"\"\"\n",
        "    🏭 Factory function untuk membuat Huber loss dengan threshold kustom\n",
        "    \"\"\"\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_fn\n",
        "\n",
        "print(\"\\n🔧 Testing configurable Huber loss (threshold=2.0):\")\n",
        "huber_2 = create_huber(threshold=2.0)\n",
        "loss_2 = huber_2(y_true, y_pred)\n",
        "print(f\"Huber losses (δ=2.0): {loss_2.numpy()}\")\n",
        "print(f\"Mean loss (δ=2.0): {tf.reduce_mean(loss_2):.4f}\")\n",
        "\n",
        "# Professional Huber Loss Class\n",
        "class HuberLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    🎭 Professional Huber Loss class untuk production use\n",
        "\n",
        "    📚 Keuntungan class approach:\n",
        "    1. Menyimpan hyperparameters dalam model\n",
        "    2. Menggunakan get_config() untuk serialization\n",
        "    3. Kompatibel dengan model saving/loading\n",
        "    4. Mendukung reduction strategies\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold=1.0, name=\"huber_loss\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"threshold\": self.threshold})\n",
        "        return config\n",
        "\n",
        "# Test HuberLoss class\n",
        "print(\"\\n🏗️ Testing HuberLoss class:\")\n",
        "huber_loss = HuberLoss(threshold=1.5, name=\"custom_huber\")\n",
        "loss_class = huber_loss(y_true, y_pred)\n",
        "print(f\"Class-based loss: {loss_class.numpy()}\")\n",
        "print(f\"Mean loss: {tf.reduce_mean(loss_class):.4f}\")\n",
        "print(f\"Loss name: {huber_loss.name}\")\n",
        "print(f\"Configuration: {huber_loss.get_config()}\")\n",
        "\n",
        "# Comparison with MSE\n",
        "print(\"\\n📊 Comparison with MSE:\")\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "mse_result = mse_loss(y_true, y_pred)\n",
        "print(f\"MSE loss: {mse_result:.4f}\")\n",
        "print(f\"Huber loss: {tf.reduce_mean(loss_class):.4f}\")\n",
        "print(f\"Difference: {mse_result - tf.reduce_mean(loss_class):.4f}\")\n",
        "\n",
        "print(\"\\n✅ Custom Loss Functions demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728f1267",
      "metadata": {
        "id": "728f1267"
      },
      "source": [
        "## 📊 **2.2 Custom Metrics**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Custom Metrics vs Loss Functions**\n",
        "\n",
        "**Perbedaan Metrics dan Loss functions:**\n",
        "\n",
        "| Aspek | Loss Functions | Metrics |\n",
        "|-------|---------------|---------|\n",
        "| **Tujuan** | Training (backpropagation) | Evaluasi |\n",
        "| **Syarat** | Harus differentiable | Boleh non-differentiable |\n",
        "| **Interpretasi** | Untuk optimizer | Untuk humans |\n",
        "| **Contoh** | MSE, Cross-entropy | Accuracy, F1-score |\n",
        "\n",
        "### **🔄 Streaming Metrics**\n",
        "**Streaming Metrics** menyimpan state antar batches untuk menghitung metric yang akurat. Diperlukan ketika metric tidak bisa di-average secara sederhana antar batches.\n",
        "\n",
        "**Contoh:** precision, recall, F1-score\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 **2.3 Custom Layers**\n",
        "\n",
        "### **📖 Penjelasan Teoritis - Custom Layers**\n",
        "\n",
        "**Custom layers berguna untuk:**\n",
        "1. **Implementasi operasi** yang tidak tersedia di Keras\n",
        "2. **Building blocks** yang dapat digunakan ulang\n",
        "3. **Menggabungkan layers** menjadi satu unit\n",
        "4. **Research purposes** dengan operasi experimental\n",
        "\n",
        "### **📋 Jenis Custom Layers:**\n",
        "1. **Stateless**: tanpa weights (gunakan `Lambda` layer)\n",
        "2. **Stateful**: dengan weights (subclass `Layer` class)\n",
        "\n",
        "### **🔧 Key Methods untuk Custom Layers:**\n",
        "- **`__init__()`**: Initialize layer parameters\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Forward pass computation\n",
        "- **`compute_output_shape()`**: Calculate output shape\n",
        "- **`get_config()`**: Serialization support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3ef16ef7",
      "metadata": {
        "id": "3ef16ef7",
        "outputId": "d39c6d3e-f3e8-4f3c-c345-506b89b29bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🔸 2.2 CUSTOM METRICS\n",
            "============================================================\n",
            "🧪 Testing streaming Huber metric:\n",
            "After batch 1: 0.0150\n",
            "After batch 2: 0.0150\n",
            "After reset: nan\n",
            "\n",
            "============================================================\n",
            "🔸 2.3 CUSTOM LAYERS\n",
            "============================================================\n",
            "🔧 1. Stateless Layer (Lambda):\n",
            "Input: [0. 1. 2.]\n",
            "exp(input): [1.        2.7182817 7.389056 ]\n",
            "\n",
            "🧪 2. Testing Custom Dense Layer:\n",
            "Input shape: (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Output:\\n[[0.19993964 1.3066338  0.30268037]\n",
            " [0.6226722  0.         2.089633  ]]\n",
            "Number of parameters: 15\n",
            "\n",
            "🎭 3. Testing Training-dependent Layer:\n",
            "Original input:\\n[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "With noise (training=True):\\n[[1.0084225 1.9139097 3.0378122]\n",
            " [3.9994805 4.9505467 6.061782 ]]\n",
            "Without noise (training=False):\\n[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "\n",
            "✅ Custom Metrics and Layers demonstration completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 📊 2.2 CUSTOM METRICS & 🧱 2.3 CUSTOM LAYERS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🔸 2.2 CUSTOM METRICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple custom metric (function-based)\n",
        "def huber_metric(y_true, y_pred, threshold=1.0):\n",
        "    \"\"\"🎯 Simple Huber metric function\"\"\"\n",
        "    return huber_fn(y_true, y_pred, threshold)\n",
        "\n",
        "# Professional Streaming Metric Class\n",
        "class HuberMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    📊 Professional Streaming Huber metric\n",
        "\n",
        "    🔄 Maintains state across batches for accurate computation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold=1.0, name='huber_metric', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Calculate metric for current batch\n",
        "        metric_values = huber_fn(y_true, y_pred, self.threshold)\n",
        "\n",
        "        # Update running totals\n",
        "        self.total.assign_add(tf.reduce_sum(metric_values))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({'threshold': self.threshold})\n",
        "        return config\n",
        "\n",
        "# Test streaming metric\n",
        "print(\"🧪 Testing streaming Huber metric:\")\n",
        "huber_metric_obj = HuberMetric(threshold=1.5, name=\"streaming_huber\")\n",
        "\n",
        "# Simulate multiple batches\n",
        "batch1_true = tf.constant([1., 2., 3.])\n",
        "batch1_pred = tf.constant([1.1, 2.2, 2.8])\n",
        "batch2_true = tf.constant([4., 5., 6.])\n",
        "batch2_pred = tf.constant([4.2, 4.8, 6.1])\n",
        "\n",
        "huber_metric_obj.update_state(batch1_true, batch1_pred)\n",
        "print(f\"After batch 1: {huber_metric_obj.result():.4f}\")\n",
        "\n",
        "huber_metric_obj.update_state(batch2_true, batch2_pred)\n",
        "print(f\"After batch 2: {huber_metric_obj.result():.4f}\")\n",
        "\n",
        "huber_metric_obj.reset_state()\n",
        "print(f\"After reset: {huber_metric_obj.result():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🔸 2.3 CUSTOM LAYERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Simple stateless layer (Lambda)\n",
        "print(\"🔧 1. Stateless Layer (Lambda):\")\n",
        "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x), name=\"exponential\")\n",
        "test_input = tf.constant([0., 1., 2.])\n",
        "exp_output = exponential_layer(test_input)\n",
        "print(f\"Input: {test_input.numpy()}\")\n",
        "print(f\"exp(input): {exp_output.numpy()}\")\n",
        "\n",
        "# 2. Professional Custom Dense Layer\n",
        "class MyDense(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    🧱 Professional Custom Dense Layer Implementation\n",
        "\n",
        "    📚 Demonstrates:\n",
        "    - Weight creation in build()\n",
        "    - Forward pass in call()\n",
        "    - Configuration serialization\n",
        "    - Proper initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create weights when input shape is known\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Forward pass computation\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output = tf.nn.bias_add(output, self.bias)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Convert the input_shape (TensorShape) to a list\n",
        "        input_shape_list = tf.TensorShape(input_shape).as_list()\n",
        "        # Slice the list and append the units\n",
        "        return tf.TensorShape(input_shape_list[:-1] + [self.units])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': tf.keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom dense layer\n",
        "print(\"\\n🧪 2. Testing Custom Dense Layer:\")\n",
        "custom_dense = MyDense(units=3, activation='relu', name='my_dense')\n",
        "test_input = tf.random.normal((2, 4))  # batch_size=2, features=4\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "output = custom_dense(test_input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output:\\\\n{output.numpy()}\")\n",
        "print(f\"Number of parameters: {custom_dense.count_params()}\")\n",
        "\n",
        "# 3. Advanced Layer with Training-dependent Behavior\n",
        "class MyGaussianNoise(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    🎭 Advanced Custom Layer dengan training-dependent behavior\n",
        "\n",
        "    🔧 Features:\n",
        "    - Different behavior during training vs inference\n",
        "    - Proper handling of training argument\n",
        "    - Regularization during training only\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            # Add noise during training for regularization\n",
        "            noise = tf.random.normal(tf.shape(inputs),\n",
        "                                   mean=0.0,\n",
        "                                   stddev=self.stddev)\n",
        "            return inputs + noise\n",
        "        else:\n",
        "            # No noise during inference\n",
        "            return inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({'stddev': self.stddev})\n",
        "        return config\n",
        "\n",
        "# Test training-dependent layer\n",
        "print(\"\\n🎭 3. Testing Training-dependent Layer:\")\n",
        "noise_layer = MyGaussianNoise(stddev=0.1, name='gaussian_noise')\n",
        "test_input = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "print(f\"Original input:\\\\n{test_input.numpy()}\")\n",
        "training_output = noise_layer(test_input, training=True)\n",
        "print(f\"With noise (training=True):\\\\n{training_output.numpy()}\")\n",
        "inference_output = noise_layer(test_input, training=False)\n",
        "print(f\"Without noise (training=False):\\\\n{inference_output.numpy()}\")\n",
        "\n",
        "print(\"\\n✅ Custom Metrics and Layers demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc2c6ec",
      "metadata": {
        "id": "dcc2c6ec"
      },
      "source": [
        "# 🏗️ Bagian 3: Advanced Custom Components\n",
        "\n",
        "## 🏗️ **3.1 Custom Models dan Autodiff**\n",
        "\n",
        "### **📖 Custom Models - Kapan Dibutuhkan:**\n",
        "- **Arsitektur kompleks** dengan skip connections\n",
        "- **Multiple inputs/outputs**\n",
        "- **Dynamic behavior** berdasarkan input\n",
        "- **Research purposes** dengan arsitektur experimental\n",
        "\n",
        "### **🔬 Automatic Differentiation (Autodiff):**\n",
        "- **Forward pass**: menghitung output dan menyimpan intermediate values\n",
        "- **Reverse pass**: menghitung gradients dengan chain rule\n",
        "- **GradientTape**: merekam operasi untuk gradient computation\n",
        "\n",
        "---\n",
        "\n",
        "# 🚀 Bagian 4: Practical Implementation\n",
        "\n",
        "## 🚀 **4.1 Putting It All Together**\n",
        "\n",
        "Bagian ini akan mendemonstrasikan penggunaan semua komponen custom dalam satu workflow lengkap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1de419fd",
      "metadata": {
        "id": "1de419fd",
        "outputId": "c4c19f1d-9cfe-48f6-f037-9b7a13cc8940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🔸 3.1 CUSTOM MODELS & AUTODIFF\n",
            "============================================================\n",
            "🏗️ Testing Custom ResNet model:\n",
            "Input shape: (5, 8)\n",
            "Output shape: (5, 1)\n",
            "Model summary:\n",
            "Total parameters: 4,021\n",
            "\n",
            "🔬 Autodiff Demonstration:\n",
            "Manual gradients: dw1=36.000003, dw2=10.000000\n",
            "Autodiff gradients: dw1=36.000000, dw2=10.000000\n",
            "\n",
            "🛡️ Testing custom gradient:\n",
            "Safe softplus gradient at x=100: 1.000000\n",
            "\n",
            "============================================================\n",
            "🔸 4.1 COMPLETE WORKFLOW DEMONSTRATION\n",
            "============================================================\n",
            "🚀 Complete Custom TensorFlow Workflow:\n",
            "\n",
            "📊 1. Data Preparation:\n",
            "Training data: (800, 10), (800, 1)\n",
            "Test data: (200, 10), (200, 1)\n",
            "\n",
            "🏗️ 2. Building Model with Custom Components:\n",
            "\n",
            "⚙️ 3. Compiling with Custom Components:\n",
            "\n",
            "🎯 4. Training Model:\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - huber_metric: 103.1037 - loss: 103.1037 - val_huber_metric: 91.8495 - val_loss: 91.8495\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric: 97.5386 - loss: 97.5386 - val_huber_metric: 78.2385 - val_loss: 78.2385\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - huber_metric: 72.0125 - loss: 72.0125 - val_huber_metric: 41.3186 - val_loss: 41.3186\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - huber_metric: 35.5277 - loss: 35.5277 - val_huber_metric: 22.8434 - val_loss: 22.8434\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric: 19.7248 - loss: 19.7248 - val_huber_metric: 11.8746 - val_loss: 11.8746\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - huber_metric: 11.8892 - loss: 11.8892 - val_huber_metric: 9.2887 - val_loss: 9.2887\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric: 8.7831 - loss: 8.7831 - val_huber_metric: 7.0321 - val_loss: 7.0321\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - huber_metric: 6.5458 - loss: 6.5458 - val_huber_metric: 5.7587 - val_loss: 5.7587\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - huber_metric: 5.4868 - loss: 5.4868 - val_huber_metric: 4.0592 - val_loss: 4.0592\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric: 4.2812 - loss: 4.2812 - val_huber_metric: 3.0740 - val_loss: 3.0740\n",
            "\n",
            "📈 5. Model Evaluation:\n",
            "Test Loss (Huber): 2.7539\n",
            "Test Metric (Huber): 2.7539\n",
            "\n",
            "🔮 6. Making Predictions:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
            "Sample predictions: [  43.627415    75.75391     -7.3805156 -291.67288     50.084236 ]\n",
            "Actual values: [  42.67138    75.014084   -4.055391 -295.72165    44.432434]\n",
            "\n",
            "============================================================\n",
            "🎉 COMPLETE WORKFLOW SUCCESSFULLY EXECUTED!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 🏗️ 3.1 CUSTOM MODELS & 🔬 AUTODIFF\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🔸 3.1 CUSTOM MODELS & AUTODIFF\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Advanced Custom Model with Residual Connections\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    🔗 Residual Block with skip connections\n",
        "\n",
        "    📚 Theory: output = input + F(input)\n",
        "    ✅ Helps with vanishing gradient problem\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_layers = []\n",
        "        for i in range(n_layers):\n",
        "            self.hidden_layers.append(\n",
        "                tf.keras.layers.Dense(\n",
        "                    n_neurons,\n",
        "                    activation='elu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=f'hidden_{i}'\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden_layers:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z  # Skip connection\n",
        "\n",
        "class CustomResNet(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    🏗️ Custom ResNet-style model\n",
        "\n",
        "    🎯 Demonstrates:\n",
        "    - Custom model architecture\n",
        "    - Residual connections\n",
        "    - Multiple custom layers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = tf.keras.layers.Dense(30, activation='elu',\n",
        "                                           kernel_initializer='he_normal')\n",
        "        self.block1 = ResidualBlock(2, 30, name='block1')\n",
        "        self.block2 = ResidualBlock(2, 30, name='block2')\n",
        "        self.output_layer = tf.keras.layers.Dense(output_dim, name='output')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.output_layer(Z)\n",
        "\n",
        "# Test custom model\n",
        "print(\"🏗️ Testing Custom ResNet model:\")\n",
        "model = CustomResNet(output_dim=1, name='custom_resnet')\n",
        "test_input = tf.random.normal((5, 8))  # 5 samples, 8 features\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "output = model(test_input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Model summary:\")\n",
        "model.build(input_shape=(None, 8))\n",
        "print(f\"Total parameters: {model.count_params():,}\")\n",
        "\n",
        "# Autodiff demonstration\n",
        "print(\"\\n🔬 Autodiff Demonstration:\")\n",
        "\n",
        "def simple_function(w1, w2):\n",
        "    \"\"\"Simple function for gradient demo\"\"\"\n",
        "    return 3 * w1**2 + 2 * w1 * w2\n",
        "\n",
        "# Manual gradient (inefficient)\n",
        "w1, w2 = 5.0, 3.0\n",
        "eps = 1e-6\n",
        "manual_dw1 = (simple_function(w1 + eps, w2) - simple_function(w1, w2)) / eps\n",
        "manual_dw2 = (simple_function(w1, w2 + eps) - simple_function(w1, w2)) / eps\n",
        "\n",
        "print(f\"Manual gradients: dw1={manual_dw1:.6f}, dw2={manual_dw2:.6f}\")\n",
        "\n",
        "# Autodiff with GradientTape\n",
        "w1, w2 = tf.Variable(5.0), tf.Variable(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    result = simple_function(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(result, [w1, w2])\n",
        "print(f\"Autodiff gradients: dw1={gradients[0].numpy():.6f}, dw2={gradients[1].numpy():.6f}\")\n",
        "\n",
        "# Advanced: Custom gradient for numerical stability\n",
        "@tf.custom_gradient\n",
        "def safe_softplus(z):\n",
        "    \"\"\"Numerically stable softplus with custom gradient\"\"\"\n",
        "    exp_z = tf.exp(z)\n",
        "    def grad(dy):\n",
        "        return dy / (1 + 1 / exp_z)\n",
        "    return tf.math.log(exp_z + 1), grad\n",
        "\n",
        "print(\"\\n🛡️ Testing custom gradient:\")\n",
        "x = tf.Variable([100.0])  # Large value that might cause issues\n",
        "with tf.GradientTape() as tape:\n",
        "    y = safe_softplus(x)\n",
        "grad = tape.gradient(y, [x])\n",
        "print(f\"Safe softplus gradient at x=100: {grad[0].numpy().item():.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🔸 4.1 COMPLETE WORKFLOW DEMONSTRATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Complete workflow using all custom components\n",
        "print(\"🚀 Complete Custom TensorFlow Workflow:\")\n",
        "\n",
        "# 1. Prepare data\n",
        "print(\"\\n📊 1. Data Preparation:\")\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to float32 for TensorFlow\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32).reshape(-1, 1)\n",
        "y_test = y_test.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "print(f\"Training data: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test data: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# 2. Build model with custom components\n",
        "print(\"\\n🏗️ 2. Building Model with Custom Components:\")\n",
        "model = tf.keras.Sequential([\n",
        "    MyDense(32, activation='relu', name='custom_dense1'),\n",
        "    MyGaussianNoise(0.1, name='custom_noise'),\n",
        "    MyDense(16, activation='relu', name='custom_dense2'),\n",
        "    tf.keras.layers.Dense(1, name='output')\n",
        "], name='custom_model')\n",
        "\n",
        "# 3. Compile with custom loss and metrics\n",
        "print(\"\\n⚙️ 3. Compiling with Custom Components:\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=HuberLoss(threshold=1.0),\n",
        "    metrics=[HuberMetric(threshold=1.0)]\n",
        ")\n",
        "\n",
        "# 4. Train model\n",
        "print(\"\\n🎯 4. Training Model:\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Evaluate\n",
        "print(\"\\n📈 5. Model Evaluation:\")\n",
        "test_loss, test_metric = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss (Huber): {test_loss:.4f}\")\n",
        "print(f\"Test Metric (Huber): {test_metric:.4f}\")\n",
        "\n",
        "# 6. Predictions\n",
        "print(\"\\n🔮 6. Making Predictions:\")\n",
        "predictions = model.predict(X_test[:5])\n",
        "print(f\"Sample predictions: {predictions.flatten()}\")\n",
        "print(f\"Actual values: {y_test[:5].flatten()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🎉 COMPLETE WORKFLOW SUCCESSFULLY EXECUTED!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5c05f2",
      "metadata": {
        "id": "4c5c05f2"
      },
      "source": [
        "# 📋 Bagian 5: Summary dan Best Practices\n",
        "\n",
        "## 🎯 **Ringkasan Chapter 12**\n",
        "\n",
        "### **✅ Apa yang Telah Dipelajari:**\n",
        "\n",
        "| Komponen | Fungsi | Kapan Digunakan |\n",
        "|----------|--------|----------------|\n",
        "| **🎭 Custom Loss** | Fungsi objektif khusus | Loss standar tidak sesuai |\n",
        "| **📊 Custom Metrics** | Evaluasi khusus | Metrik standar tidak cukup |\n",
        "| **🧱 Custom Layers** | Operasi layer khusus | Operasi tidak tersedia di Keras |\n",
        "| **🏗️ Custom Models** | Arsitektur kompleks | Arsitektur non-standard |\n",
        "| **🔬 Autodiff** | Gradient computation | Custom training loops |\n",
        "\n",
        "### **📊 Decision Tree: Kapan Menggunakan Custom Components?**\n",
        "\n",
        "```\n",
        "95% kasus: tf.keras sudah cukup ✅\n",
        "    ↓\n",
        "5% kasus yang memerlukan custom:\n",
        "    ├── Research dengan algoritma baru 🔬\n",
        "    ├── Arsitektur yang sangat khusus 🏗️\n",
        "    ├── Performance optimization ekstrem ⚡\n",
        "    └── Integration dengan sistem khusus 🔧\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Best Practices dan Guidelines**\n",
        "\n",
        "### **1. 🏁 Getting Started:**\n",
        "- ✅ **Mulai dengan tf.keras** standard components\n",
        "- ✅ **Custom hanya jika benar-benar perlu**\n",
        "- ✅ **Test thoroughly** sebelum production\n",
        "\n",
        "### **2. 🔧 Development Practices:**\n",
        "- ✅ **Follow existing patterns** dalam TensorFlow/Keras\n",
        "- ✅ **Implement `get_config()`** untuk serialization\n",
        "- ✅ **Use type hints** dan comprehensive docstrings\n",
        "- ✅ **Add proper error handling**\n",
        "\n",
        "### **3. 📚 Code Quality:**\n",
        "- ✅ **Clear naming conventions**\n",
        "- ✅ **Comprehensive documentation**\n",
        "- ✅ **Unit tests** untuk setiap component\n",
        "- ✅ **Performance profiling** jika diperlukan\n",
        "\n",
        "### **4. 🚀 Performance:**\n",
        "- ✅ **Use TF Functions** untuk graph optimization\n",
        "- ✅ **Batch operations** instead of loops\n",
        "- ✅ **Proper data types** (float32 vs float64)\n",
        "- ✅ **GPU-friendly operations**\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **Next Steps**\n",
        "\n",
        "### **📈 Untuk Further Learning:**\n",
        "1. **TensorFlow Probability** - Advanced probabilistic models\n",
        "2. **TensorFlow Serving** - Model deployment\n",
        "3. **TensorFlow Lite** - Mobile deployment\n",
        "4. **TensorFlow.js** - Web deployment\n",
        "5. **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "### **💡 Project Ideas:**\n",
        "- Implement paper algorithms with custom components\n",
        "- Build domain-specific layers untuk aplikasi khusus\n",
        "- Create reusable component library\n",
        "- Optimize existing models dengan custom training loops\n",
        "\n",
        "---\n",
        "\n",
        "## 🎓 **Kesimpulan**\n",
        "\n",
        "**Chapter 12** memberikan foundation yang solid untuk:\n",
        "- **🔧 Low-level TensorFlow development**\n",
        "- **🎯 Custom component creation**\n",
        "- **🚀 Advanced model architectures**\n",
        "- **📊 Production-ready implementations**\n",
        "\n",
        "**Dengan knowledge ini, Anda siap untuk:**\n",
        "- **Research dan development** model advanced\n",
        "- **Production deployment** dengan custom requirements\n",
        "- **Performance optimization** untuk kasus spesifik\n",
        "- **Integration** dengan existing systems\n",
        "\n",
        "---\n",
        "\n",
        "### **🎉 Selamat! Anda telah menguasai Custom Models and Training with TensorFlow!**\n",
        "\n",
        "**Next:** Ready untuk tackle real-world custom TensorFlow projects! 🚀"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}