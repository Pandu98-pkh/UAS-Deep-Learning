{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88326d84",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
    "# Hands-On Machine Learning - Implementasi dan Penjelasan Teoretis\n",
    "\n",
    "## Daftar Isi\n",
    "1. [Pengantar](#pengantar)\n",
    "2. [Dari Neuron Biologis ke Neuron Tiruan](#dari-neuron-biologis-ke-neuron-tiruan)\n",
    "3. [Perceptron](#perceptron)\n",
    "4. [Multilayer Perceptron dan Backpropagation](#multilayer-perceptron-dan-backpropagation)\n",
    "5. [Implementasi MLP dengan Keras](#implementasi-mlp-dengan-keras)\n",
    "6. [Functional API untuk Arsitektur Kompleks](#functional-api-untuk-arsitektur-kompleks)\n",
    "7. [Subclassing API](#subclassing-api)\n",
    "8. [Saving dan Loading Models](#saving-dan-loading-models)\n",
    "9. [Callbacks](#callbacks)\n",
    "10. [TensorBoard](#tensorboard)\n",
    "11. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "12. [Best Practices dan Guidelines](#best-practices-dan-guidelines)\n",
    "13. [Kesimpulan dan Rangkuman](#kesimpulan-dan-rangkuman)\n",
    "\n",
    "## Pengantar\n",
    "\n",
    "**Teori:**\n",
    "Artificial Neural Networks (ANN) adalah salah satu algoritma machine learning yang paling powerful dan fleksibel. ANN terinspirasi dari cara kerja otak manusia, di mana informasi diproses oleh jaringan neuron yang saling terhubung.\n",
    "\n",
    "**Mengapa Neural Networks Penting?**\n",
    "- **Universal Approximation Theorem**: ANN dengan cukup neuron dapat mendekati fungsi kontinyu apa pun\n",
    "- **Feature Learning**: Dapat otomatis belajar representasi fitur yang optimal dari data raw\n",
    "- **Non-linearity**: Mampu memodelkan hubungan non-linear yang kompleks\n",
    "- **Scalability**: Dapat menangani dataset yang sangat besar dan high-dimensional\n",
    "\n",
    "**Aplikasi ANN:**\n",
    "- **Computer Vision**: Image classification, object detection, segmentasi\n",
    "- **Natural Language Processing**: Translation, sentiment analysis, chatbots\n",
    "- **Speech Recognition**: Voice assistants, transcription\n",
    "- **Recommendation Systems**: Netflix, Spotify, e-commerce\n",
    "- **Game Playing**: AlphaGo, OpenAI Five, game AI\n",
    "- **Medical Diagnosis**: Radiology, drug discovery\n",
    "- **Autonomous Vehicles**: Perception, decision making\n",
    "\n",
    "**Perkembangan Historis:**\n",
    "- **1943**: McCulloch-Pitts neuron pertama\n",
    "- **1957**: Perceptron oleh Frank Rosenblatt\n",
    "- **1969**: Minsky & Papert tunjukkan keterbatasan Perceptron\n",
    "- **1986**: Backpropagation algorithm oleh Rumelhart, Hinton, Williams\n",
    "- **2006**: Deep Learning renaissance\n",
    "- **2012**: AlexNet breakthrough dalam Computer Vision\n",
    "- **2017**: Transformer architecture untuk NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ccf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries yang diperlukan\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ceaeb",
   "metadata": {},
   "source": [
    "## 1. Dari Neuron Biologis ke Neuron Tiruan\n",
    "\n",
    "### Penjelasan Teoritis: Neuron Biologis\n",
    "\n",
    "Neuron biologis memiliki komponen utama:\n",
    "- **Dendrit**: Menerima sinyal dari neuron lain\n",
    "- **Soma (badan sel)**: Memproses sinyal\n",
    "- **Akson**: Mengirim sinyal ke neuron lain\n",
    "- **Sinapsis**: Penghubung antar neuron\n",
    "\n",
    "### Model McCulloch-Pitts\n",
    "\n",
    "Model pertama neuron tiruan yang sangat sederhana:\n",
    "- Input biner (0 atau 1)\n",
    "- Output biner\n",
    "- Aktivasi berdasarkan threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi sederhana neuron McCulloch-Pitts\n",
    "def mcculloch_pitts_neuron(inputs, weights, threshold):\n",
    "    \"\"\"\n",
    "    Implementasi neuron McCulloch-Pitts\n",
    "    \n",
    "    Args:\n",
    "        inputs: list input (0 atau 1)\n",
    "        weights: list bobot untuk setiap input\n",
    "        threshold: nilai ambang batas\n",
    "    \n",
    "    Returns:\n",
    "        output: 0 atau 1\n",
    "    \"\"\"\n",
    "    weighted_sum = sum(i * w for i, w in zip(inputs, weights))\n",
    "    return 1 if weighted_sum >= threshold else 0\n",
    "\n",
    "# Contoh penggunaan - Logical AND\n",
    "print(\"=== Logical AND dengan McCulloch-Pitts ===\")\n",
    "inputs_and = [\n",
    "    [0, 0],\n",
    "    [0, 1], \n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "weights_and = [1, 1]  # Bobot untuk kedua input\n",
    "threshold_and = 2     # Threshold = 2, sehingga kedua input harus 1\n",
    "\n",
    "for inp in inputs_and:\n",
    "    output = mcculloch_pitts_neuron(inp, weights_and, threshold_and)\n",
    "    print(f\"Input: {inp} -> Output: {output}\")\n",
    "\n",
    "print(\"\\n=== Logical OR dengan McCulloch-Pitts ===\")\n",
    "weights_or = [1, 1]   # Bobot untuk kedua input\n",
    "threshold_or = 1      # Threshold = 1, sehingga salah satu input 1 sudah cukup\n",
    "\n",
    "for inp in inputs_and:\n",
    "    output = mcculloch_pitts_neuron(inp, weights_or, threshold_or)\n",
    "    print(f\"Input: {inp} -> Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658356d",
   "metadata": {},
   "source": [
    "## 2. Perceptron\n",
    "\n",
    "### Penjelasan Teoritis: Perceptron\n",
    "\n",
    "Perceptron adalah evolusi dari model McCulloch-Pitts dengan beberapa perbedaan penting:\n",
    "- **Input kontinu**: Tidak hanya 0 dan 1\n",
    "- **Bobot yang dapat dipelajari**: Algoritma pembelajaran untuk menyesuaikan bobot\n",
    "- **Bias term**: Menambah fleksibilitas model\n",
    "\n",
    "**Rumus Perceptron:**\n",
    "```\n",
    "output = step_function(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)\n",
    "```\n",
    "\n",
    "**Algoritma Pembelajaran Perceptron:**\n",
    "```\n",
    "w(i,j)^(next) = w(i,j) + η(y_j - ŷ_j)x_i\n",
    "```\n",
    "\n",
    "### Implementasi Perceptron dengan Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55751432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset Iris untuk klasifikasi biner\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length dan width\n",
    "y = (iris.target == 0).astype(np.int32)  # Iris setosa vs others\n",
    "\n",
    "print(\"=== Dataset Iris untuk Perceptron ===\")\n",
    "print(f\"Shape data: {X.shape}\")\n",
    "print(f\"Jumlah kelas 0 (bukan setosa): {np.sum(y == 0)}\")\n",
    "print(f\"Jumlah kelas 1 (setosa): {np.sum(y == 1)}\")\n",
    "\n",
    "# Visualisasi data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', label='Bukan Setosa')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', label='Setosa')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.title('Dataset Iris: Setosa vs Others')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Training Perceptron\n",
    "perceptron = Perceptron(random_state=42)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Evaluasi\n",
    "accuracy = perceptron.score(X, y)\n",
    "print(f\"\\nAkurasi Perceptron: {accuracy:.3f}\")\n",
    "\n",
    "# Prediksi untuk contoh data baru\n",
    "X_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = perceptron.predict(X_new)\n",
    "print(f\"Prediksi untuk {X_new}: {y_pred}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrasi keterbatasan Perceptron: XOR Problem\n",
    "print(\"\\n=== Masalah XOR ===\")\n",
    "print(\"Perceptron tidak dapat menyelesaikan masalah XOR karena tidak linear separable\")\n",
    "\n",
    "# Data XOR\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR logic\n",
    "\n",
    "# Coba training Perceptron untuk XOR\n",
    "perceptron_xor = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Prediksi\n",
    "y_pred_xor = perceptron_xor.predict(X_xor)\n",
    "accuracy_xor = perceptron_xor.score(X_xor, y_xor)\n",
    "\n",
    "print(\"Input | Target | Prediksi\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\" {X_xor[i]}  |   {y_xor[i]}    |    {y_pred_xor[i]}\")\n",
    "\n",
    "print(f\"\\nAkurasi XOR: {accuracy_xor:.3f}\")\n",
    "print(\"Hasil ini menunjukkan bahwa Perceptron gagal menyelesaikan XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28e38e",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron (MLP) dan Backpropagation\n",
    "\n",
    "### Penjelasan Teoritis: MLP\n",
    "\n",
    "**Multilayer Perceptron (MLP)** adalah arsitektur neural network yang terdiri dari:\n",
    "1. **Input Layer**: Menerima data input\n",
    "2. **Hidden Layer(s)**: Melakukan transformasi non-linear\n",
    "3. **Output Layer**: Menghasilkan prediksi akhir\n",
    "\n",
    "**Mengapa MLP lebih powerful dari Perceptron?**\n",
    "- **Non-linearity**: Fungsi aktivasi non-linear memungkinkan pembelajaran pola kompleks\n",
    "- **Multiple layers**: Dapat mempelajari representasi hierarkis\n",
    "- **Universal approximation**: Dapat mendekati fungsi non-linear apa pun\n",
    "\n",
    "### Fungsi Aktivasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi berbagai fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def step_function(x):\n",
    "    \"\"\"Step function (used in Perceptron)\"\"\"\n",
    "    return (x >= 0).astype(int)\n",
    "\n",
    "# Visualisasi fungsi aktivasi\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot fungsi aktivasi\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x, sigmoid(x), 'b-', linewidth=2, label='Sigmoid')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('σ(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x, tanh(x), 'g-', linewidth=2, label='Tanh')\n",
    "plt.title('Hyperbolic Tangent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x, relu(x), 'r-', linewidth=2, label='ReLU')\n",
    "plt.title('ReLU Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x, step_function(x), 'm-', linewidth=2, label='Step')\n",
    "plt.title('Step Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('step(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot turunan (derivative)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x, sigmoid(x) * (1 - sigmoid(x)), 'b--', linewidth=2, label=\"Sigmoid'\")\n",
    "plt.plot(x, 1 - tanh(x)**2, 'g--', linewidth=2, label=\"Tanh'\")\n",
    "plt.plot(x, (x > 0).astype(float), 'r--', linewidth=2, label=\"ReLU'\")\n",
    "plt.title('Derivatives of Activation Functions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Penjelasan karakteristik setiap fungsi aktivasi\n",
    "print(\"=== Karakteristik Fungsi Aktivasi ===\")\n",
    "print(\"\\n1. SIGMOID:\")\n",
    "print(\"   - Range output: (0, 1)\")\n",
    "print(\"   - Smooth dan differentiable\")\n",
    "print(\"   - Masalah: Vanishing gradient untuk nilai ekstrem\")\n",
    "print(\"   - Penggunaan: Output layer untuk klasifikasi biner\")\n",
    "\n",
    "print(\"\\n2. TANH:\")\n",
    "print(\"   - Range output: (-1, 1)\")\n",
    "print(\"   - Zero-centered (lebih baik dari sigmoid)\")\n",
    "print(\"   - Masalah: Masih ada vanishing gradient\")\n",
    "print(\"   - Penggunaan: Hidden layers (jarang digunakan sekarang)\")\n",
    "\n",
    "print(\"\\n3. ReLU:\")\n",
    "print(\"   - Range output: [0, ∞)\")\n",
    "print(\"   - Sederhana dan cepat\")\n",
    "print(\"   - Mengatasi vanishing gradient\")\n",
    "print(\"   - Masalah: Dead ReLU (neuron mati)\")\n",
    "print(\"   - Penggunaan: Hidden layers (paling populer)\")\n",
    "\n",
    "print(\"\\n4. STEP:\")\n",
    "print(\"   - Range output: {0, 1}\")\n",
    "print(\"   - Tidak differentiable\")\n",
    "print(\"   - Penggunaan: Perceptron klasik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294a919",
   "metadata": {},
   "source": [
    "## 4. Implementasi MLP dengan Keras\n",
    "\n",
    "### Load dan Prepare Data: Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "print(\"=== Loading Fashion MNIST Dataset ===\")\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(f\"Training set shape: {X_train_full.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Data type: {X_train_full.dtype}\")\n",
    "print(f\"Pixel value range: {X_train_full.min()} - {X_train_full.max()}\")\n",
    "\n",
    "# Split validation set dan normalisasi\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Class names untuk Fashion MNIST\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(f\"\\nSetelah preprocessing:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_valid.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Visualisasi beberapa sample\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f'{class_names[y_train[i]]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Fashion MNIST Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === SEQUENTIAL API ===\n",
    "print(\"\\n=== Membuat Model dengan Sequential API ===\")\n",
    "\n",
    "# Buat model Sequential\n",
    "model_sequential = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Tampilkan summary model\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "model_sequential.summary()\n",
    "\n",
    "# Compile model\n",
    "model_sequential.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Penjelasan Arsitektur ===\")\n",
    "print(\"1. Flatten Layer: Mengubah input 28x28 menjadi vector 784\")\n",
    "print(\"2. Dense Layer 1: 300 neuron dengan aktivasi ReLU\")\n",
    "print(\"3. Dense Layer 2: 100 neuron dengan aktivasi ReLU\")\n",
    "print(\"4. Output Layer: 10 neuron dengan aktivasi Softmax (10 kelas)\")\n",
    "print(\"\\nTotal parameter: 266,610\")\n",
    "print(\"- Layer 1: 784 × 300 + 300 = 235,500\")\n",
    "print(\"- Layer 2: 300 × 100 + 100 = 30,100\") \n",
    "print(\"- Layer 3: 100 × 10 + 10 = 1,010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0933dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Training Model ===\")\n",
    "\n",
    "# Callback untuk early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    monitor='val_accuracy'\n",
    ")\n",
    "\n",
    "# Training\n",
    "history = model_sequential.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluasi pada test set\n",
    "print(\"\\n=== Evaluasi Model ===\")\n",
    "test_loss, test_accuracy = model_sequential.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_learning_curves(history):\n",
    "    \"\"\"Plot training dan validation curves\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Membuat prediksi\n",
    "print(\"\\n=== Membuat Prediksi ===\")\n",
    "\n",
    "# Prediksi probabilitas\n",
    "X_new = X_test[:5]  # 5 sampel pertama dari test set\n",
    "y_probabilities = model_sequential.predict(X_new)\n",
    "y_predictions = np.argmax(y_probabilities, axis=1)\n",
    "\n",
    "print(\"Prediksi untuk 5 sampel pertama:\")\n",
    "print(\"Index | True Label | Predicted | Confidence\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(5):\n",
    "    true_label = class_names[y_test[i]]\n",
    "    pred_label = class_names[y_predictions[i]]\n",
    "    confidence = y_probabilities[i].max()\n",
    "    \n",
    "    print(f\"  {i}   | {true_label:12} | {pred_label:9} | {confidence:.4f}\")\n",
    "\n",
    "# Visualisasi prediksi\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(5):\n",
    "    # Plot gambar\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_new[i], cmap='gray')\n",
    "    true_label = class_names[y_test[i]]\n",
    "    pred_label = class_names[y_predictions[i]]\n",
    "    color = 'green' if y_predictions[i] == y_test[i] else 'red'\n",
    "    plt.title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot probabilitas\n",
    "    plt.subplot(2, 5, i + 6)\n",
    "    plt.bar(range(10), y_probabilities[i])\n",
    "    plt.xticks(range(10), [class_names[j][:3] for j in range(10)], rotation=45)\n",
    "    plt.title(f'Confidence: {y_probabilities[i].max():.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f6d26",
   "metadata": {},
   "source": [
    "## 5. Functional API untuk Arsitektur Kompleks\n",
    "\n",
    "### Wide & Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ee7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCTIONAL API ===\n",
    "print(\"=== Functional API: Wide & Deep Network ===\")\n",
    "\n",
    "# Load California Housing dataset untuk regression\n",
    "housing = fetch_california_housing()\n",
    "X_house_full, y_house_full = housing.data, housing.target\n",
    "\n",
    "# Split data\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "    X_house_full, y_house_full, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_house, X_valid_house, y_train_house, y_valid_house = train_test_split(\n",
    "    X_train_house, y_train_house, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_house_scaled = scaler.fit_transform(X_train_house)\n",
    "X_valid_house_scaled = scaler.transform(X_valid_house)\n",
    "X_test_house_scaled = scaler.transform(X_test_house)\n",
    "\n",
    "print(f\"Housing dataset shape: {X_train_house_scaled.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "\n",
    "# Wide & Deep Architecture\n",
    "input_shape = X_train_house_scaled.shape[1:]\n",
    "\n",
    "# Input layer\n",
    "input_layer = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# Deep path\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\", name=\"hidden1\")(input_layer)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\", name=\"hidden2\")(hidden1)\n",
    "\n",
    "# Wide path (langsung ke output)\n",
    "# Concatenate input langsung dengan output hidden layer terakhir\n",
    "concat = keras.layers.Concatenate(name=\"concat\")([input_layer, hidden2])\n",
    "\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "\n",
    "# Buat model\n",
    "model_wide_deep = keras.Model(inputs=[input_layer], outputs=[output])\n",
    "\n",
    "print(\"\\n=== Wide & Deep Model Summary ===\")\n",
    "model_wide_deep.summary()\n",
    "\n",
    "# Compile dan train\n",
    "model_wide_deep.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"mae\"])\n",
    "\n",
    "history_wide_deep = model_wide_deep.fit(\n",
    "    X_train_house_scaled, y_train_house,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid_house_scaled, y_valid_house),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluasi\n",
    "test_mse, test_mae = model_wide_deep.evaluate(X_test_house_scaled, y_test_house, verbose=0)\n",
    "print(f\"\\nTest MSE: {test_mse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d819a2",
   "metadata": {},
   "source": [
    "## 6. Subclassing API\n",
    "\n",
    "### Penjelasan Teoritis: Subclassing API\n",
    "\n",
    "**Subclassing API** memberikan fleksibilitas maksimal untuk membuat arsitektur neural network yang kompleks dengan:\n",
    "- **Dynamic behavior**: Loop, conditional, dll.\n",
    "- **Custom forward pass**: Kontrol penuh atas alur data\n",
    "- **Research-friendly**: Cocok untuk eksperimen arsitektur baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUBCLASSING API ===\n",
    "print(\"=== Subclassing API: Custom Model ===\")\n",
    "\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Custom Wide & Deep model menggunakan Subclassing API\n",
    "    \"\"\"\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation, name=\"hidden1\")\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation, name=\"hidden2\")  \n",
    "        self.main_output = keras.layers.Dense(1, name=\"main_output\")\n",
    "        self.aux_output = keras.layers.Dense(1, name=\"aux_output\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Define forward pass\"\"\"\n",
    "        input_wide, input_deep = inputs\n",
    "        \n",
    "        # Deep path\n",
    "        hidden1 = self.hidden1(input_deep)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        \n",
    "        # Wide path + Deep path\n",
    "        concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "        \n",
    "        # Outputs\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        \n",
    "        return main_output, aux_output\n",
    "\n",
    "# Simulasi dengan membagi fitur menjadi wide dan deep inputs\n",
    "n_features = X_train_house_scaled.shape[1]\n",
    "wide_features = 5  # 5 fitur pertama untuk wide path\n",
    "deep_features = n_features - 2  # fitur 2-7 untuk deep path\n",
    "\n",
    "X_train_wide = X_train_house_scaled[:, :wide_features]\n",
    "X_train_deep = X_train_house_scaled[:, 2:2+deep_features]\n",
    "X_valid_wide = X_valid_house_scaled[:, :wide_features]\n",
    "X_valid_deep = X_valid_house_scaled[:, 2:2+deep_features]\n",
    "\n",
    "# Buat instance model\n",
    "model_subclass = WideAndDeepModel(units=30)\n",
    "\n",
    "# Compile\n",
    "model_subclass.compile(\n",
    "    loss=[\"mse\", \"mse\"],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=\"sgd\"\n",
    ")\n",
    "\n",
    "# Dummy training untuk build model\n",
    "model_subclass.fit(\n",
    "    [X_train_wide[:100], X_train_deep[:100]], \n",
    "    [y_train_house[:100], y_train_house[:100]],\n",
    "    epochs=1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"=== Subclassing Model Info ===\")\n",
    "print(f\"Model built: {model_subclass.built}\")\n",
    "print(f\"Model layers: {len(model_subclass.layers)}\")\n",
    "print(\"✅ Subclassing API berhasil diimplementasi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24740a43",
   "metadata": {},
   "source": [
    "## 7. Saving dan Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37efa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVING DAN LOADING MODELS ===\n",
    "print(\"=== Saving dan Loading Models ===\")\n",
    "\n",
    "# Buat folder untuk menyimpan model\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# 1. Save seluruh model (arsitektur + weights + optimizer state)\n",
    "model_sequential.save(\"saved_models/fashion_mnist_model.h5\")\n",
    "print(\"✅ Model disimpan sebagai 'fashion_mnist_model.h5'\")\n",
    "\n",
    "# 2. Save hanya weights\n",
    "model_sequential.save_weights(\"saved_models/fashion_mnist_weights.h5\")\n",
    "print(\"✅ Weights disimpan sebagai 'fashion_mnist_weights.h5'\")\n",
    "\n",
    "# 3. Load model\n",
    "print(\"\\n=== Loading Model ===\")\n",
    "loaded_model = keras.models.load_model(\"saved_models/fashion_mnist_model.h5\")\n",
    "print(\"✅ Model berhasil di-load\")\n",
    "\n",
    "# Verifikasi model yang di-load\n",
    "test_loss_original, test_acc_original = model_sequential.evaluate(X_test[:100], y_test[:100], verbose=0)\n",
    "test_loss_loaded, test_acc_loaded = loaded_model.evaluate(X_test[:100], y_test[:100], verbose=0)\n",
    "\n",
    "print(f\"\\nOriginal model - Loss: {test_loss_original:.4f}, Accuracy: {test_acc_original:.4f}\")\n",
    "print(f\"Loaded model   - Loss: {test_loss_loaded:.4f}, Accuracy: {test_acc_loaded:.4f}\")\n",
    "print(f\"Difference: {abs(test_acc_original - test_acc_loaded):.6f}\")\n",
    "\n",
    "# 4. Model architecture only (untuk Functional/Sequential API)\n",
    "model_json = model_sequential.to_json()\n",
    "with open(\"saved_models/model_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"✅ Arsitektur model disimpan sebagai JSON\")\n",
    "\n",
    "# Load architecture\n",
    "with open(\"saved_models/model_architecture.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    \n",
    "model_from_json = keras.models.model_from_json(loaded_model_json)\n",
    "print(\"✅ Arsitektur model berhasil di-load dari JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a279bd6",
   "metadata": {},
   "source": [
    "## 8. Callbacks\n",
    "\n",
    "### Penjelasan Teoritis: Callbacks\n",
    "\n",
    "**Callbacks** adalah objek yang dapat dipanggil pada berbagai tahap training untuk:\n",
    "- **Monitoring**: Melacak metrik dan loss\n",
    "- **Checkpointing**: Menyimpan model terbaik\n",
    "- **Early Stopping**: Menghentikan training jika tidak ada improvement\n",
    "- **Learning Rate Schedule**: Mengatur learning rate dinamis\n",
    "- **Custom Actions**: Aksi khusus yang didefinisikan user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CALLBACKS ===\n",
    "print(\"=== Implementing Various Callbacks ===\")\n",
    "\n",
    "# 1. ModelCheckpoint - Save best model\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"saved_models/best_model.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. EarlyStopping - Stop training jika tidak improve\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. ReduceLROnPlateau - Kurangi learning rate jika plateau\n",
    "reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.2,          # LR = LR * factor\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Custom Callback\n",
    "class TrainingMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback untuk monitor training\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Dipanggil di akhir setiap epoch\"\"\"\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Hitung ratio val/train loss\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        train_loss = logs.get('loss', 0)\n",
    "        \n",
    "        if train_loss > 0:\n",
    "            ratio = val_loss / train_loss\n",
    "            print(f\" - val/train loss ratio: {ratio:.3f}\", end=\"\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(\" ⚠️ Possible overfitting!\")\n",
    "            else:\n",
    "                print(\" ✅\")\n",
    "\n",
    "# Buat model baru untuk demo callbacks\n",
    "model_with_callbacks = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_with_callbacks.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training dengan callbacks (subset kecil untuk demo)\n",
    "print(\"\\n=== Training dengan Multiple Callbacks ===\")\n",
    "history_callbacks = model_with_callbacks.fit(\n",
    "    X_train[:1000], y_train[:1000],\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid[:200], y_valid[:200]),\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        early_stopping_callback,\n",
    "        reduce_lr_callback,\n",
    "        TrainingMonitor()\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped at epoch: {len(history_callbacks.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6880670",
   "metadata": {},
   "source": [
    "## 9. TensorBoard\n",
    "\n",
    "### Penjelasan Teoritis: TensorBoard\n",
    "\n",
    "**TensorBoard** adalah tool visualisasi yang powerful untuk:\n",
    "- **Scalars**: Visualisasi loss, accuracy, dll.\n",
    "- **Graphs**: Visualisasi arsitektur model\n",
    "- **Histograms**: Distribusi weights dan biases\n",
    "- **Images**: Visualisasi input/output images\n",
    "- **Embeddings**: Visualisasi high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TENSORBOARD ===\n",
    "print(\"=== Setting up TensorBoard ===\")\n",
    "\n",
    "# Buat direktori log\n",
    "root_logdir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "os.makedirs(root_logdir, exist_ok=True)\n",
    "\n",
    "def get_run_logdir():\n",
    "    \"\"\"Generate unique log directory name\"\"\"\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# TensorBoard callback\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=run_logdir,\n",
    "    histogram_freq=1,      # Log weight histograms setiap epoch\n",
    "    write_graph=True,      # Log computation graph\n",
    "    write_images=True,     # Log model images\n",
    "    update_freq='epoch'    # Update setiap epoch\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs akan disimpan di: {run_logdir}\")\n",
    "\n",
    "# Model sederhana untuk demo TensorBoard\n",
    "model_tensorboard = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28], name=\"flatten\"),\n",
    "    keras.layers.Dense(128, activation=\"relu\", name=\"dense_1\"),\n",
    "    keras.layers.Dropout(0.2, name=\"dropout\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", name=\"output\")\n",
    "])\n",
    "\n",
    "model_tensorboard.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training dengan TensorBoard logging\n",
    "print(\"\\n=== Training dengan TensorBoard Logging ===\")\n",
    "history_tb = model_tensorboard.fit(\n",
    "    X_train[:1000], y_train[:1000],  # Subset untuk demo cepat\n",
    "    epochs=3,\n",
    "    validation_data=(X_valid[:200], y_valid[:200]),\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ TensorBoard logs tersimpan di: {run_logdir}\")\n",
    "print(\"Untuk melihat TensorBoard, jalankan:\")\n",
    "print(f\"tensorboard --logdir={root_logdir}\")\n",
    "print(\"Kemudian buka http://localhost:6006 di browser\")\n",
    "\n",
    "# Contoh manual logging dengan tf.summary\n",
    "print(\"\\n=== Manual TensorBoard Logging ===\")\n",
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "\n",
    "with writer.as_default():\n",
    "    # Log scalar values\n",
    "    for step in range(100):\n",
    "        tf.summary.scalar(\"sine_wave\", np.sin(step * 0.1), step=step)\n",
    "        tf.summary.scalar(\"cosine_wave\", np.cos(step * 0.1), step=step)\n",
    "    \n",
    "    # Log histogram\n",
    "    data = np.random.normal(0, 1, 1000)\n",
    "    tf.summary.histogram(\"random_data\", data, step=0)\n",
    "\n",
    "print(f\"✅ Manual logs tersimpan di: {test_logdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc054ba",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "### Penjelasan Teoritis: Hyperparameter Tuning\n",
    "\n",
    "**Hyperparameter** adalah parameter yang tidak dipelajari oleh model, melainkan ditetapkan sebelum training:\n",
    "\n",
    "**Hyperparameter Penting:**\n",
    "- **Arsitektur**: Jumlah layer dan neuron\n",
    "- **Learning Rate**: Seberapa cepat model belajar\n",
    "- **Batch Size**: Jumlah sampel per update\n",
    "- **Optimizer**: Algoritma optimisasi\n",
    "- **Activation Functions**: Fungsi aktivasi\n",
    "- **Regularization**: Dropout, L1/L2\n",
    "\n",
    "**Strategi Tuning:**\n",
    "1. **Grid Search**: Mencoba semua kombinasi\n",
    "2. **Random Search**: Sampling random\n",
    "3. **Bayesian Optimization**: Menggunakan prior knowledge\n",
    "4. **Hyperband**: Multi-armed bandit approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e39b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPERPARAMETER TUNING ===\n",
    "print(\"=== Manual Hyperparameter Tuning ===\")\n",
    "\n",
    "def evaluate_model(n_neurons, learning_rate, epochs=10):\n",
    "    \"\"\"Evaluate model dengan hyperparameter tertentu\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(n_neurons, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train[:5000], y_train[:5000],  # Subset untuk demo cepat\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_valid[:1000], y_valid[:1000]),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Return final validation accuracy\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# Test beberapa kombinasi hyperparameter\n",
    "hyperparams_to_test = [\n",
    "    {\"n_neurons\": 64, \"learning_rate\": 0.01},\n",
    "    {\"n_neurons\": 128, \"learning_rate\": 0.01},\n",
    "    {\"n_neurons\": 64, \"learning_rate\": 0.001},\n",
    "    {\"n_neurons\": 128, \"learning_rate\": 0.001},\n",
    "]\n",
    "\n",
    "print(\"Testing hyperparameter combinations:\")\n",
    "print(\"Neurons | LR     | Val Accuracy\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "results = []\n",
    "for params in hyperparams_to_test:\n",
    "    val_acc = evaluate_model(**params, epochs=5)\n",
    "    results.append((params, val_acc))\n",
    "    print(f\"{params['n_neurons']:7d} | {params['learning_rate']:6.3f} | {val_acc:.4f}\")\n",
    "\n",
    "# Best combination\n",
    "best_params, best_acc = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest combination: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Demonstrasi perbandingan optimizer\n",
    "print(\"\\n=== Perbandingan Optimizers ===\")\n",
    "\n",
    "optimizers_to_test = {\n",
    "    \"SGD\": keras.optimizers.SGD(learning_rate=0.01),\n",
    "    \"Adam\": keras.optimizers.Adam(learning_rate=0.001),\n",
    "    \"RMSprop\": keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "}\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for name, optimizer in optimizers_to_test.items():\n",
    "    print(f\"\\nTesting {name} optimizer...\")\n",
    "    \n",
    "    # Buat model yang sama\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train[:2000], y_train[:2000],\n",
    "        epochs=3,\n",
    "        validation_data=(X_valid[:500], y_valid[:500]),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    final_acc = history.history['val_accuracy'][-1]\n",
    "    optimizer_results[name] = final_acc\n",
    "    print(f\"{name} final validation accuracy: {final_acc:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(optimizer_results.keys())\n",
    "accuracies = list(optimizer_results.values())\n",
    "\n",
    "plt.bar(names, accuracies, color=['blue', 'green', 'red'])\n",
    "plt.title('Optimizer Comparison')\n",
    "plt.ylabel('Final Validation Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest optimizer: {max(optimizer_results, key=optimizer_results.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09761254",
   "metadata": {},
   "source": [
    "## 11. Best Practices dan Guidelines\n",
    "\n",
    "### Penjelasan Teoritis: Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BEST PRACTICES DAN GUIDELINES ===\n",
    "print(\"=== Neural Network Best Practices ===\")\n",
    "\n",
    "# 1. Jumlah Hidden Layers\n",
    "print(\"\\n1. JUMLAH HIDDEN LAYERS:\")\n",
    "print(\"   • 1 layer: Simple problems, dapat model fungsi apa pun\")\n",
    "print(\"   • 2-3 layers: Most common problems\")\n",
    "print(\"   • 4+ layers: Complex problems (image, speech, NLP)\")\n",
    "print(\"   • Very deep: Membutuhkan teknik khusus (BatchNorm, ResNet, dll)\")\n",
    "\n",
    "# 2. Jumlah Neurons per Layer\n",
    "print(\"\\n2. JUMLAH NEURONS PER LAYER:\")\n",
    "print(\"   • Input layer: Ditentukan oleh jumlah fitur\")\n",
    "print(\"   • Hidden layers: Biasanya 10-100 neurons\")\n",
    "print(\"   • Output layer: Ditentukan oleh jumlah kelas/target\")\n",
    "print(\"   • Rule of thumb: Lebih baik oversize + regularization\")\n",
    "\n",
    "# 3. Activation Functions\n",
    "print(\"\\n3. ACTIVATION FUNCTIONS:\")\n",
    "print(\"   • Hidden layers: ReLU (default choice)\")\n",
    "print(\"   • Output regression: Linear (no activation)\")\n",
    "print(\"   • Output binary classification: Sigmoid\")\n",
    "print(\"   • Output multiclass: Softmax\")\n",
    "print(\"   • Alternative: LeakyReLU, ELU, Swish\")\n",
    "\n",
    "# 4. Loss Functions\n",
    "print(\"\\n4. LOSS FUNCTIONS:\")\n",
    "print(\"   • Regression: MSE, MAE, Huber\")\n",
    "print(\"   • Binary classification: Binary crossentropy\")\n",
    "print(\"   • Multiclass: Categorical crossentropy (one-hot)\")\n",
    "print(\"   • Multiclass sparse: Sparse categorical crossentropy\")\n",
    "\n",
    "# 5. Optimizers\n",
    "print(\"\\n5. OPTIMIZERS:\")\n",
    "print(\"   • SGD: Simple, need tuning LR\")\n",
    "print(\"   • Adam: Good default choice, adaptive LR\")\n",
    "print(\"   • RMSprop: Good for RNNs\")\n",
    "print(\"   • AdaGrad: Good for sparse data\")\n",
    "\n",
    "# === LEARNING RATE SCHEDULING ===\n",
    "print(\"\\n=== Learning Rate Scheduling ===\")\n",
    "\n",
    "# 1. Exponential Decay\n",
    "exponential_decay = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Custom learning rate callback\n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    \"\"\"Log learning rate setiap epoch\"\"\"\n",
    "    def __init__(self):\n",
    "        self.learning_rates = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        # Handle tensor vs scalar\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr_value = lr.numpy()\n",
    "        else:\n",
    "            lr_value = float(lr)\n",
    "        self.learning_rates.append(lr_value)\n",
    "        print(f\" - LR: {lr_value:.6f}\")\n",
    "\n",
    "# Test exponential decay\n",
    "print(\"\\n=== Testing Exponential Decay ===\")\n",
    "model_lr = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_lr.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=exponential_decay),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "history_lr = model_lr.fit(\n",
    "    X_train[:1000], y_train[:1000],\n",
    "    epochs=5,\n",
    "    callbacks=[lr_logger],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot learning rate schedule\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lr_logger.learning_rates, 'b-o')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_lr.history['loss'], 'r-', label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e30952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCTIONAL API ===\n",
    "print(\"=== Functional API: Wide & Deep Network ===\")\n",
    "\n",
    "# Load California Housing dataset untuk regression\n",
    "housing = fetch_california_housing()\n",
    "X_house_full, y_house_full = housing.data, housing.target\n",
    "\n",
    "# Split data\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "    X_house_full, y_house_full, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_house, X_valid_house, y_train_house, y_valid_house = train_test_split(\n",
    "    X_train_house, y_train_house, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_house_scaled = scaler.fit_transform(X_train_house)\n",
    "X_valid_house_scaled = scaler.transform(X_valid_house)\n",
    "X_test_house_scaled = scaler.transform(X_test_house)\n",
    "\n",
    "print(f\"Housing dataset shape: {X_train_house_scaled.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "\n",
    "# Wide & Deep Architecture\n",
    "input_shape = X_train_house_scaled.shape[1:]\n",
    "\n",
    "# Input layer\n",
    "input_layer = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# Deep path\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\", name=\"hidden1\")(input_layer)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\", name=\"hidden2\")(hidden1)\n",
    "\n",
    "# Wide path (langsung ke output)\n",
    "# Concatenate input langsung dengan output hidden layer terakhir\n",
    "concat = keras.layers.Concatenate(name=\"concat\")([input_layer, hidden2])\n",
    "\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "\n",
    "# Buat model\n",
    "model_wide_deep = keras.Model(inputs=[input_layer], outputs=[output])\n",
    "\n",
    "print(\"\\n=== Wide & Deep Model Summary ===\")\n",
    "model_wide_deep.summary()\n",
    "\n",
    "# Compile dan train\n",
    "model_wide_deep.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"mae\"])\n",
    "\n",
    "history_wide_deep = model_wide_deep.fit(\n",
    "    X_train_house_scaled, y_train_house,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid_house_scaled, y_valid_house),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluasi\n",
    "test_mse, test_mae = model_wide_deep.evaluate(X_test_house_scaled, y_test_house, verbose=0)\n",
    "print(f\"\\nTest MSE: {test_mse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8347b",
   "metadata": {},
   "source": [
    "## 12. Kesimpulan dan Rangkuman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\" RANGKUMAN CHAPTER 10: ARTIFICIAL NEURAL NETWORKS \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n🧠 KONSEP DASAR:\")\n",
    "print(\"• ANN terinspirasi dari jaringan saraf biologis\")\n",
    "print(\"• Neuron tiruan: input → weighted sum → activation → output\")\n",
    "print(\"• Universal approximator: dapat mempelajari fungsi non-linear kompleks\")\n",
    "\n",
    "print(\"\\n📈 EVOLUSI ARSITEKTUR:\")\n",
    "print(\"• McCulloch-Pitts (1943): Model neuron pertama\")\n",
    "print(\"• Perceptron (1957): Linear classifier dengan pembelajaran\")\n",
    "print(\"• MLP + Backpropagation (1986): Breakthrough untuk deep learning\")\n",
    "print(\"• Modern deep networks: CNN, RNN, Transformer, dll.\")\n",
    "\n",
    "print(\"\\n🔧 IMPLEMENTASI KERAS:\")\n",
    "print(\"• Sequential API: Model layer-by-layer sederhana\")\n",
    "print(\"• Functional API: Arsitektur kompleks (multi-input/output)\")  \n",
    "print(\"• Subclassing API: Kontrol penuh, research-friendly\")\n",
    "\n",
    "print(\"\\n⚙️ KOMPONEN KUNCI:\")\n",
    "print(\"• Activation Functions: ReLU (hidden), Softmax/Sigmoid (output)\")\n",
    "print(\"• Loss Functions: MSE (regression), Crossentropy (classification)\")\n",
    "print(\"• Optimizers: SGD, Adam, RMSprop\")\n",
    "print(\"• Regularization: Dropout, Early Stopping, L1/L2\")\n",
    "\n",
    "print(\"\\n📊 MONITORING & DEBUGGING:\")\n",
    "print(\"• Callbacks: Early stopping, checkpointing, LR scheduling\")\n",
    "print(\"• TensorBoard: Visualisasi loss, architecture, weights\")\n",
    "print(\"• Learning Curves: Deteksi overfitting/underfitting\")\n",
    "\n",
    "print(\"\\n🎯 HYPERPARAMETER TUNING:\")\n",
    "print(\"• Architecture: # layers, # neurons per layer\")\n",
    "print(\"• Training: Learning rate, batch size, epochs\")\n",
    "print(\"• Regularization: Dropout rate, weight decay\")\n",
    "print(\"• Search strategies: Grid, Random, Bayesian, Hyperband\")\n",
    "\n",
    "print(\"\\n✅ BEST PRACTICES:\")\n",
    "print(\"• Start simple: 1-2 hidden layers, ReLU activation\")\n",
    "print(\"• Oversize + regularize better than undersize\")\n",
    "print(\"• Monitor val/train gap untuk deteksi overfitting\")\n",
    "print(\"• Use callbacks untuk automated training\")\n",
    "print(\"• Standardize inputs, monitor gradients\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"• Chapter 11: Training Deep Neural Networks\")\n",
    "print(\"• Advanced architectures: CNN, RNN\")\n",
    "print(\"• Regularization techniques\")\n",
    "print(\"• Transfer learning\")\n",
    "print(\"• Production deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" SELAMAT! Anda telah menguasai dasar Neural Networks! \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Final demonstration: Complete workflow\n",
    "print(\"\\n🎯 DEMO: Complete ML Workflow dengan Neural Networks\")\n",
    "\n",
    "# 1. Data preparation\n",
    "print(\"\\n1. 📁 Data Preparation\")\n",
    "X_demo, y_demo = X_train[:1000], y_train[:1000]\n",
    "print(f\"   Dataset shape: {X_demo.shape}\")\n",
    "\n",
    "# 2. Model building\n",
    "print(\"\\n2. 🏗️  Model Building\")\n",
    "final_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(128, activation=\"relu\", name=\"hidden1\"),\n",
    "    keras.layers.Dropout(0.2, name=\"dropout\"),\n",
    "    keras.layers.Dense(64, activation=\"relu\", name=\"hidden2\"), \n",
    "    keras.layers.Dense(10, activation=\"softmax\", name=\"output\")\n",
    "])\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "print(f\"   Model created with {final_model.count_params():,} parameters\")\n",
    "\n",
    "# 3. Training with callbacks\n",
    "print(\"\\n3. 🏃 Training with Callbacks\")\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "]\n",
    "\n",
    "history_final = final_model.fit(\n",
    "    X_demo, y_demo,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"   Training completed in {len(history_final.history['loss'])} epochs\")\n",
    "\n",
    "# 4. Evaluation\n",
    "print(\"\\n4. 📊 Evaluation\")\n",
    "test_loss, test_acc = final_model.evaluate(X_test[:500], y_test[:500], verbose=0)\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# 5. Save model\n",
    "print(\"\\n5. 💾 Save Model\")\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "final_model.save(\"saved_models/final_demo_model.h5\")\n",
    "print(\"   ✅ Model saved successfully!\")\n",
    "\n",
    "print(\"\\n🎉 Workflow completed successfully!\")\n",
    "print(\"   Anda sekarang siap untuk tackle real-world problems!\")\n",
    "\n",
    "print(\"\\n📚 RESOURCES DAN REFERENSI LANJUTAN:\")\n",
    "print(\"\\n📖 Books:\")\n",
    "print(\"• Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville\")\n",
    "print(\"• Neural Networks and Deep Learning - Michael Nielsen (online)\")\n",
    "print(\"• Deep Learning with Python - François Chollet\")\n",
    "\n",
    "print(\"\\n🌐 Online Courses:\")\n",
    "print(\"• Deep Learning Specialization - Andrew Ng (Coursera)\")\n",
    "print(\"• Fast.ai - Practical Deep Learning\")\n",
    "print(\"• CS231n - Stanford CNN Course\")\n",
    "\n",
    "print(\"\\n🔗 Websites & Tools:\")\n",
    "print(\"• TensorFlow.org - Official documentation\")\n",
    "print(\"• Keras.io - High-level API documentation\")\n",
    "print(\"• TensorBoard.dev - Share tensorboard logs\")\n",
    "print(\"• Papers With Code - Latest research\")\n",
    "\n",
    "print(\"\\n🛠️ Practical Tools:\")\n",
    "print(\"• Google Colab - Free GPU/TPU\")\n",
    "print(\"• Kaggle Kernels - Competitions & datasets\")\n",
    "print(\"• Weights & Biases - Experiment tracking\")\n",
    "print(\"• MLflow - ML lifecycle management\")\n",
    "\n",
    "print(\"\\n💡 Tips for Further Learning:\")\n",
    "print(\"• Practice dengan real datasets\")\n",
    "print(\"• Join Kaggle competitions\")\n",
    "print(\"• Read dan implement paper terbaru\")\n",
    "print(\"• Build projects dari scratch\")\n",
    "print(\"• Contribute ke open source projects\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
