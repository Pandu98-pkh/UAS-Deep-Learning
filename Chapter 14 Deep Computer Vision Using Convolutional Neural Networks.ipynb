{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223a3c3b",
   "metadata": {},
   "source": [
    "# 🖼️ Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
    "# Bab 14: Deep Computer Vision Menggunakan Convolutional Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Tujuan Pembelajaran\n",
    "\n",
    "Setelah menyelesaikan chapter ini, Anda akan mampu:\n",
    "- ✅ Memahami arsitektur dan konsep CNN\n",
    "- ✅ Mengimplementasikan berbagai layer CNN (Conv2D, MaxPooling, dll)\n",
    "- ✅ Membangun model CNN untuk image classification\n",
    "- ✅ Menerapkan teknik transfer learning\n",
    "- ✅ Mengoptimalkan performa model CNN\n",
    "- ✅ Memahami teknik data augmentation untuk computer vision\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Outline Chapter\n",
    "\n",
    "1. **Pengantar Computer Vision & CNN** 🧠\n",
    "2. **Konvolusi dan Filter** 🔍  \n",
    "3. **Pooling Layers** 🏊\n",
    "4. **Arsitektur CNN Lengkap** 🏗️\n",
    "5. **Implementasi CNN dengan TensorFlow/Keras** 💻\n",
    "6. **Transfer Learning** 🔄\n",
    "7. **Data Augmentation** 📸\n",
    "8. **CNN Architectures Populer** 🏆\n",
    "9. **Best Practices & Tips** 💡\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Pengantar: Revolusi Computer Vision\n",
    "\n",
    "### 🤖 Mengapa Computer Vision Sulit?\n",
    "\n",
    "Convolutional Neural Networks (CNN) merupakan **revolusi** dalam bidang computer vision. Berbeda dengan Deep Blue IBM yang mengalahkan juara catur dunia Garry Kasparov pada 1996, tugas-tugas visual yang tampak trivial bagi manusia (seperti mendeteksi anak anjing dalam gambar) baru bisa diatasi komputer dalam beberapa tahun terakhir.\n",
    "\n",
    "### 🧠 Tantangan Persepsi Visual:\n",
    "- **Otomatis**: Persepsi terjadi di luar kesadaran kita, dalam modul-modul khusus di otak\n",
    "- **Kompleks**: Ketika informasi mencapai kesadaran, sudah dilengkapi dengan fitur-fitur tingkat tinggi  \n",
    "- **Implicit**: Kita tidak bisa menjelaskan bagaimana kita mengenali sesuatu - itu otomatis\n",
    "\n",
    "### 🚀 CNN: Terobosan Teknologi\n",
    "\n",
    "CNN meniru cara kerja **korteks visual otak** dan telah mencapai performa superhuman dalam berbagai tugas visual berkat:\n",
    "\n",
    "1. **💪 Kekuatan Komputasi** - GPU powerful untuk parallel processing\n",
    "2. **📊 Big Data** - Dataset besar seperti ImageNet (14+ million images)\n",
    "3. **🧠 Advanced Techniques** - Teknik training deep networks yang canggih\n",
    "4. **🏗️ Better Architectures** - LeNet → AlexNet → VGG → ResNet → EfficientNet\n",
    "\n",
    "### 🎯 Aplikasi CNN Modern:\n",
    "- 📱 **Mobile Vision** - Face recognition, object detection\n",
    "- 🚗 **Autonomous Vehicles** - Self-driving cars  \n",
    "- 🏥 **Medical Imaging** - Cancer detection, X-ray analysis\n",
    "- 🎮 **Augmented Reality** - Real-time object tracking\n",
    "- 🛡️ **Security** - Surveillance, biometric systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Setup & Import Libraries\n",
    "print(\"=\" * 70)\n",
    "print(\"🖼️ CHAPTER 14: Deep Computer Vision Using CNNs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import libraries yang diperlukan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display versions\n",
    "print(f\"📦 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"📦 Keras version: {keras.__version__}\")\n",
    "print(f\"📦 NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🎮 GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "    print(\"🚀 CUDA enabled - Ready for accelerated training!\")\n",
    "else:\n",
    "    print(\"💻 Running on CPU\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\n✅ Setup complete! Ready to explore Computer Vision with CNNs\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec6078",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔍 1. Konvolusi dan Filter - Dasar CNN\n",
    "\n",
    "## 🧮 Apa itu Konvolusi?\n",
    "\n",
    "**Konvolusi** adalah operasi matematika fundamental dalam CNN yang menerapkan **filter** (kernel) pada gambar untuk mengekstrak fitur.\n",
    "\n",
    "### 📐 Konsep Dasar:\n",
    "- **Input**: Image matrix (contoh: 5×5 pixels)\n",
    "- **Filter/Kernel**: Small matrix (contoh: 3×3) dengan weights yang dapat dipelajari\n",
    "- **Output**: Feature map hasil konvolusi\n",
    "- **Sliding Window**: Filter bergeser pixel demi pixel\n",
    "\n",
    "### 🎯 Fungsi Konvolusi:\n",
    "- **Edge Detection** - Mendeteksi tepi/garis dalam gambar\n",
    "- **Feature Extraction** - Mengekstrak pola dan tekstur\n",
    "- **Spatial Hierarchy** - Membangun representasi hierarkis dari sederhana ke kompleks\n",
    "\n",
    "### ⚙️ Parameter Penting:\n",
    "- **Stride**: Langkah pergeseran filter (default: 1)\n",
    "- **Padding**: Penambahan pixel di tepi (same/valid)\n",
    "- **Dilation**: Spacing antara kernel elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb85428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 1.1 Konvolusi dalam Praktik - Visual Demonstration\n",
    "print(\"🔍 CONVOLUTION FUNDAMENTALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load sample image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "\n",
    "print(f\"📷 Sample images loaded:\")\n",
    "print(f\"   China image shape: {china.shape}\")\n",
    "print(f\"   Flower image shape: {flower.shape}\")\n",
    "\n",
    "# Convert to grayscale for simplicity\n",
    "def rgb_to_grayscale(images):\n",
    "    \"\"\"Convert RGB images to grayscale\"\"\"\n",
    "    return tf.reduce_mean(tf.cast(images, tf.float32), axis=-1, keepdims=True)\n",
    "\n",
    "# Convert images\n",
    "china_gray = rgb_to_grayscale(china[np.newaxis])\n",
    "flower_gray = rgb_to_grayscale(flower[np.newaxis])\n",
    "\n",
    "print(f\"   Grayscale shapes: {china_gray.shape}\")\n",
    "\n",
    "print(\"\\n🔧 DEFINING CONVOLUTION FILTERS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Define various edge detection filters\n",
    "filters = {\n",
    "    \"Vertical Edge\": np.array([\n",
    "        [[-1, 0, 1],\n",
    "         [-1, 0, 1], \n",
    "         [-1, 0, 1]]\n",
    "    ]),\n",
    "    \"Horizontal Edge\": np.array([\n",
    "        [[-1, -1, -1],\n",
    "         [ 0,  0,  0],\n",
    "         [ 1,  1,  1]]\n",
    "    ]),\n",
    "    \"Diagonal Edge\": np.array([\n",
    "        [[-1, -1,  0],\n",
    "         [-1,  0,  1],\n",
    "         [ 0,  1,  1]]\n",
    "    ]),\n",
    "    \"Sharpen\": np.array([\n",
    "        [[ 0, -1,  0],\n",
    "         [-1,  5, -1],\n",
    "         [ 0, -1,  0]]\n",
    "    ]),\n",
    "    \"Blur\": np.array([\n",
    "        [[1/9, 1/9, 1/9],\n",
    "         [1/9, 1/9, 1/9],\n",
    "         [1/9, 1/9, 1/9]]\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Convert filters to TensorFlow format\n",
    "tf_filters = {}\n",
    "for name, filter_array in filters.items():\n",
    "    # Shape: [height, width, in_channels, out_channels]\n",
    "    tf_filters[name] = tf.constant(filter_array.reshape(3, 3, 1, 1), dtype=tf.float32)\n",
    "    print(f\"✅ {name} filter prepared: {filter_array.reshape(3, 3)}\")\n",
    "\n",
    "print(\"\\n🎨 APPLYING CONVOLUTION FILTERS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "def apply_convolution(image, filter_tensor, filter_name):\n",
    "    \"\"\"Apply convolution filter to image\"\"\"\n",
    "    # Apply convolution\n",
    "    filtered = tf.nn.conv2d(image, filter_tensor, strides=1, padding='SAME')\n",
    "    return filtered\n",
    "\n",
    "# Apply filters to sample image\n",
    "results = {}\n",
    "sample_image = china_gray\n",
    "\n",
    "print(f\"🖼️ Applying filters to image (shape: {sample_image.shape}):\")\n",
    "\n",
    "for filter_name, filter_tensor in tf_filters.items():\n",
    "    filtered_image = apply_convolution(sample_image, filter_tensor, filter_name)\n",
    "    results[filter_name] = filtered_image\n",
    "    print(f\"   ✅ {filter_name}: Output shape {filtered_image.shape}\")\n",
    "\n",
    "print(\"\\n📊 VISUALIZING CONVOLUTION RESULTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_image[0, :, :, 0], cmap='gray')\n",
    "axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Filtered images\n",
    "for i, (filter_name, filtered_image) in enumerate(results.items(), 1):\n",
    "    if i < len(axes):\n",
    "        axes[i].imshow(filtered_image[0, :, :, 0], cmap='gray')\n",
    "        axes[i].set_title(f'{filter_name}', fontsize=12, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('🔍 Convolution Filter Effects', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 CONVOLUTION INSIGHTS:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"🔸 Vertical Edge filter detects vertical boundaries\")\n",
    "print(\"🔸 Horizontal Edge filter detects horizontal boundaries\") \n",
    "print(\"🔸 Diagonal Edge filter detects diagonal patterns\")\n",
    "print(\"🔸 Sharpen filter enhances image details\")\n",
    "print(\"🔸 Blur filter smooths the image\")\n",
    "print(\"🔸 Each filter extracts different features!\")\n",
    "\n",
    "print(\"\\n✅ Convolution demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e082ab3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🏊 2. Pooling Layers - Dimensionality Reduction\n",
    "\n",
    "## 🎯 Mengapa Pooling?\n",
    "\n",
    "**Pooling layers** melakukan **downsampling** pada feature maps dengan tujuan:\n",
    "\n",
    "### ✅ Keuntungan Pooling:\n",
    "- **📉 Reduce Dimensionality** - Mengurangi ukuran spatial (height × width)\n",
    "- **⚡ Computational Efficiency** - Faster training dan inference\n",
    "- **🛡️ Translation Invariance** - Robust terhadap small shifts\n",
    "- **🎯 Feature Abstraction** - Focus pada fitur penting\n",
    "- **📊 Reduce Overfitting** - Fewer parameters to learn\n",
    "\n",
    "### 🔧 Jenis Pooling:\n",
    "\n",
    "#### 1. **Max Pooling** 🏆\n",
    "- Mengambil nilai **maksimum** dari setiap window\n",
    "- Paling populer untuk feature extraction\n",
    "- Preserves strong features\n",
    "\n",
    "#### 2. **Average Pooling** 📊  \n",
    "- Mengambil nilai **rata-rata** dari setiap window\n",
    "- Smoother output\n",
    "- Often used in final layers\n",
    "\n",
    "#### 3. **Global Pooling** 🌐\n",
    "- **Global Max/Average** dari entire feature map\n",
    "- Reduces to single value per channel\n",
    "- Common before final classification layer\n",
    "\n",
    "### ⚙️ Parameter Pooling:\n",
    "- **Pool Size**: Ukuran window (umum: 2×2)\n",
    "- **Strides**: Langkah pergeseran (umum: sama dengan pool size)\n",
    "- **Padding**: Handling border pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac302fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏊 2.1 Pooling Layers dalam Praktik\n",
    "print(\"🏊 POOLING LAYERS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the filtered image from previous convolution\n",
    "sample_feature_map = results[\"Vertical Edge\"]  # Shape: [1, height, width, 1]\n",
    "\n",
    "print(f\"📊 Input feature map shape: {sample_feature_map.shape}\")\n",
    "\n",
    "print(\"\\n🔧 APPLYING DIFFERENT POOLING OPERATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Max Pooling\n",
    "max_pooled = tf.nn.max_pool2d(\n",
    "    sample_feature_map,\n",
    "    ksize=[1, 2, 2, 1],    # Pool size 2x2\n",
    "    strides=[1, 2, 2, 1],  # Stride 2x2\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# 2. Average Pooling  \n",
    "avg_pooled = tf.nn.avg_pool2d(\n",
    "    sample_feature_map,\n",
    "    ksize=[1, 2, 2, 1],    # Pool size 2x2\n",
    "    strides=[1, 2, 2, 1],  # Stride 2x2\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "print(f\"✅ Max Pooling: {sample_feature_map.shape} → {max_pooled.shape}\")\n",
    "print(f\"✅ Average Pooling: {sample_feature_map.shape} → {avg_pooled.shape}\")\n",
    "\n",
    "# Size reduction calculation\n",
    "original_size = sample_feature_map.shape[1] * sample_feature_map.shape[2]\n",
    "pooled_size = max_pooled.shape[1] * max_pooled.shape[2]\n",
    "reduction = (1 - pooled_size / original_size) * 100\n",
    "\n",
    "print(f\"📉 Size reduction: {original_size} → {pooled_size} pixels ({reduction:.1f}% smaller)\")\n",
    "\n",
    "print(\"\\n📊 VISUALIZING POOLING EFFECTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original feature map\n",
    "axes[0].imshow(sample_feature_map[0, :, :, 0], cmap='gray')\n",
    "axes[0].set_title(f'Original Feature Map\\n{sample_feature_map.shape[1]}×{sample_feature_map.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Max pooled\n",
    "axes[1].imshow(max_pooled[0, :, :, 0], cmap='gray')\n",
    "axes[1].set_title(f'Max Pooled (2×2)\\n{max_pooled.shape[1]}×{max_pooled.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Average pooled\n",
    "axes[2].imshow(avg_pooled[0, :, :, 0], cmap='gray')\n",
    "axes[2].set_title(f'Average Pooled (2×2)\\n{avg_pooled.shape[1]}×{avg_pooled.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('🏊 Pooling Operations Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 POOLING WITH DIFFERENT PARAMETERS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Different pool sizes\n",
    "pool_sizes = [2, 3, 4]\n",
    "pooling_results = {}\n",
    "\n",
    "for pool_size in pool_sizes:\n",
    "    pooled = tf.nn.max_pool2d(\n",
    "        sample_feature_map,\n",
    "        ksize=[1, pool_size, pool_size, 1],\n",
    "        strides=[1, pool_size, pool_size, 1],\n",
    "        padding='SAME'\n",
    "    )\n",
    "    pooling_results[f\"{pool_size}x{pool_size}\"] = pooled\n",
    "    \n",
    "    new_height, new_width = pooled.shape[1], pooled.shape[2]\n",
    "    size_reduction = (1 - (new_height * new_width) / original_size) * 100\n",
    "    \n",
    "    print(f\"🔸 Pool {pool_size}×{pool_size}: {sample_feature_map.shape[1:3]} → {(new_height, new_width)} \"\n",
    "          f\"({size_reduction:.1f}% reduction)\")\n",
    "\n",
    "print(\"\\n💡 POOLING INSIGHTS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"🔸 Max Pooling preserves strongest features (sharp edges)\")\n",
    "print(\"🔸 Average Pooling creates smoother representation\")\n",
    "print(\"🔸 Larger pool sizes = more aggressive dimensionality reduction\")\n",
    "print(\"🔸 Trade-off: Smaller size vs Information loss\")\n",
    "print(\"🔸 Common choice: 2×2 max pooling with stride 2\")\n",
    "\n",
    "print(\"\\n🎯 WHEN TO USE POOLING:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"✅ After convolutional layers\")\n",
    "print(\"✅ To reduce computational load\")\n",
    "print(\"✅ To achieve translation invariance\")\n",
    "print(\"✅ Before fully connected layers\")\n",
    "print(\"❌ Not needed with Global Average Pooling in modern architectures\")\n",
    "\n",
    "print(\"\\n✅ Pooling demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac4162",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🏗️ 3. Arsitektur CNN Lengkap\n",
    "\n",
    "## 🧱 Struktur Typical CNN\n",
    "\n",
    "Sebuah CNN umumnya terdiri dari **dua bagian utama**:\n",
    "\n",
    "### 1. **Feature Extraction** 🔍\n",
    "```\n",
    "INPUT IMAGE → CONV → RELU → POOL → CONV → RELU → POOL → ... → FLATTEN\n",
    "```\n",
    "\n",
    "- **Convolutional Layers**: Extract features menggunakan filters\n",
    "- **Activation Functions**: ReLU untuk non-linearity  \n",
    "- **Pooling Layers**: Reduce spatial dimensions\n",
    "- **Multiple Blocks**: Hierarchical feature learning\n",
    "\n",
    "### 2. **Classification** 🎯\n",
    "```\n",
    "FLATTENED FEATURES → DENSE → RELU → DROPOUT → DENSE → SOFTMAX → PREDICTIONS\n",
    "```\n",
    "\n",
    "- **Flatten**: Convert 2D features to 1D vector\n",
    "- **Dense Layers**: Traditional fully-connected neural network\n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "- **Output Layer**: Softmax for multi-class classification\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Information Flow dalam CNN\n",
    "\n",
    "### 📊 **Spatial Dimensions**:\n",
    "- **Width & Height**: Decrease through pooling\n",
    "- **Depth (Channels)**: Increase through convolution\n",
    "\n",
    "### 📈 **Feature Complexity**:\n",
    "- **Early Layers**: Simple features (edges, corners)\n",
    "- **Middle Layers**: Textures, patterns\n",
    "- **Deep Layers**: Complex objects, shapes\n",
    "\n",
    "### 💡 **Design Principles**:\n",
    "- **Gradient of Complexity**: Simple → Complex features\n",
    "- **Spatial Trade-off**: Smaller spatial size, more channels\n",
    "- **Hierarchical Learning**: Build complex features from simple ones\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Hyperparameters\n",
    "\n",
    "### 🔧 **Convolutional Layers**:\n",
    "- **Filters/Kernels**: Number of feature detectors\n",
    "- **Kernel Size**: Spatial size of filters (3×3, 5×5)\n",
    "- **Stride**: Step size for filter movement\n",
    "- **Padding**: Border handling (same/valid)\n",
    "\n",
    "### 🏊 **Pooling Layers**:\n",
    "- **Pool Size**: Size of pooling window (2×2)\n",
    "- **Stride**: Step size for pooling\n",
    "- **Type**: Max pooling vs Average pooling\n",
    "\n",
    "### 🧠 **Architecture Choices**:\n",
    "- **Depth**: Number of layers\n",
    "- **Width**: Number of filters per layer\n",
    "- **Skip Connections**: ResNet-style shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa751acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ 3.1 Building Complete CNN - CIFAR-10 Classification\n",
    "print(\"🏗️ COMPLETE CNN ARCHITECTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"📦 Loading CIFAR-10 dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Dataset info\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"✅ Dataset loaded:\")\n",
    "print(f\"   Training images: {X_train.shape}\")\n",
    "print(f\"   Training labels: {y_train.shape}\")\n",
    "print(f\"   Test images: {X_test.shape}\")\n",
    "print(f\"   Test labels: {y_test.shape}\")\n",
    "print(f\"   Classes: {len(class_names)} ({class_names})\")\n",
    "\n",
    "# Data preprocessing\n",
    "print(f\"\\n🔧 PREPROCESSING DATA\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "X_train_norm = X_train.astype('float32') / 255.0\n",
    "X_test_norm = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"✅ Preprocessing complete:\")\n",
    "print(f\"   Pixel range: {X_train_norm.min():.1f} to {X_train_norm.max():.1f}\")\n",
    "print(f\"   Label shape: {y_train_cat.shape}\")\n",
    "\n",
    "# Visualize sample images\n",
    "print(f\"\\n📊 VISUALIZING SAMPLE DATA\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find first image of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(X_train[idx])\n",
    "    axes[i].set_title(f'{class_names[i]}', fontsize=10, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('🖼️ CIFAR-10 Sample Images (One per Class)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🏗️ BUILDING CNN ARCHITECTURE\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Build CNN model\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), name='conv1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Dropout(0.25, name='dropout1'),\n",
    "    \n",
    "    # Second Convolutional Block  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv3'),\n",
    "    layers.BatchNormalization(name='bn2'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv4'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Dropout(0.25, name='dropout2'),\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', name='conv5'),\n",
    "    layers.BatchNormalization(name='bn3'),\n",
    "    layers.Dropout(0.25, name='dropout3'),\n",
    "    \n",
    "    # Classification Head\n",
    "    layers.GlobalAveragePooling2D(name='global_pool'),\n",
    "    layers.Dense(512, activation='relu', name='dense1'),\n",
    "    layers.BatchNormalization(name='bn4'),\n",
    "    layers.Dropout(0.5, name='dropout4'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "print(\"✅ CNN Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\n📊 ARCHITECTURE ANALYSIS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Analyze model complexity\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"🔢 Model Complexity:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Layer-wise output shapes\n",
    "print(f\"\\n🔍 Layer Output Shapes:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if hasattr(layer, 'output_shape'):\n",
    "        print(f\"   {i+1:2d}. {layer.name:15} → {str(layer.output_shape):20}\")\n",
    "\n",
    "print(f\"\\n⚙️ KEY ARCHITECTURE FEATURES\")\n",
    "print(\"-\" * 30)\n",
    "print(\"🔸 Batch Normalization: Faster training, better gradients\")\n",
    "print(\"🔸 Dropout: Regularization to prevent overfitting\")\n",
    "print(\"🔸 Global Average Pooling: Alternative to flatten + dense\")\n",
    "print(\"🔸 Progressive Filters: 32 → 64 → 128 (increasing complexity)\")\n",
    "print(\"🔸 Small Kernels: 3×3 filters for detailed feature extraction\")\n",
    "\n",
    "print(\"\\n✅ CNN architecture complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 3.2 Training CNN Model\n",
    "print(\"🚀 CNN MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compile model\n",
    "print(\"🔧 Compiling model...\")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"✅ Model compiled with:\")\n",
    "print(\"   Optimizer: Adam\")\n",
    "print(\"   Loss: Categorical Crossentropy\")\n",
    "print(\"   Metrics: Accuracy\")\n",
    "\n",
    "# Prepare callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Training with callbacks:\")\n",
    "print(\"   Early Stopping: Stop if val_accuracy doesn't improve for 5 epochs\")\n",
    "print(\"   Learning Rate Reduction: Reduce LR by 50% if val_loss plateaus\")\n",
    "\n",
    "# Use subset for demo (to speed up training)\n",
    "print(f\"\\n📊 Using subset for demonstration:\")\n",
    "n_samples = 5000  # Use 5000 samples for faster demo\n",
    "indices = np.random.choice(len(X_train_norm), n_samples, replace=False)\n",
    "X_train_demo = X_train_norm[indices]\n",
    "y_train_demo = y_train_cat[indices]\n",
    "\n",
    "print(f\"   Training subset: {X_train_demo.shape}\")\n",
    "print(f\"   Full test set: {X_test_norm.shape}\")\n",
    "\n",
    "# Train model\n",
    "print(f\"\\n🚀 Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=10,  # Reduced for demo\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "\n",
    "print(f\"\\n📊 TRAINING RESULTS VISUALIZATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "ax1.set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax2.set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('🚀 CNN Training Progress', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\n🎯 FINAL MODEL EVALUATION\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "print(f\"📊 Test Results:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(f\"\\n🔮 MAKING PREDICTIONS\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Predict on small test batch\n",
    "test_batch = X_test_norm[:8]\n",
    "test_labels_batch = y_test[:8]\n",
    "predictions = model.predict(test_batch, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(test_batch[i])\n",
    "    true_class = class_names[test_labels_batch[i][0]]\n",
    "    pred_class = class_names[predicted_classes[i]]\n",
    "    confidence = predictions[i][predicted_classes[i]] * 100\n",
    "    \n",
    "    # Color based on correctness\n",
    "    color = 'green' if true_class == pred_class else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'True: {true_class}\\nPred: {pred_class} ({confidence:.1f}%)', \n",
    "                     fontsize=9, color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('🔮 CNN Predictions on Test Images', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(f\"\\n📊 PER-CLASS PERFORMANCE\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "all_predictions = model.predict(X_test_norm, verbose=0)\n",
    "all_predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "all_true_classes = y_test.flatten()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (all_true_classes == i)\n",
    "    class_accuracy = np.mean(all_predicted_classes[class_mask] == all_true_classes[class_mask])\n",
    "    print(f\"   {class_name:10}: {class_accuracy:.3f} ({class_accuracy*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ CNN training and evaluation complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43951e01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔄 4. Transfer Learning - Leveraging Pre-trained Models\n",
    "\n",
    "## 🎯 Mengapa Transfer Learning?\n",
    "\n",
    "**Transfer Learning** adalah teknik menggunakan model yang sudah dilatih pada dataset besar untuk task yang berbeda namun terkait.\n",
    "\n",
    "### ✅ **Keuntungan Transfer Learning**:\n",
    "- **⚡ Faster Training** - Tidak perlu training dari scratch\n",
    "- **📊 Better Performance** - Pre-trained features already good\n",
    "- **💾 Less Data Required** - Effective dengan dataset kecil\n",
    "- **💰 Cost Effective** - Menghemat computational resources\n",
    "- **🎯 Better Generalization** - Features learned from large dataset\n",
    "\n",
    "### 🏗️ **Strategi Transfer Learning**:\n",
    "\n",
    "#### 1. **Feature Extraction** 🔒\n",
    "- **Freeze** pre-trained layers (weights tidak berubah)\n",
    "- **Add** custom classifier di atas\n",
    "- **Use** pre-trained sebagai fixed feature extractor\n",
    "\n",
    "#### 2. **Fine-tuning** 🔧\n",
    "- **Unfreeze** some/all pre-trained layers\n",
    "- **Train** dengan learning rate sangat kecil\n",
    "- **Adapt** features untuk task spesifik\n",
    "\n",
    "#### 3. **Hybrid Approach** 🎭\n",
    "- **Start** dengan feature extraction\n",
    "- **Then** fine-tune top layers\n",
    "- **Gradual** unfreezing dari top ke bottom\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Popular Pre-trained Models\n",
    "\n",
    "### 📚 **ImageNet Pre-trained Models**:\n",
    "- **VGG16/VGG19** - Simple, deep architecture\n",
    "- **ResNet50/ResNet101** - Skip connections, very deep\n",
    "- **InceptionV3** - Multi-scale convolutions\n",
    "- **MobileNet** - Lightweight untuk mobile\n",
    "- **EfficientNet** - State-of-the-art accuracy/efficiency\n",
    "\n",
    "### 🎯 **When to Use Each Strategy**:\n",
    "- **Small dataset + Similar task** → Feature extraction\n",
    "- **Small dataset + Different task** → Feature extraction + fine-tuning\n",
    "- **Large dataset + Similar task** → Fine-tuning\n",
    "- **Large dataset + Different task** → Train from scratch or fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 4.1 Transfer Learning dengan VGG16\n",
    "print(\"🔄 TRANSFER LEARNING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "print(\"📦 Loading VGG16 pre-trained model...\")\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',        # Pre-trained on ImageNet\n",
    "    include_top=False,         # Exclude final classification layer\n",
    "    input_shape=(32, 32, 3)    # CIFAR-10 input shape\n",
    ")\n",
    "\n",
    "print(f\"✅ VGG16 loaded:\")\n",
    "print(f\"   Pre-trained on: ImageNet (1.4M images, 1000 classes)\")\n",
    "print(f\"   Architecture: {len(base_model.layers)} layers\")\n",
    "print(f\"   Parameters: {base_model.count_params():,}\")\n",
    "\n",
    "print(f\"\\n🔒 FEATURE EXTRACTION APPROACH\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "print(f\"✅ Base model frozen (trainable = False)\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in base_model.trainable_weights]):,}\")\n",
    "\n",
    "# Build transfer learning model\n",
    "transfer_model = models.Sequential([\n",
    "    base_model,                                          # Frozen VGG16 base\n",
    "    layers.GlobalAveragePooling2D(name='global_pool'),   # Reduce dimensions\n",
    "    layers.Dense(128, activation='relu', name='dense1'), # Custom classifier\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(10, activation='softmax', name='output') # CIFAR-10 classes\n",
    "])\n",
    "\n",
    "print(f\"\\n🏗️ Transfer Learning Architecture:\")\n",
    "transfer_model.summary()\n",
    "\n",
    "# Compile model\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 TRAINING TRANSFER LEARNING MODEL\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Use same subset as before for comparison\n",
    "print(\"📊 Training with feature extraction...\")\n",
    "transfer_history = transfer_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=5,  # Fewer epochs needed with transfer learning\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 TRANSFER LEARNING RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Evaluate transfer learning model\n",
    "tl_test_loss, tl_test_accuracy = transfer_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"🎯 Transfer Learning Performance:\")\n",
    "print(f\"   Test Loss: {tl_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Compare with from-scratch model\n",
    "if 'test_accuracy' in locals():\n",
    "    improvement = (tl_test_accuracy - test_accuracy) * 100\n",
    "    print(f\"\\n📈 COMPARISON WITH FROM-SCRATCH MODEL:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Transfer Learning: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\n🔧 FINE-TUNING APPROACH\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Fine-tuning: Unfreeze top layers\n",
    "print(\"🔓 Unfreezing top layers for fine-tuning...\")\n",
    "\n",
    "# Unfreeze the top layers of VGG16\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers) - 4  # Unfreeze last 4 layers\n",
    "\n",
    "# Freeze all layers except the top ones\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"   Unfrozen layers: {sum([layer.trainable for layer in base_model.layers])}/{len(base_model.layers)}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in transfer_model.trainable_weights]):,}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"✅ Model recompiled with lower learning rate (0.0001)\")\n",
    "\n",
    "# Fine-tune the model\n",
    "print(f\"\\n🎯 Fine-tuning training...\")\n",
    "fine_tune_history = transfer_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=3,  # Few epochs for fine-tuning\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final evaluation after fine-tuning\n",
    "ft_test_loss, ft_test_accuracy = transfer_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n🏆 FINAL TRANSFER LEARNING RESULTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"🎯 Fine-tuned Performance:\")\n",
    "print(f\"   Test Loss: {ft_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "if 'test_accuracy' in locals():\n",
    "    ft_improvement = (ft_test_accuracy - test_accuracy) * 100\n",
    "    print(f\"\\n📊 COMPLETE COMPARISON:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Feature Extraction: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Fine-tuned: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Total Improvement: {ft_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\n💡 TRANSFER LEARNING INSIGHTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"🔸 Pre-trained features boost performance significantly\")\n",
    "print(\"🔸 Feature extraction: Fast training, good baseline\")\n",
    "print(\"🔸 Fine-tuning: Better adaptation to specific task\")\n",
    "print(\"🔸 Lower learning rates essential for fine-tuning\")\n",
    "print(\"🔸 Fewer epochs needed compared to training from scratch\")\n",
    "\n",
    "print(f\"\\n✅ Transfer learning demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948499a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📸 5. Data Augmentation - Expanding Training Data\n",
    "\n",
    "## 🎯 Mengapa Data Augmentation?\n",
    "\n",
    "**Data Augmentation** adalah teknik memperbanyak training data dengan menerapkan transformasi pada gambar existing.\n",
    "\n",
    "### ✅ **Keuntungan Data Augmentation**:\n",
    "- **📊 More Training Data** - Dari dataset terbatas\n",
    "- **🛡️ Reduce Overfitting** - Model lebih generalizable  \n",
    "- **🔄 Improve Robustness** - Model tahan terhadap variasi\n",
    "- **💰 Cost Effective** - Tidak perlu kumpul data baru\n",
    "- **🎯 Better Performance** - Higher accuracy pada test set\n",
    "\n",
    "### 🔧 **Common Augmentations**:\n",
    "\n",
    "#### **Geometric Transformations** 📐\n",
    "- **Rotation** - Putar gambar (±15°, ±30°)\n",
    "- **Translation** - Geser horizontal/vertical\n",
    "- **Scaling/Zoom** - Perbesar/perkecil gambar\n",
    "- **Flipping** - Horizontal/vertical flip\n",
    "- **Shearing** - Transformasi miring\n",
    "\n",
    "#### **Photometric Transformations** 🎨\n",
    "- **Brightness** - Ubah kecerahan\n",
    "- **Contrast** - Ubah kontras\n",
    "- **Saturation** - Ubah saturasi warna\n",
    "- **Hue** - Ubah hue/warna\n",
    "- **Noise** - Tambah gaussian/salt-pepper noise\n",
    "\n",
    "#### **Advanced Augmentations** 🚀\n",
    "- **Cutout** - Hapus bagian random gambar\n",
    "- **Mixup** - Blend dua gambar berbeda\n",
    "- **CutMix** - Kombinasi cutout dan mixup\n",
    "- **AutoAugment** - Policy search untuk optimal augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Best Practices Data Augmentation**\n",
    "\n",
    "### ✅ **Do's**:\n",
    "- **Domain-appropriate** - Sesuai dengan nature data\n",
    "- **Moderate intensity** - Jangan terlalu ekstrem\n",
    "- **Test different combinations** - A/B test augmentations\n",
    "- **Apply only to training** - Tidak untuk validation/test\n",
    "\n",
    "### ❌ **Don'ts**:\n",
    "- **Semantic changes** - Jangan ubah makna gambar\n",
    "- **Excessive distortion** - Jangan sampai tidak recognizable\n",
    "- **Same augmentation for all** - Sesuaikan dengan dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📸 5.1 Data Augmentation dalam Praktik\n",
    "print(\"📸 DATA AUGMENTATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create data augmentation layers\n",
    "print(\"🔧 Creating augmentation layers...\")\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", seed=42),\n",
    "    layers.RandomRotation(0.1, seed=42),  # ±10% rotation\n",
    "    layers.RandomZoom(0.1, seed=42),      # ±10% zoom\n",
    "    layers.RandomTranslation(0.1, 0.1, seed=42),  # ±10% translation\n",
    "    layers.RandomBrightness(0.2, seed=42),         # ±20% brightness\n",
    "    layers.RandomContrast(0.2, seed=42),           # ±20% contrast\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"✅ Augmentation pipeline created:\")\n",
    "for layer in data_augmentation.layers:\n",
    "    print(f\"   - {layer.__class__.__name__}\")\n",
    "\n",
    "print(f\"\\n📊 VISUALIZING AUGMENTATION EFFECTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Take one sample image\n",
    "sample_image = X_train_norm[0:1]  # Shape: (1, 32, 32, 3)\n",
    "sample_label = class_names[y_train[0][0]]\n",
    "\n",
    "print(f\"Sample image: {sample_label}\")\n",
    "\n",
    "# Generate augmented versions\n",
    "augmented_images = []\n",
    "for i in range(8):\n",
    "    augmented = data_augmentation(sample_image, training=True)\n",
    "    augmented_images.append(augmented[0])\n",
    "\n",
    "# Visualize original + augmentations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_image[0])\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Augmented images\n",
    "for i, aug_img in enumerate(augmented_images, 1):\n",
    "    axes[i].imshow(aug_img)\n",
    "    axes[i].set_title(f'Augmented {i}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'📸 Data Augmentation Effects - {sample_label}', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🏗️ BUILDING MODEL WITH AUGMENTATION\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Create model with built-in augmentation\n",
    "augmented_model = models.Sequential([\n",
    "    # Data augmentation (applied only during training)\n",
    "    data_augmentation,\n",
    "    \n",
    "    # CNN layers\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"✅ Model with augmentation created\")\n",
    "\n",
    "# Compile model\n",
    "augmented_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 TRAINING WITH DATA AUGMENTATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Train with augmentation\n",
    "print(\"Training model with data augmentation...\")\n",
    "aug_history = augmented_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate augmented model\n",
    "aug_test_loss, aug_test_accuracy = augmented_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n📊 DATA AUGMENTATION RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"🎯 Augmented Model Performance:\")\n",
    "print(f\"   Test Loss: {aug_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {aug_test_accuracy:.4f} ({aug_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Compare with previous models\n",
    "if 'test_accuracy' in locals():\n",
    "    print(f\"\\n📈 COMPREHENSIVE COMPARISON:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    if 'ft_test_accuracy' in locals():\n",
    "        print(f\"   Transfer Learning: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   With Augmentation: {aug_test_accuracy:.4f} ({aug_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n🔍 AUGMENTATION ANALYSIS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Show training curves comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Compare training accuracies\n",
    "if 'history' in locals():\n",
    "    ax1.plot(history.history['val_accuracy'], label='Without Augmentation', marker='o')\n",
    "ax1.plot(aug_history.history['val_accuracy'], label='With Augmentation', marker='s')\n",
    "ax1.set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare training losses\n",
    "if 'history' in locals():\n",
    "    ax2.plot(history.history['val_loss'], label='Without Augmentation', marker='o')\n",
    "ax2.plot(aug_history.history['val_loss'], label='With Augmentation', marker='s')\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('📊 Impact of Data Augmentation', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 DATA AUGMENTATION INSIGHTS:\")\n",
    "print(\"-\" * 32)\n",
    "print(\"🔸 Augmentation increases training data variety\")\n",
    "print(\"🔸 Helps prevent overfitting to specific image orientations\")\n",
    "print(\"🔸 Model becomes more robust to real-world variations\")\n",
    "print(\"🔸 Particularly effective with limited training data\")\n",
    "print(\"🔸 Should be applied only during training, not validation/test\")\n",
    "\n",
    "print(f\"\\n🎯 BEST PRACTICES SUMMARY:\")\n",
    "print(\"-\" * 27)\n",
    "print(\"✅ Use domain-appropriate augmentations\")\n",
    "print(\"✅ Start with moderate intensity levels\")\n",
    "print(\"✅ Combine multiple augmentation types\")\n",
    "print(\"✅ Monitor validation performance to avoid over-augmentation\")\n",
    "print(\"✅ Consider advanced techniques (Cutout, Mixup) for better results\")\n",
    "\n",
    "print(f\"\\n✅ Data augmentation demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228449f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 💡 6. Best Practices & Summary\n",
    "\n",
    "## 🎯 CNN Design Best Practices\n",
    "\n",
    "### 🏗️ **Architecture Design**:\n",
    "- **Start Simple**: Begin dengan basic CNN, kemudian complex\n",
    "- **Progressive Filters**: Increase filter count dengan depth (32→64→128→256)\n",
    "- **Small Kernels**: 3×3 filters lebih efektif dari large kernels\n",
    "- **Batch Normalization**: Tambahkan untuk faster dan stable training\n",
    "- **Dropout**: Regularization untuk prevent overfitting\n",
    "\n",
    "### 📊 **Training Strategies**:\n",
    "- **Data Preprocessing**: Normalize pixel values ke [0,1] atau [-1,1]\n",
    "- **Learning Rate**: Start dengan 0.001, gunakan scheduling\n",
    "- **Batch Size**: 32-128 untuk most cases\n",
    "- **Early Stopping**: Monitor validation accuracy untuk avoid overfitting\n",
    "- **Checkpointing**: Save best model selama training\n",
    "\n",
    "### 🔄 **Advanced Techniques**:\n",
    "- **Transfer Learning**: Gunakan pre-trained models untuk boost performance\n",
    "- **Data Augmentation**: Essential untuk small datasets\n",
    "- **Mixed Precision**: Untuk faster training dengan minimal accuracy loss\n",
    "- **Gradient Accumulation**: Untuk effective large batch sizes\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Popular CNN Architectures\n",
    "\n",
    "### 📚 **Classic Architectures**:\n",
    "- **LeNet-5** (1998) - Pioneer CNN untuk digit recognition\n",
    "- **AlexNet** (2012) - ImageNet breakthrough, 8 layers\n",
    "- **VGG** (2014) - Simple, deep architecture dengan small filters\n",
    "- **ResNet** (2015) - Skip connections, very deep networks\n",
    "\n",
    "### 🚀 **Modern Architectures**:\n",
    "- **Inception/GoogLeNet** - Multi-scale convolutions\n",
    "- **MobileNet** - Depthwise separable convolutions untuk efficiency\n",
    "- **EfficientNet** - Compound scaling untuk optimal accuracy/efficiency\n",
    "- **Vision Transformer** - Attention-based architecture\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Common Pitfalls & Solutions\n",
    "\n",
    "### ❌ **Overfitting**:\n",
    "- **Problem**: High training accuracy, low validation accuracy\n",
    "- **Solutions**: Dropout, data augmentation, early stopping, regularization\n",
    "\n",
    "### ❌ **Vanishing Gradients**:\n",
    "- **Problem**: Deep networks fail to train properly\n",
    "- **Solutions**: Batch normalization, ResNet-style skip connections, proper initialization\n",
    "\n",
    "### ❌ **Computational Efficiency**:\n",
    "- **Problem**: Models too slow untuk production\n",
    "- **Solutions**: MobileNet, quantization, pruning, knowledge distillation\n",
    "\n",
    "### ❌ **Data Imbalance**:\n",
    "- **Problem**: Some classes have much more data\n",
    "- **Solutions**: Weighted loss, oversampling, undersampling, focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e98428",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 Chapter Summary & Conclusion\n",
    "\n",
    "## 📚 Apa yang Telah Dipelajari\n",
    "\n",
    "Dalam Chapter 14 ini, kita telah mempelajari:\n",
    "\n",
    "### 🔍 **1. Fundamental Concepts**\n",
    "- ✅ Konvolusi dan filter operations\n",
    "- ✅ Pooling untuk dimensionality reduction\n",
    "- ✅ Hierarchical feature learning\n",
    "\n",
    "### 🏗️ **2. CNN Architecture**\n",
    "- ✅ Complete CNN pipeline: Conv → Pool → Dense\n",
    "- ✅ Modern techniques: Batch normalization, dropout\n",
    "- ✅ Practical implementation dengan TensorFlow/Keras\n",
    "\n",
    "### 🔄 **3. Transfer Learning**\n",
    "- ✅ Feature extraction vs fine-tuning\n",
    "- ✅ Pre-trained models (VGG16, ResNet, etc.)\n",
    "- ✅ Adaptation untuk specific tasks\n",
    "\n",
    "### 📸 **4. Data Augmentation**\n",
    "- ✅ Geometric dan photometric transformations\n",
    "- ✅ Prevention of overfitting\n",
    "- ✅ Robust model training\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Key Takeaways\n",
    "\n",
    "### 💡 **CNN Revolution**:\n",
    "> \"CNNs revolutionized computer vision by learning hierarchical features automatically\"\n",
    "\n",
    "### 🚀 **Transfer Learning Power**:\n",
    "> \"Don't train from scratch - leverage pre-trained models untuk faster dan better results\"\n",
    "\n",
    "### 📊 **Data Augmentation Magic**:\n",
    "> \"More diverse training data leads to more robust models\"\n",
    "\n",
    "### 🔧 **Practical Wisdom**:\n",
    "> \"Start simple, add complexity gradually, always validate on unseen data\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔮 Next Steps & Advanced Topics\n",
    "\n",
    "### 📈 **Immediate Next Steps**:\n",
    "- 🔥 **Object Detection** - YOLO, R-CNN untuk detect multiple objects\n",
    "- 🎭 **Semantic Segmentation** - U-Net, DeepLab untuk pixel-level classification\n",
    "- 👁️ **Face Recognition** - Siamese networks, FaceNet\n",
    "- 🎨 **Style Transfer** - Neural artistic style transfer\n",
    "\n",
    "### 🚀 **Advanced Concepts**:\n",
    "- 🧠 **Attention Mechanisms** - Focus pada important regions\n",
    "- 🌟 **Vision Transformers** - Attention-based architectures\n",
    "- 🎯 **Few-shot Learning** - Learning dengan minimal data\n",
    "- 🔄 **Generative Models** - GANs untuk image generation\n",
    "\n",
    "### 💼 **Real-world Applications**:\n",
    "- 🏥 **Medical Imaging** - X-ray analysis, tumor detection\n",
    "- 🚗 **Autonomous Vehicles** - Self-driving car vision\n",
    "- 📱 **Mobile Applications** - Real-time image recognition\n",
    "- 🛡️ **Security Systems** - Surveillance dan biometric authentication\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Congratulations!\n",
    "\n",
    "🎉 **Excellent work!** Anda telah menguasai fundamental deep computer vision dengan CNNs!\n",
    "\n",
    "**You now understand:**\n",
    "- 🔍 How CNNs extract features from images\n",
    "- 🏗️ How to build dan train CNN architectures\n",
    "- 🔄 How to leverage transfer learning effectively\n",
    "- 📸 How to use data augmentation untuk better models\n",
    "- 💡 Best practices untuk production-ready systems\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Resources for Further Learning\n",
    "\n",
    "### 📖 **Books**:\n",
    "- \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n",
    "- \"Hands-On Machine Learning\" by Aurélien Géron\n",
    "- \"Computer Vision: Algorithms and Applications\" by Richard Szeliski\n",
    "\n",
    "### 🌐 **Online Resources**:\n",
    "- [CS231n: CNN for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "- [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "- [TensorFlow Computer Vision Tutorials](https://www.tensorflow.org/tutorials/images)\n",
    "- [PyTorch Vision Tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "\n",
    "### 🛠️ **Tools & Frameworks**:\n",
    "- **TensorFlow/Keras** - Production-ready deep learning\n",
    "- **PyTorch** - Research-friendly framework\n",
    "- **OpenCV** - Computer vision library\n",
    "- **Albumentations** - Advanced data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning & Building Amazing Computer Vision Applications! 🚀🎯**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
