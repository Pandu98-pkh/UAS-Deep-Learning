{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223a3c3b",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
    "# Bab 14: Deep Computer Vision Menggunakan Convolutional Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Tujuan Pembelajaran\n",
    "\n",
    "Setelah menyelesaikan chapter ini, Anda akan mampu:\n",
    "- âœ… Memahami arsitektur dan konsep CNN\n",
    "- âœ… Mengimplementasikan berbagai layer CNN (Conv2D, MaxPooling, dll)\n",
    "- âœ… Membangun model CNN untuk image classification\n",
    "- âœ… Menerapkan teknik transfer learning\n",
    "- âœ… Mengoptimalkan performa model CNN\n",
    "- âœ… Memahami teknik data augmentation untuk computer vision\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Outline Chapter\n",
    "\n",
    "1. **Pengantar Computer Vision & CNN** ğŸ§ \n",
    "2. **Konvolusi dan Filter** ğŸ”  \n",
    "3. **Pooling Layers** ğŸŠ\n",
    "4. **Arsitektur CNN Lengkap** ğŸ—ï¸\n",
    "5. **Implementasi CNN dengan TensorFlow/Keras** ğŸ’»\n",
    "6. **Transfer Learning** ğŸ”„\n",
    "7. **Data Augmentation** ğŸ“¸\n",
    "8. **CNN Architectures Populer** ğŸ†\n",
    "9. **Best Practices & Tips** ğŸ’¡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Pengantar: Revolusi Computer Vision\n",
    "\n",
    "### ğŸ¤– Mengapa Computer Vision Sulit?\n",
    "\n",
    "Convolutional Neural Networks (CNN) merupakan **revolusi** dalam bidang computer vision. Berbeda dengan Deep Blue IBM yang mengalahkan juara catur dunia Garry Kasparov pada 1996, tugas-tugas visual yang tampak trivial bagi manusia (seperti mendeteksi anak anjing dalam gambar) baru bisa diatasi komputer dalam beberapa tahun terakhir.\n",
    "\n",
    "### ğŸ§  Tantangan Persepsi Visual:\n",
    "- **Otomatis**: Persepsi terjadi di luar kesadaran kita, dalam modul-modul khusus di otak\n",
    "- **Kompleks**: Ketika informasi mencapai kesadaran, sudah dilengkapi dengan fitur-fitur tingkat tinggi  \n",
    "- **Implicit**: Kita tidak bisa menjelaskan bagaimana kita mengenali sesuatu - itu otomatis\n",
    "\n",
    "### ğŸš€ CNN: Terobosan Teknologi\n",
    "\n",
    "CNN meniru cara kerja **korteks visual otak** dan telah mencapai performa superhuman dalam berbagai tugas visual berkat:\n",
    "\n",
    "1. **ğŸ’ª Kekuatan Komputasi** - GPU powerful untuk parallel processing\n",
    "2. **ğŸ“Š Big Data** - Dataset besar seperti ImageNet (14+ million images)\n",
    "3. **ğŸ§  Advanced Techniques** - Teknik training deep networks yang canggih\n",
    "4. **ğŸ—ï¸ Better Architectures** - LeNet â†’ AlexNet â†’ VGG â†’ ResNet â†’ EfficientNet\n",
    "\n",
    "### ğŸ¯ Aplikasi CNN Modern:\n",
    "- ğŸ“± **Mobile Vision** - Face recognition, object detection\n",
    "- ğŸš— **Autonomous Vehicles** - Self-driving cars  \n",
    "- ğŸ¥ **Medical Imaging** - Cancer detection, X-ray analysis\n",
    "- ğŸ® **Augmented Reality** - Real-time object tracking\n",
    "- ğŸ›¡ï¸ **Security** - Surveillance, biometric systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Setup & Import Libraries\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ–¼ï¸ CHAPTER 14: Deep Computer Vision Using CNNs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import libraries yang diperlukan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display versions\n",
    "print(f\"ğŸ“¦ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ“¦ Keras version: {keras.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ğŸ® GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "    print(\"ğŸš€ CUDA enabled - Ready for accelerated training!\")\n",
    "else:\n",
    "    print(\"ğŸ’» Running on CPU\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nâœ… Setup complete! Ready to explore Computer Vision with CNNs\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec6078",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ” 1. Konvolusi dan Filter - Dasar CNN\n",
    "\n",
    "## ğŸ§® Apa itu Konvolusi?\n",
    "\n",
    "**Konvolusi** adalah operasi matematika fundamental dalam CNN yang menerapkan **filter** (kernel) pada gambar untuk mengekstrak fitur.\n",
    "\n",
    "### ğŸ“ Konsep Dasar:\n",
    "- **Input**: Image matrix (contoh: 5Ã—5 pixels)\n",
    "- **Filter/Kernel**: Small matrix (contoh: 3Ã—3) dengan weights yang dapat dipelajari\n",
    "- **Output**: Feature map hasil konvolusi\n",
    "- **Sliding Window**: Filter bergeser pixel demi pixel\n",
    "\n",
    "### ğŸ¯ Fungsi Konvolusi:\n",
    "- **Edge Detection** - Mendeteksi tepi/garis dalam gambar\n",
    "- **Feature Extraction** - Mengekstrak pola dan tekstur\n",
    "- **Spatial Hierarchy** - Membangun representasi hierarkis dari sederhana ke kompleks\n",
    "\n",
    "### âš™ï¸ Parameter Penting:\n",
    "- **Stride**: Langkah pergeseran filter (default: 1)\n",
    "- **Padding**: Penambahan pixel di tepi (same/valid)\n",
    "- **Dilation**: Spacing antara kernel elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb85428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” 1.1 Konvolusi dalam Praktik - Visual Demonstration\n",
    "print(\"ğŸ” CONVOLUTION FUNDAMENTALS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load sample image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "\n",
    "print(f\"ğŸ“· Sample images loaded:\")\n",
    "print(f\"   China image shape: {china.shape}\")\n",
    "print(f\"   Flower image shape: {flower.shape}\")\n",
    "\n",
    "# Convert to grayscale for simplicity\n",
    "def rgb_to_grayscale(images):\n",
    "    \"\"\"Convert RGB images to grayscale\"\"\"\n",
    "    return tf.reduce_mean(tf.cast(images, tf.float32), axis=-1, keepdims=True)\n",
    "\n",
    "# Convert images\n",
    "china_gray = rgb_to_grayscale(china[np.newaxis])\n",
    "flower_gray = rgb_to_grayscale(flower[np.newaxis])\n",
    "\n",
    "print(f\"   Grayscale shapes: {china_gray.shape}\")\n",
    "\n",
    "print(\"\\nğŸ”§ DEFINING CONVOLUTION FILTERS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Define various edge detection filters\n",
    "filters = {\n",
    "    \"Vertical Edge\": np.array([\n",
    "        [[-1, 0, 1],\n",
    "         [-1, 0, 1], \n",
    "         [-1, 0, 1]]\n",
    "    ]),\n",
    "    \"Horizontal Edge\": np.array([\n",
    "        [[-1, -1, -1],\n",
    "         [ 0,  0,  0],\n",
    "         [ 1,  1,  1]]\n",
    "    ]),\n",
    "    \"Diagonal Edge\": np.array([\n",
    "        [[-1, -1,  0],\n",
    "         [-1,  0,  1],\n",
    "         [ 0,  1,  1]]\n",
    "    ]),\n",
    "    \"Sharpen\": np.array([\n",
    "        [[ 0, -1,  0],\n",
    "         [-1,  5, -1],\n",
    "         [ 0, -1,  0]]\n",
    "    ]),\n",
    "    \"Blur\": np.array([\n",
    "        [[1/9, 1/9, 1/9],\n",
    "         [1/9, 1/9, 1/9],\n",
    "         [1/9, 1/9, 1/9]]\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Convert filters to TensorFlow format\n",
    "tf_filters = {}\n",
    "for name, filter_array in filters.items():\n",
    "    # Shape: [height, width, in_channels, out_channels]\n",
    "    tf_filters[name] = tf.constant(filter_array.reshape(3, 3, 1, 1), dtype=tf.float32)\n",
    "    print(f\"âœ… {name} filter prepared: {filter_array.reshape(3, 3)}\")\n",
    "\n",
    "print(\"\\nğŸ¨ APPLYING CONVOLUTION FILTERS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "def apply_convolution(image, filter_tensor, filter_name):\n",
    "    \"\"\"Apply convolution filter to image\"\"\"\n",
    "    # Apply convolution\n",
    "    filtered = tf.nn.conv2d(image, filter_tensor, strides=1, padding='SAME')\n",
    "    return filtered\n",
    "\n",
    "# Apply filters to sample image\n",
    "results = {}\n",
    "sample_image = china_gray\n",
    "\n",
    "print(f\"ğŸ–¼ï¸ Applying filters to image (shape: {sample_image.shape}):\")\n",
    "\n",
    "for filter_name, filter_tensor in tf_filters.items():\n",
    "    filtered_image = apply_convolution(sample_image, filter_tensor, filter_name)\n",
    "    results[filter_name] = filtered_image\n",
    "    print(f\"   âœ… {filter_name}: Output shape {filtered_image.shape}\")\n",
    "\n",
    "print(\"\\nğŸ“Š VISUALIZING CONVOLUTION RESULTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_image[0, :, :, 0], cmap='gray')\n",
    "axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Filtered images\n",
    "for i, (filter_name, filtered_image) in enumerate(results.items(), 1):\n",
    "    if i < len(axes):\n",
    "        axes[i].imshow(filtered_image[0, :, :, 0], cmap='gray')\n",
    "        axes[i].set_title(f'{filter_name}', fontsize=12, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ” Convolution Filter Effects', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ CONVOLUTION INSIGHTS:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"ğŸ”¸ Vertical Edge filter detects vertical boundaries\")\n",
    "print(\"ğŸ”¸ Horizontal Edge filter detects horizontal boundaries\") \n",
    "print(\"ğŸ”¸ Diagonal Edge filter detects diagonal patterns\")\n",
    "print(\"ğŸ”¸ Sharpen filter enhances image details\")\n",
    "print(\"ğŸ”¸ Blur filter smooths the image\")\n",
    "print(\"ğŸ”¸ Each filter extracts different features!\")\n",
    "\n",
    "print(\"\\nâœ… Convolution demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e082ab3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸŠ 2. Pooling Layers - Dimensionality Reduction\n",
    "\n",
    "## ğŸ¯ Mengapa Pooling?\n",
    "\n",
    "**Pooling layers** melakukan **downsampling** pada feature maps dengan tujuan:\n",
    "\n",
    "### âœ… Keuntungan Pooling:\n",
    "- **ğŸ“‰ Reduce Dimensionality** - Mengurangi ukuran spatial (height Ã— width)\n",
    "- **âš¡ Computational Efficiency** - Faster training dan inference\n",
    "- **ğŸ›¡ï¸ Translation Invariance** - Robust terhadap small shifts\n",
    "- **ğŸ¯ Feature Abstraction** - Focus pada fitur penting\n",
    "- **ğŸ“Š Reduce Overfitting** - Fewer parameters to learn\n",
    "\n",
    "### ğŸ”§ Jenis Pooling:\n",
    "\n",
    "#### 1. **Max Pooling** ğŸ†\n",
    "- Mengambil nilai **maksimum** dari setiap window\n",
    "- Paling populer untuk feature extraction\n",
    "- Preserves strong features\n",
    "\n",
    "#### 2. **Average Pooling** ğŸ“Š  \n",
    "- Mengambil nilai **rata-rata** dari setiap window\n",
    "- Smoother output\n",
    "- Often used in final layers\n",
    "\n",
    "#### 3. **Global Pooling** ğŸŒ\n",
    "- **Global Max/Average** dari entire feature map\n",
    "- Reduces to single value per channel\n",
    "- Common before final classification layer\n",
    "\n",
    "### âš™ï¸ Parameter Pooling:\n",
    "- **Pool Size**: Ukuran window (umum: 2Ã—2)\n",
    "- **Strides**: Langkah pergeseran (umum: sama dengan pool size)\n",
    "- **Padding**: Handling border pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac302fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŠ 2.1 Pooling Layers dalam Praktik\n",
    "print(\"ğŸŠ POOLING LAYERS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the filtered image from previous convolution\n",
    "sample_feature_map = results[\"Vertical Edge\"]  # Shape: [1, height, width, 1]\n",
    "\n",
    "print(f\"ğŸ“Š Input feature map shape: {sample_feature_map.shape}\")\n",
    "\n",
    "print(\"\\nğŸ”§ APPLYING DIFFERENT POOLING OPERATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Max Pooling\n",
    "max_pooled = tf.nn.max_pool2d(\n",
    "    sample_feature_map,\n",
    "    ksize=[1, 2, 2, 1],    # Pool size 2x2\n",
    "    strides=[1, 2, 2, 1],  # Stride 2x2\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "# 2. Average Pooling  \n",
    "avg_pooled = tf.nn.avg_pool2d(\n",
    "    sample_feature_map,\n",
    "    ksize=[1, 2, 2, 1],    # Pool size 2x2\n",
    "    strides=[1, 2, 2, 1],  # Stride 2x2\n",
    "    padding='SAME'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Max Pooling: {sample_feature_map.shape} â†’ {max_pooled.shape}\")\n",
    "print(f\"âœ… Average Pooling: {sample_feature_map.shape} â†’ {avg_pooled.shape}\")\n",
    "\n",
    "# Size reduction calculation\n",
    "original_size = sample_feature_map.shape[1] * sample_feature_map.shape[2]\n",
    "pooled_size = max_pooled.shape[1] * max_pooled.shape[2]\n",
    "reduction = (1 - pooled_size / original_size) * 100\n",
    "\n",
    "print(f\"ğŸ“‰ Size reduction: {original_size} â†’ {pooled_size} pixels ({reduction:.1f}% smaller)\")\n",
    "\n",
    "print(\"\\nğŸ“Š VISUALIZING POOLING EFFECTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original feature map\n",
    "axes[0].imshow(sample_feature_map[0, :, :, 0], cmap='gray')\n",
    "axes[0].set_title(f'Original Feature Map\\n{sample_feature_map.shape[1]}Ã—{sample_feature_map.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Max pooled\n",
    "axes[1].imshow(max_pooled[0, :, :, 0], cmap='gray')\n",
    "axes[1].set_title(f'Max Pooled (2Ã—2)\\n{max_pooled.shape[1]}Ã—{max_pooled.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Average pooled\n",
    "axes[2].imshow(avg_pooled[0, :, :, 0], cmap='gray')\n",
    "axes[2].set_title(f'Average Pooled (2Ã—2)\\n{avg_pooled.shape[1]}Ã—{avg_pooled.shape[2]}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸŠ Pooling Operations Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” POOLING WITH DIFFERENT PARAMETERS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Different pool sizes\n",
    "pool_sizes = [2, 3, 4]\n",
    "pooling_results = {}\n",
    "\n",
    "for pool_size in pool_sizes:\n",
    "    pooled = tf.nn.max_pool2d(\n",
    "        sample_feature_map,\n",
    "        ksize=[1, pool_size, pool_size, 1],\n",
    "        strides=[1, pool_size, pool_size, 1],\n",
    "        padding='SAME'\n",
    "    )\n",
    "    pooling_results[f\"{pool_size}x{pool_size}\"] = pooled\n",
    "    \n",
    "    new_height, new_width = pooled.shape[1], pooled.shape[2]\n",
    "    size_reduction = (1 - (new_height * new_width) / original_size) * 100\n",
    "    \n",
    "    print(f\"ğŸ”¸ Pool {pool_size}Ã—{pool_size}: {sample_feature_map.shape[1:3]} â†’ {(new_height, new_width)} \"\n",
    "          f\"({size_reduction:.1f}% reduction)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ POOLING INSIGHTS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"ğŸ”¸ Max Pooling preserves strongest features (sharp edges)\")\n",
    "print(\"ğŸ”¸ Average Pooling creates smoother representation\")\n",
    "print(\"ğŸ”¸ Larger pool sizes = more aggressive dimensionality reduction\")\n",
    "print(\"ğŸ”¸ Trade-off: Smaller size vs Information loss\")\n",
    "print(\"ğŸ”¸ Common choice: 2Ã—2 max pooling with stride 2\")\n",
    "\n",
    "print(\"\\nğŸ¯ WHEN TO USE POOLING:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"âœ… After convolutional layers\")\n",
    "print(\"âœ… To reduce computational load\")\n",
    "print(\"âœ… To achieve translation invariance\")\n",
    "print(\"âœ… Before fully connected layers\")\n",
    "print(\"âŒ Not needed with Global Average Pooling in modern architectures\")\n",
    "\n",
    "print(\"\\nâœ… Pooling demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac4162",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ—ï¸ 3. Arsitektur CNN Lengkap\n",
    "\n",
    "## ğŸ§± Struktur Typical CNN\n",
    "\n",
    "Sebuah CNN umumnya terdiri dari **dua bagian utama**:\n",
    "\n",
    "### 1. **Feature Extraction** ğŸ”\n",
    "```\n",
    "INPUT IMAGE â†’ CONV â†’ RELU â†’ POOL â†’ CONV â†’ RELU â†’ POOL â†’ ... â†’ FLATTEN\n",
    "```\n",
    "\n",
    "- **Convolutional Layers**: Extract features menggunakan filters\n",
    "- **Activation Functions**: ReLU untuk non-linearity  \n",
    "- **Pooling Layers**: Reduce spatial dimensions\n",
    "- **Multiple Blocks**: Hierarchical feature learning\n",
    "\n",
    "### 2. **Classification** ğŸ¯\n",
    "```\n",
    "FLATTENED FEATURES â†’ DENSE â†’ RELU â†’ DROPOUT â†’ DENSE â†’ SOFTMAX â†’ PREDICTIONS\n",
    "```\n",
    "\n",
    "- **Flatten**: Convert 2D features to 1D vector\n",
    "- **Dense Layers**: Traditional fully-connected neural network\n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "- **Output Layer**: Softmax for multi-class classification\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Information Flow dalam CNN\n",
    "\n",
    "### ğŸ“Š **Spatial Dimensions**:\n",
    "- **Width & Height**: Decrease through pooling\n",
    "- **Depth (Channels)**: Increase through convolution\n",
    "\n",
    "### ğŸ“ˆ **Feature Complexity**:\n",
    "- **Early Layers**: Simple features (edges, corners)\n",
    "- **Middle Layers**: Textures, patterns\n",
    "- **Deep Layers**: Complex objects, shapes\n",
    "\n",
    "### ğŸ’¡ **Design Principles**:\n",
    "- **Gradient of Complexity**: Simple â†’ Complex features\n",
    "- **Spatial Trade-off**: Smaller spatial size, more channels\n",
    "- **Hierarchical Learning**: Build complex features from simple ones\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Hyperparameters\n",
    "\n",
    "### ğŸ”§ **Convolutional Layers**:\n",
    "- **Filters/Kernels**: Number of feature detectors\n",
    "- **Kernel Size**: Spatial size of filters (3Ã—3, 5Ã—5)\n",
    "- **Stride**: Step size for filter movement\n",
    "- **Padding**: Border handling (same/valid)\n",
    "\n",
    "### ğŸŠ **Pooling Layers**:\n",
    "- **Pool Size**: Size of pooling window (2Ã—2)\n",
    "- **Stride**: Step size for pooling\n",
    "- **Type**: Max pooling vs Average pooling\n",
    "\n",
    "### ğŸ§  **Architecture Choices**:\n",
    "- **Depth**: Number of layers\n",
    "- **Width**: Number of filters per layer\n",
    "- **Skip Connections**: ResNet-style shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa751acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ 3.1 Building Complete CNN - CIFAR-10 Classification\n",
    "print(\"ğŸ—ï¸ COMPLETE CNN ARCHITECTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"ğŸ“¦ Loading CIFAR-10 dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Dataset info\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"âœ… Dataset loaded:\")\n",
    "print(f\"   Training images: {X_train.shape}\")\n",
    "print(f\"   Training labels: {y_train.shape}\")\n",
    "print(f\"   Test images: {X_test.shape}\")\n",
    "print(f\"   Test labels: {y_test.shape}\")\n",
    "print(f\"   Classes: {len(class_names)} ({class_names})\")\n",
    "\n",
    "# Data preprocessing\n",
    "print(f\"\\nğŸ”§ PREPROCESSING DATA\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "X_train_norm = X_train.astype('float32') / 255.0\n",
    "X_test_norm = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"âœ… Preprocessing complete:\")\n",
    "print(f\"   Pixel range: {X_train_norm.min():.1f} to {X_train_norm.max():.1f}\")\n",
    "print(f\"   Label shape: {y_train_cat.shape}\")\n",
    "\n",
    "# Visualize sample images\n",
    "print(f\"\\nğŸ“Š VISUALIZING SAMPLE DATA\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find first image of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(X_train[idx])\n",
    "    axes[i].set_title(f'{class_names[i]}', fontsize=10, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ–¼ï¸ CIFAR-10 Sample Images (One per Class)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING CNN ARCHITECTURE\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Build CNN model\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), name='conv1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Dropout(0.25, name='dropout1'),\n",
    "    \n",
    "    # Second Convolutional Block  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv3'),\n",
    "    layers.BatchNormalization(name='bn2'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv4'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Dropout(0.25, name='dropout2'),\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', name='conv5'),\n",
    "    layers.BatchNormalization(name='bn3'),\n",
    "    layers.Dropout(0.25, name='dropout3'),\n",
    "    \n",
    "    # Classification Head\n",
    "    layers.GlobalAveragePooling2D(name='global_pool'),\n",
    "    layers.Dense(512, activation='relu', name='dense1'),\n",
    "    layers.BatchNormalization(name='bn4'),\n",
    "    layers.Dropout(0.5, name='dropout4'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "print(\"âœ… CNN Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nğŸ“Š ARCHITECTURE ANALYSIS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Analyze model complexity\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"ğŸ”¢ Model Complexity:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Layer-wise output shapes\n",
    "print(f\"\\nğŸ” Layer Output Shapes:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if hasattr(layer, 'output_shape'):\n",
    "        print(f\"   {i+1:2d}. {layer.name:15} â†’ {str(layer.output_shape):20}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ KEY ARCHITECTURE FEATURES\")\n",
    "print(\"-\" * 30)\n",
    "print(\"ğŸ”¸ Batch Normalization: Faster training, better gradients\")\n",
    "print(\"ğŸ”¸ Dropout: Regularization to prevent overfitting\")\n",
    "print(\"ğŸ”¸ Global Average Pooling: Alternative to flatten + dense\")\n",
    "print(\"ğŸ”¸ Progressive Filters: 32 â†’ 64 â†’ 128 (increasing complexity)\")\n",
    "print(\"ğŸ”¸ Small Kernels: 3Ã—3 filters for detailed feature extraction\")\n",
    "\n",
    "print(\"\\nâœ… CNN architecture complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ 3.2 Training CNN Model\n",
    "print(\"ğŸš€ CNN MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compile model\n",
    "print(\"ğŸ”§ Compiling model...\")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled with:\")\n",
    "print(\"   Optimizer: Adam\")\n",
    "print(\"   Loss: Categorical Crossentropy\")\n",
    "print(\"   Metrics: Accuracy\")\n",
    "\n",
    "# Prepare callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ¯ Training with callbacks:\")\n",
    "print(\"   Early Stopping: Stop if val_accuracy doesn't improve for 5 epochs\")\n",
    "print(\"   Learning Rate Reduction: Reduce LR by 50% if val_loss plateaus\")\n",
    "\n",
    "# Use subset for demo (to speed up training)\n",
    "print(f\"\\nğŸ“Š Using subset for demonstration:\")\n",
    "n_samples = 5000  # Use 5000 samples for faster demo\n",
    "indices = np.random.choice(len(X_train_norm), n_samples, replace=False)\n",
    "X_train_demo = X_train_norm[indices]\n",
    "y_train_demo = y_train_cat[indices]\n",
    "\n",
    "print(f\"   Training subset: {X_train_demo.shape}\")\n",
    "print(f\"   Full test set: {X_test_norm.shape}\")\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nğŸš€ Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=10,  # Reduced for demo\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š TRAINING RESULTS VISUALIZATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "ax1.set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax2.set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸš€ CNN Training Progress', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\nğŸ¯ FINAL MODEL EVALUATION\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "print(f\"ğŸ“Š Test Results:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(f\"\\nğŸ”® MAKING PREDICTIONS\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Predict on small test batch\n",
    "test_batch = X_test_norm[:8]\n",
    "test_labels_batch = y_test[:8]\n",
    "predictions = model.predict(test_batch, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(test_batch[i])\n",
    "    true_class = class_names[test_labels_batch[i][0]]\n",
    "    pred_class = class_names[predicted_classes[i]]\n",
    "    confidence = predictions[i][predicted_classes[i]] * 100\n",
    "    \n",
    "    # Color based on correctness\n",
    "    color = 'green' if true_class == pred_class else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'True: {true_class}\\nPred: {pred_class} ({confidence:.1f}%)', \n",
    "                     fontsize=9, color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ”® CNN Predictions on Test Images', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(f\"\\nğŸ“Š PER-CLASS PERFORMANCE\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "all_predictions = model.predict(X_test_norm, verbose=0)\n",
    "all_predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "all_true_classes = y_test.flatten()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (all_true_classes == i)\n",
    "    class_accuracy = np.mean(all_predicted_classes[class_mask] == all_true_classes[class_mask])\n",
    "    print(f\"   {class_name:10}: {class_accuracy:.3f} ({class_accuracy*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… CNN training and evaluation complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43951e01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”„ 4. Transfer Learning - Leveraging Pre-trained Models\n",
    "\n",
    "## ğŸ¯ Mengapa Transfer Learning?\n",
    "\n",
    "**Transfer Learning** adalah teknik menggunakan model yang sudah dilatih pada dataset besar untuk task yang berbeda namun terkait.\n",
    "\n",
    "### âœ… **Keuntungan Transfer Learning**:\n",
    "- **âš¡ Faster Training** - Tidak perlu training dari scratch\n",
    "- **ğŸ“Š Better Performance** - Pre-trained features already good\n",
    "- **ğŸ’¾ Less Data Required** - Effective dengan dataset kecil\n",
    "- **ğŸ’° Cost Effective** - Menghemat computational resources\n",
    "- **ğŸ¯ Better Generalization** - Features learned from large dataset\n",
    "\n",
    "### ğŸ—ï¸ **Strategi Transfer Learning**:\n",
    "\n",
    "#### 1. **Feature Extraction** ğŸ”’\n",
    "- **Freeze** pre-trained layers (weights tidak berubah)\n",
    "- **Add** custom classifier di atas\n",
    "- **Use** pre-trained sebagai fixed feature extractor\n",
    "\n",
    "#### 2. **Fine-tuning** ğŸ”§\n",
    "- **Unfreeze** some/all pre-trained layers\n",
    "- **Train** dengan learning rate sangat kecil\n",
    "- **Adapt** features untuk task spesifik\n",
    "\n",
    "#### 3. **Hybrid Approach** ğŸ­\n",
    "- **Start** dengan feature extraction\n",
    "- **Then** fine-tune top layers\n",
    "- **Gradual** unfreezing dari top ke bottom\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Popular Pre-trained Models\n",
    "\n",
    "### ğŸ“š **ImageNet Pre-trained Models**:\n",
    "- **VGG16/VGG19** - Simple, deep architecture\n",
    "- **ResNet50/ResNet101** - Skip connections, very deep\n",
    "- **InceptionV3** - Multi-scale convolutions\n",
    "- **MobileNet** - Lightweight untuk mobile\n",
    "- **EfficientNet** - State-of-the-art accuracy/efficiency\n",
    "\n",
    "### ğŸ¯ **When to Use Each Strategy**:\n",
    "- **Small dataset + Similar task** â†’ Feature extraction\n",
    "- **Small dataset + Different task** â†’ Feature extraction + fine-tuning\n",
    "- **Large dataset + Similar task** â†’ Fine-tuning\n",
    "- **Large dataset + Different task** â†’ Train from scratch or fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ 4.1 Transfer Learning dengan VGG16\n",
    "print(\"ğŸ”„ TRANSFER LEARNING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "print(\"ğŸ“¦ Loading VGG16 pre-trained model...\")\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',        # Pre-trained on ImageNet\n",
    "    include_top=False,         # Exclude final classification layer\n",
    "    input_shape=(32, 32, 3)    # CIFAR-10 input shape\n",
    ")\n",
    "\n",
    "print(f\"âœ… VGG16 loaded:\")\n",
    "print(f\"   Pre-trained on: ImageNet (1.4M images, 1000 classes)\")\n",
    "print(f\"   Architecture: {len(base_model.layers)} layers\")\n",
    "print(f\"   Parameters: {base_model.count_params():,}\")\n",
    "\n",
    "print(f\"\\nğŸ”’ FEATURE EXTRACTION APPROACH\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Freeze base model layers\n",
    "base_model.trainable = False\n",
    "print(f\"âœ… Base model frozen (trainable = False)\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in base_model.trainable_weights]):,}\")\n",
    "\n",
    "# Build transfer learning model\n",
    "transfer_model = models.Sequential([\n",
    "    base_model,                                          # Frozen VGG16 base\n",
    "    layers.GlobalAveragePooling2D(name='global_pool'),   # Reduce dimensions\n",
    "    layers.Dense(128, activation='relu', name='dense1'), # Custom classifier\n",
    "    layers.Dropout(0.5, name='dropout'),\n",
    "    layers.Dense(10, activation='softmax', name='output') # CIFAR-10 classes\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Transfer Learning Architecture:\")\n",
    "transfer_model.summary()\n",
    "\n",
    "# Compile model\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING TRANSFER LEARNING MODEL\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Use same subset as before for comparison\n",
    "print(\"ğŸ“Š Training with feature extraction...\")\n",
    "transfer_history = transfer_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=5,  # Fewer epochs needed with transfer learning\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š TRANSFER LEARNING RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Evaluate transfer learning model\n",
    "tl_test_loss, tl_test_accuracy = transfer_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"ğŸ¯ Transfer Learning Performance:\")\n",
    "print(f\"   Test Loss: {tl_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Compare with from-scratch model\n",
    "if 'test_accuracy' in locals():\n",
    "    improvement = (tl_test_accuracy - test_accuracy) * 100\n",
    "    print(f\"\\nğŸ“ˆ COMPARISON WITH FROM-SCRATCH MODEL:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Transfer Learning: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ”§ FINE-TUNING APPROACH\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Fine-tuning: Unfreeze top layers\n",
    "print(\"ğŸ”“ Unfreezing top layers for fine-tuning...\")\n",
    "\n",
    "# Unfreeze the top layers of VGG16\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers) - 4  # Unfreeze last 4 layers\n",
    "\n",
    "# Freeze all layers except the top ones\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"   Unfrozen layers: {sum([layer.trainable for layer in base_model.layers])}/{len(base_model.layers)}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in transfer_model.trainable_weights]):,}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model recompiled with lower learning rate (0.0001)\")\n",
    "\n",
    "# Fine-tune the model\n",
    "print(f\"\\nğŸ¯ Fine-tuning training...\")\n",
    "fine_tune_history = transfer_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=3,  # Few epochs for fine-tuning\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final evaluation after fine-tuning\n",
    "ft_test_loss, ft_test_accuracy = transfer_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\nğŸ† FINAL TRANSFER LEARNING RESULTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"ğŸ¯ Fine-tuned Performance:\")\n",
    "print(f\"   Test Loss: {ft_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "if 'test_accuracy' in locals():\n",
    "    ft_improvement = (ft_test_accuracy - test_accuracy) * 100\n",
    "    print(f\"\\nğŸ“Š COMPLETE COMPARISON:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Feature Extraction: {tl_test_accuracy:.4f} ({tl_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Fine-tuned: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Total Improvement: {ft_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ TRANSFER LEARNING INSIGHTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"ğŸ”¸ Pre-trained features boost performance significantly\")\n",
    "print(\"ğŸ”¸ Feature extraction: Fast training, good baseline\")\n",
    "print(\"ğŸ”¸ Fine-tuning: Better adaptation to specific task\")\n",
    "print(\"ğŸ”¸ Lower learning rates essential for fine-tuning\")\n",
    "print(\"ğŸ”¸ Fewer epochs needed compared to training from scratch\")\n",
    "\n",
    "print(f\"\\nâœ… Transfer learning demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948499a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“¸ 5. Data Augmentation - Expanding Training Data\n",
    "\n",
    "## ğŸ¯ Mengapa Data Augmentation?\n",
    "\n",
    "**Data Augmentation** adalah teknik memperbanyak training data dengan menerapkan transformasi pada gambar existing.\n",
    "\n",
    "### âœ… **Keuntungan Data Augmentation**:\n",
    "- **ğŸ“Š More Training Data** - Dari dataset terbatas\n",
    "- **ğŸ›¡ï¸ Reduce Overfitting** - Model lebih generalizable  \n",
    "- **ğŸ”„ Improve Robustness** - Model tahan terhadap variasi\n",
    "- **ğŸ’° Cost Effective** - Tidak perlu kumpul data baru\n",
    "- **ğŸ¯ Better Performance** - Higher accuracy pada test set\n",
    "\n",
    "### ğŸ”§ **Common Augmentations**:\n",
    "\n",
    "#### **Geometric Transformations** ğŸ“\n",
    "- **Rotation** - Putar gambar (Â±15Â°, Â±30Â°)\n",
    "- **Translation** - Geser horizontal/vertical\n",
    "- **Scaling/Zoom** - Perbesar/perkecil gambar\n",
    "- **Flipping** - Horizontal/vertical flip\n",
    "- **Shearing** - Transformasi miring\n",
    "\n",
    "#### **Photometric Transformations** ğŸ¨\n",
    "- **Brightness** - Ubah kecerahan\n",
    "- **Contrast** - Ubah kontras\n",
    "- **Saturation** - Ubah saturasi warna\n",
    "- **Hue** - Ubah hue/warna\n",
    "- **Noise** - Tambah gaussian/salt-pepper noise\n",
    "\n",
    "#### **Advanced Augmentations** ğŸš€\n",
    "- **Cutout** - Hapus bagian random gambar\n",
    "- **Mixup** - Blend dua gambar berbeda\n",
    "- **CutMix** - Kombinasi cutout dan mixup\n",
    "- **AutoAugment** - Policy search untuk optimal augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ **Best Practices Data Augmentation**\n",
    "\n",
    "### âœ… **Do's**:\n",
    "- **Domain-appropriate** - Sesuai dengan nature data\n",
    "- **Moderate intensity** - Jangan terlalu ekstrem\n",
    "- **Test different combinations** - A/B test augmentations\n",
    "- **Apply only to training** - Tidak untuk validation/test\n",
    "\n",
    "### âŒ **Don'ts**:\n",
    "- **Semantic changes** - Jangan ubah makna gambar\n",
    "- **Excessive distortion** - Jangan sampai tidak recognizable\n",
    "- **Same augmentation for all** - Sesuaikan dengan dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¸ 5.1 Data Augmentation dalam Praktik\n",
    "print(\"ğŸ“¸ DATA AUGMENTATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create data augmentation layers\n",
    "print(\"ğŸ”§ Creating augmentation layers...\")\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\", seed=42),\n",
    "    layers.RandomRotation(0.1, seed=42),  # Â±10% rotation\n",
    "    layers.RandomZoom(0.1, seed=42),      # Â±10% zoom\n",
    "    layers.RandomTranslation(0.1, 0.1, seed=42),  # Â±10% translation\n",
    "    layers.RandomBrightness(0.2, seed=42),         # Â±20% brightness\n",
    "    layers.RandomContrast(0.2, seed=42),           # Â±20% contrast\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"âœ… Augmentation pipeline created:\")\n",
    "for layer in data_augmentation.layers:\n",
    "    print(f\"   - {layer.__class__.__name__}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š VISUALIZING AUGMENTATION EFFECTS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Take one sample image\n",
    "sample_image = X_train_norm[0:1]  # Shape: (1, 32, 32, 3)\n",
    "sample_label = class_names[y_train[0][0]]\n",
    "\n",
    "print(f\"Sample image: {sample_label}\")\n",
    "\n",
    "# Generate augmented versions\n",
    "augmented_images = []\n",
    "for i in range(8):\n",
    "    augmented = data_augmentation(sample_image, training=True)\n",
    "    augmented_images.append(augmented[0])\n",
    "\n",
    "# Visualize original + augmentations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_image[0])\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Augmented images\n",
    "for i, aug_img in enumerate(augmented_images, 1):\n",
    "    axes[i].imshow(aug_img)\n",
    "    axes[i].set_title(f'Augmented {i}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'ğŸ“¸ Data Augmentation Effects - {sample_label}', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ BUILDING MODEL WITH AUGMENTATION\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "# Create model with built-in augmentation\n",
    "augmented_model = models.Sequential([\n",
    "    # Data augmentation (applied only during training)\n",
    "    data_augmentation,\n",
    "    \n",
    "    # CNN layers\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"âœ… Model with augmentation created\")\n",
    "\n",
    "# Compile model\n",
    "augmented_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸš€ TRAINING WITH DATA AUGMENTATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Train with augmentation\n",
    "print(\"Training model with data augmentation...\")\n",
    "aug_history = augmented_model.fit(\n",
    "    X_train_demo, y_train_demo,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test_norm, y_test_cat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate augmented model\n",
    "aug_test_loss, aug_test_accuracy = augmented_model.evaluate(X_test_norm, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\nğŸ“Š DATA AUGMENTATION RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"ğŸ¯ Augmented Model Performance:\")\n",
    "print(f\"   Test Loss: {aug_test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {aug_test_accuracy:.4f} ({aug_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Compare with previous models\n",
    "if 'test_accuracy' in locals():\n",
    "    print(f\"\\nğŸ“ˆ COMPREHENSIVE COMPARISON:\")\n",
    "    print(f\"   From Scratch: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    if 'ft_test_accuracy' in locals():\n",
    "        print(f\"   Transfer Learning: {ft_test_accuracy:.4f} ({ft_test_accuracy*100:.2f}%)\")\n",
    "    print(f\"   With Augmentation: {aug_test_accuracy:.4f} ({aug_test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ” AUGMENTATION ANALYSIS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Show training curves comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Compare training accuracies\n",
    "if 'history' in locals():\n",
    "    ax1.plot(history.history['val_accuracy'], label='Without Augmentation', marker='o')\n",
    "ax1.plot(aug_history.history['val_accuracy'], label='With Augmentation', marker='s')\n",
    "ax1.set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare training losses\n",
    "if 'history' in locals():\n",
    "    ax2.plot(history.history['val_loss'], label='Without Augmentation', marker='o')\n",
    "ax2.plot(aug_history.history['val_loss'], label='With Augmentation', marker='s')\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ“Š Impact of Data Augmentation', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ DATA AUGMENTATION INSIGHTS:\")\n",
    "print(\"-\" * 32)\n",
    "print(\"ğŸ”¸ Augmentation increases training data variety\")\n",
    "print(\"ğŸ”¸ Helps prevent overfitting to specific image orientations\")\n",
    "print(\"ğŸ”¸ Model becomes more robust to real-world variations\")\n",
    "print(\"ğŸ”¸ Particularly effective with limited training data\")\n",
    "print(\"ğŸ”¸ Should be applied only during training, not validation/test\")\n",
    "\n",
    "print(f\"\\nğŸ¯ BEST PRACTICES SUMMARY:\")\n",
    "print(\"-\" * 27)\n",
    "print(\"âœ… Use domain-appropriate augmentations\")\n",
    "print(\"âœ… Start with moderate intensity levels\")\n",
    "print(\"âœ… Combine multiple augmentation types\")\n",
    "print(\"âœ… Monitor validation performance to avoid over-augmentation\")\n",
    "print(\"âœ… Consider advanced techniques (Cutout, Mixup) for better results\")\n",
    "\n",
    "print(f\"\\nâœ… Data augmentation demonstration complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228449f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ’¡ 6. Best Practices & Summary\n",
    "\n",
    "## ğŸ¯ CNN Design Best Practices\n",
    "\n",
    "### ğŸ—ï¸ **Architecture Design**:\n",
    "- **Start Simple**: Begin dengan basic CNN, kemudian complex\n",
    "- **Progressive Filters**: Increase filter count dengan depth (32â†’64â†’128â†’256)\n",
    "- **Small Kernels**: 3Ã—3 filters lebih efektif dari large kernels\n",
    "- **Batch Normalization**: Tambahkan untuk faster dan stable training\n",
    "- **Dropout**: Regularization untuk prevent overfitting\n",
    "\n",
    "### ğŸ“Š **Training Strategies**:\n",
    "- **Data Preprocessing**: Normalize pixel values ke [0,1] atau [-1,1]\n",
    "- **Learning Rate**: Start dengan 0.001, gunakan scheduling\n",
    "- **Batch Size**: 32-128 untuk most cases\n",
    "- **Early Stopping**: Monitor validation accuracy untuk avoid overfitting\n",
    "- **Checkpointing**: Save best model selama training\n",
    "\n",
    "### ğŸ”„ **Advanced Techniques**:\n",
    "- **Transfer Learning**: Gunakan pre-trained models untuk boost performance\n",
    "- **Data Augmentation**: Essential untuk small datasets\n",
    "- **Mixed Precision**: Untuk faster training dengan minimal accuracy loss\n",
    "- **Gradient Accumulation**: Untuk effective large batch sizes\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Popular CNN Architectures\n",
    "\n",
    "### ğŸ“š **Classic Architectures**:\n",
    "- **LeNet-5** (1998) - Pioneer CNN untuk digit recognition\n",
    "- **AlexNet** (2012) - ImageNet breakthrough, 8 layers\n",
    "- **VGG** (2014) - Simple, deep architecture dengan small filters\n",
    "- **ResNet** (2015) - Skip connections, very deep networks\n",
    "\n",
    "### ğŸš€ **Modern Architectures**:\n",
    "- **Inception/GoogLeNet** - Multi-scale convolutions\n",
    "- **MobileNet** - Depthwise separable convolutions untuk efficiency\n",
    "- **EfficientNet** - Compound scaling untuk optimal accuracy/efficiency\n",
    "- **Vision Transformer** - Attention-based architecture\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Common Pitfalls & Solutions\n",
    "\n",
    "### âŒ **Overfitting**:\n",
    "- **Problem**: High training accuracy, low validation accuracy\n",
    "- **Solutions**: Dropout, data augmentation, early stopping, regularization\n",
    "\n",
    "### âŒ **Vanishing Gradients**:\n",
    "- **Problem**: Deep networks fail to train properly\n",
    "- **Solutions**: Batch normalization, ResNet-style skip connections, proper initialization\n",
    "\n",
    "### âŒ **Computational Efficiency**:\n",
    "- **Problem**: Models too slow untuk production\n",
    "- **Solutions**: MobileNet, quantization, pruning, knowledge distillation\n",
    "\n",
    "### âŒ **Data Imbalance**:\n",
    "- **Problem**: Some classes have much more data\n",
    "- **Solutions**: Weighted loss, oversampling, undersampling, focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e98428",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ‰ Chapter Summary & Conclusion\n",
    "\n",
    "## ğŸ“š Apa yang Telah Dipelajari\n",
    "\n",
    "Dalam Chapter 14 ini, kita telah mempelajari:\n",
    "\n",
    "### ğŸ” **1. Fundamental Concepts**\n",
    "- âœ… Konvolusi dan filter operations\n",
    "- âœ… Pooling untuk dimensionality reduction\n",
    "- âœ… Hierarchical feature learning\n",
    "\n",
    "### ğŸ—ï¸ **2. CNN Architecture**\n",
    "- âœ… Complete CNN pipeline: Conv â†’ Pool â†’ Dense\n",
    "- âœ… Modern techniques: Batch normalization, dropout\n",
    "- âœ… Practical implementation dengan TensorFlow/Keras\n",
    "\n",
    "### ğŸ”„ **3. Transfer Learning**\n",
    "- âœ… Feature extraction vs fine-tuning\n",
    "- âœ… Pre-trained models (VGG16, ResNet, etc.)\n",
    "- âœ… Adaptation untuk specific tasks\n",
    "\n",
    "### ğŸ“¸ **4. Data Augmentation**\n",
    "- âœ… Geometric dan photometric transformations\n",
    "- âœ… Prevention of overfitting\n",
    "- âœ… Robust model training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### ğŸ’¡ **CNN Revolution**:\n",
    "> \"CNNs revolutionized computer vision by learning hierarchical features automatically\"\n",
    "\n",
    "### ğŸš€ **Transfer Learning Power**:\n",
    "> \"Don't train from scratch - leverage pre-trained models untuk faster dan better results\"\n",
    "\n",
    "### ğŸ“Š **Data Augmentation Magic**:\n",
    "> \"More diverse training data leads to more robust models\"\n",
    "\n",
    "### ğŸ”§ **Practical Wisdom**:\n",
    "> \"Start simple, add complexity gradually, always validate on unseen data\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® Next Steps & Advanced Topics\n",
    "\n",
    "### ğŸ“ˆ **Immediate Next Steps**:\n",
    "- ğŸ”¥ **Object Detection** - YOLO, R-CNN untuk detect multiple objects\n",
    "- ğŸ­ **Semantic Segmentation** - U-Net, DeepLab untuk pixel-level classification\n",
    "- ğŸ‘ï¸ **Face Recognition** - Siamese networks, FaceNet\n",
    "- ğŸ¨ **Style Transfer** - Neural artistic style transfer\n",
    "\n",
    "### ğŸš€ **Advanced Concepts**:\n",
    "- ğŸ§  **Attention Mechanisms** - Focus pada important regions\n",
    "- ğŸŒŸ **Vision Transformers** - Attention-based architectures\n",
    "- ğŸ¯ **Few-shot Learning** - Learning dengan minimal data\n",
    "- ğŸ”„ **Generative Models** - GANs untuk image generation\n",
    "\n",
    "### ğŸ’¼ **Real-world Applications**:\n",
    "- ğŸ¥ **Medical Imaging** - X-ray analysis, tumor detection\n",
    "- ğŸš— **Autonomous Vehicles** - Self-driving car vision\n",
    "- ğŸ“± **Mobile Applications** - Real-time image recognition\n",
    "- ğŸ›¡ï¸ **Security Systems** - Surveillance dan biometric authentication\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Congratulations!\n",
    "\n",
    "ğŸ‰ **Excellent work!** Anda telah menguasai fundamental deep computer vision dengan CNNs!\n",
    "\n",
    "**You now understand:**\n",
    "- ğŸ” How CNNs extract features from images\n",
    "- ğŸ—ï¸ How to build dan train CNN architectures\n",
    "- ğŸ”„ How to leverage transfer learning effectively\n",
    "- ğŸ“¸ How to use data augmentation untuk better models\n",
    "- ğŸ’¡ Best practices untuk production-ready systems\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Resources for Further Learning\n",
    "\n",
    "### ğŸ“– **Books**:\n",
    "- \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n",
    "- \"Hands-On Machine Learning\" by AurÃ©lien GÃ©ron\n",
    "- \"Computer Vision: Algorithms and Applications\" by Richard Szeliski\n",
    "\n",
    "### ğŸŒ **Online Resources**:\n",
    "- [CS231n: CNN for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "- [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "- [TensorFlow Computer Vision Tutorials](https://www.tensorflow.org/tutorials/images)\n",
    "- [PyTorch Vision Tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "\n",
    "### ğŸ› ï¸ **Tools & Frameworks**:\n",
    "- **TensorFlow/Keras** - Production-ready deep learning\n",
    "- **PyTorch** - Research-friendly framework\n",
    "- **OpenCV** - Computer vision library\n",
    "- **Albumentations** - Advanced data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning & Building Amazing Computer Vision Applications! ğŸš€ğŸ¯**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
